[
  {
    "id": 8700775,
    "doi": "10.1016/j.envsoft.2015.10.025",
    "title": "The Biodiversity and Climate Change Virtual Laboratory: Where ecology meets big data",
    "abstract": "Advances in computing power and infrastructure, increases in the number and size of ecological and environmental datasets, and the number and type of data collection methods, are revolutionizing the field of Ecology. To integrate these advances, virtual laboratories offer a unique tool to facilitate, expedite, and accelerate research into the impacts of climate change on biodiversity. We introduce the uniquely cloud-based Biodiversity and Climate Change Virtual Laboratory (BCCVL), which provides access to numerous species distribution modelling tools; a large and growing collection of biological, climate, and other environmental datasets; and a variety of experiment types to conduct research into the impact of climate change on biodiversity. Users can upload and share datasets, potentially increasing collaboration, cross-fertilisation of ideas, and innovation among the user community. Feedback confirms that the BCCVL's goals of lowering the technical requirements for species distribution modelling, and reducing time spent on such research, are being met",
    "fullText": "lable at ScienceDirectEnvironmental Modelling & Software xxx (2015) 1e5Contents lists avaiEnvironmental Modelling & Softwarejournal homepage: www.elsevier .com/locate/envsoftSoftware data newsThe Biodiversity and Climate Change Virtual Laboratory: Whereecology meets big dataWillow Hallgren a, *, Linda Beaumont b, Andrew Bowness a, Lynda Chambers c,Erin Graham d, Hamish Holewa a, Shawn Laffan e, Brendan Mackey a, Henry Nix f,Jeff Price g, Jeremy Vanderwal d, Rachel Warren g, Gerhard Weis aa Grif\ufb01th University, Gold Coast Campus, Parklands Drive, Southport, QLD, 4215, Australiab Department of Biological Sciences, Faculty of Science and Engineering, Macquarie University, NSW, 2109, Australiac Bureau of Meteorology, Melbourne, 3001, Australiad eResearch and the Centre for Tropical Biodiversity and Climate Change, James Cook University, Townsville, QLD, 4810, Australiae Centre for Ecosystem Science, School of Biological, Earth and Environmental Science, University of New South Wales, 2052, Australiaf The Fenner School of Environment and Society, The Australian National University, Building 141 Linnaeus WayCanberra, ACT, 2601, Australiag Tyndall Centre for Climate Change Research, School of Environmental Sciences, University of East Anglia, Norwich, NR4 7TJ, United Kingdoma r t i c l e i n f oArticle history:Received 22 May 2015Received in revised form24 October 2015Accepted 29 October 2015Available online xxxKeywords:BiodiversityClimate changeVirtual LaboratorySpecies distribution modelling* Corresponding author. Grif\ufb01th School of EnviroGrif\ufb01th University, QLD, 4222, Australia.E-mail address: w.hallgren@grif\ufb01th.edu.au (W. Hahttp://dx.doi.org/10.1016/j.envsoft.2015.10.0251364-8152/\u00a9 2015 The Authors. Published by ElsevierPlease cite this article in press as: Hallgren,Environmental Modelling & Software (2015a b s t r a c tAdvances in computing power and infrastructure, increases in the number and size of ecological andenvironmental datasets, and the number and type of data collection methods, are revolutionizing the\ufb01eld of Ecology. To integrate these advances, virtual laboratories offer a unique tool to facilitate, expedite,and accelerate research into the impacts of climate change on biodiversity. We introduce the uniquelycloud-based Biodiversity and Climate Change Virtual Laboratory (BCCVL), which provides access tonumerous species distribution modelling tools; a large and growing collection of biological, climate, andother environmental datasets; and a variety of experiment types to conduct research into the impact ofclimate change on biodiversity.Users can upload and share datasets, potentially increasing collaboration, cross-fertilisation of ideas,and innovation among the user community. Feedback con\ufb01rms that the BCCVL's goals of lowering thetechnical requirements for species distribution modelling, and reducing time spent on such research, arebeing met.\u00a9 2015 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/).1. Introduction and purposeMany \ufb01elds of research are undergoing a methodological revo-lution, and in recent years this has particularly applied to the \ufb01eldsof both ecology and \u201ce-research\u201d. \u201ce-Research\u201d is the application ofInformation and Communication Technologies (ICT), tools andinfrastructure to scienti\ufb01c investigations. In developed countries, e-Research utilises national computing networks, virtual laboratories,research clouds, high performance computing, and \u201capps\u201d formonitoring and data collection, as well as extensions to citizenscience.nment, Gold Coast campus,llgren).Ltd. This is an open access article uW., et al., The Biodiversity an), http://dx.doi.org/10.1016/j.eData repositories such as the Atlas of Living Australia (ALA) andthe Global Biodiversity Information Facility (GBIF, http://www.gbif.org/) have 50 and 530 million specimen records respectively, andare increasing the rate at which they amass information. Satelliteand airborne sensors are also generating petabytes of spatiallyexplicit environmental data and increasing the diversity of avail-able data types.Due to the growth in the size, complexity and diversity ofdatasets (\u201cBig Data\u201d), computational and analytical improvementsin statistical and simulation models, such as machine learning(Peters et al., 2014), as well as the recent evolution in web tech-nologies to utilise and work with environmental big data (Vitoloet al., 2015), e-Research has a great potential to advance scienti\ufb01cknowledge, particularly in the \ufb01eld of Ecology. Due to the unprec-edented and growing ability to securely store, manage, share,nder the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).d Climate Change Virtual Laboratory: Where ecology meets big data,nvsoft.2015.10.025W. Hallgren et al. / Environmental Modelling & Software xxx (2015) 1e52analyse and synthesise research data within and across the entirediscipline, e-Research has the potential to facilitate research andcreate new scienti\ufb01c insights in the \ufb01eld of Ecology.The combination of e-Research and Big Data has made possiblethe development of the Biodiversity and Climate Change VirtualLaboratory (BCCVL), which we introduce here.The BCCVL is a comprehensive platform for species distributionand trait modelling, and is designed to assist the ecological researchcommunity by connecting researchers to existing and new researchfacilities; datasets, data repositories, and major data storage andmanagement facilities; and the high-performance computational,analytical, work-\ufb02ow and visualisation tools enabled by e-Research.Through a cloud-based e-Research facility, the BCCVL provides re-searchers, environmental managers, policy analysts and otherinterested communities with access to:1. A suite of the most commonly used and robust modelling toolsand functions to spatially analyse biological data;2. A comprehensive set of climate change data comprisingmonthly estimates of downscaled climate change projections ofnational to international extent;3. Ancillary physical, environmental, vegetation and land coverdata of national extent;4. Important post-modelling diagnostic, mapping and other visu-alization capacities;5. A facility to upload and share data and work\ufb02ows; and6. The means to undertake spatial modelling at multiple spatialscales down to a 250 m resolution.The BCCVL is currently populated with predominantly Austra-lian datasets, but the provision of global datasets, and links toglobal databases is in development. However, the BCCVL cancurrently be used for species distribution modelling and otherbiodiversity analyses for anywhere in the world through theassimilation of user-provided data. Research (in progress) whichutilises the BCCVL and user-provided data is focussing on themodelling of international species, thus con\ufb01rming the BCCVL'sutility for conducting biodiversity research internationally. TheBCCVL can also serve as a template for other countries wanting toset up their own Virtual Laboratory.There are many advantages to using the BCCVL: it enables re-searchers to conduct modelling experiments and related analysesfar more ef\ufb01ciently and effectively. It decreases the preparationtime associated with modelling, including data preparation (i.e.identifying, acquiring, scaling/standardising, validating and visu-alising data), setting up the modelling environment (which couldrequire learning a programming language such as, for example, theR language, then importing data into R, identifying the algorithmsto use and the R package for each, importing R package/s into the Renvironment, running the algorithms (individually), then visual-ising and manipulating outputs to create maps and graphics), andwriting scripts to run complex ensemble experiments.Depending on the user, this process could take from weeks tomonths. The BCCVL negates the need for this preparatory work, andfor advanced programming and modelling expertise. This results inan increase in research capability and ef\ufb01ciency, and this will likelyfacilitate the development of additional research trajectoriescurrently not feasible due to both the logistical and computationallimitations of individuals and many research groups.The BCCVL allows comprehensive ensemble modelling experi-ments (already the norm in the climate modelling community)involving large numbers of species, SDM algorithms, climate modelprojections, and emissions scenarios. This was once logisticallyquite challenging, but is now accessible via a web browser. Theability to do this easily will enable more comprehensivePlease cite this article in press as: Hallgren, W., et al., The Biodiversity anEnvironmental Modelling & Software (2015), http://dx.doi.org/10.1016/j.ecomparisons of different SDM algorithms, competing sets of po-tential explanatory variables, and climate impacts, as simulated bya large range of available climate models, emissions scenarios andprojection periods. It will also greatly facilitate comprehensivesensitivity analyses on SDM parameter values, which is an exampleof a research topic that has received little attention previously, duepartly to logistical and computational resource constraints.As such, the increase in the feasibility of large ensemblemodelling experiments, together with education about theirimportance for scienti\ufb01c rigour and the greater possibility of sci-enti\ufb01c consilience with it's potential importance for policy devel-opment and planning, will encourage users to implement thesemore complex experimental designs.The factors mentioned above will contribute to an increase inresearch productivity (in terms of time saved and scienti\ufb01c output)for species distribution modellers who use the BCCVL, which willconfer advantages both at the level of the individual scientist andresearch communities. Moreover, the BCCVL enables researchers toshare data andmodelling frameworks, promoting the use and reuseof data, which is currently underexploited (Peters et al. 2014) andenabling greater transparency in the research process. In the sec-tions below we illustrate the use of the BCCVL in the Australianenvironment.2. Description of the Biodiversity and Climate Change VirtualLaboratoryA link from the BCCVL homepage at www.bccvl.org.au (Fig. 1)allows anyone with an Australian Access Federation (AAF) pass-word to log into the BCCVL. Other domestic and international userscan request a login account with the BCCVL, or log in either byacquiring a guest AAF account, or via the AAF Virtual Home.2.1. Structure and functionalityThe BCCVL comprises three components:2.1.1. DatasetsThe Datasets section of the BCCVL houses species location andtrait data, current and future climate data, and other environmentaldata (e.g., soil, geology and vegetation type).Brief dataset summaries are listed on the front page of thedataset section, which provides the choice of viewing a map of thedataset (overlain on a national map, with a choice of maps avail-able), downloading the dataset, or accessing metadata in a pop-upbox. This page allows users to search among the datasets providedby the BCCVL, shared datasets, and self-uploaded datasets.Searches can be \ufb01ltered by dataset type (species absence,abundance, occurrence, and species traits (e.g. functional traitssuch as seed mass, rooting depth, root type, \ufb01re tolerance, whichrespond to environmental changes); current and future climate,and other environmental datasets), as well as resolution (90 m,250 m, 1 km, 5 km, 10 km, 20 km, and 50 km). The datasets pagealso provides a facility to search for, view, import and share adataset from online repositories such as the ALA. Users can uploadtheir own species occurrence, abundance or trait datasets, andother environmental datasets, and share them with fellow BCCVLusers if they choose to.Recent changes to open (free) data policies, such as the Landsatprogram, and the requirements of some government fundingbodies, have greatly expanded the range of data accessible to re-searchers. Datasets currently accessible within the BCCVL are listedin Table A.1 (Appendix A), and the BCCVL has the capacity to addadditional datasets as needed.d Climate Change Virtual Laboratory: Where ecology meets big data,nvsoft.2015.10.025Fig. 1. The BCCVL homepage at www.bccvl.org.au.W. Hallgren et al. / Environmental Modelling & Software xxx (2015) 1e5 32.1.1.1. Climate change projection data. Global climate changemodelling was carried out by driving the MAGICC4.1 climate model(Wigley and Raper, 2001; Lowe et al., 2009) with time series of 21stcentury emissions to create a projection of 21st century climatechange. Simulations undertaken explored uncertainties in threekey parameters: climate sensitivity, ocean mixing rate, and a cli-mateecarbon cycle feedback factor in MAGICC.Median projections from these simulations were used to drive apattern-scaling module ClimGEN (developed from Mitchell, 2003;see also Warren et al., 2008; Osborn, 2009) in which scaled climatechange patterns diagnosed from seven of the archived CMIP3 GCMsimulations are combined with a baseline climate (CRUTS 3.0 for1961e1990, updated from Mitchell and Jones, 2005).The resulting projections of four indicators (monthly mean,minimum and maximum temperatures, de\ufb01ned as the average ofthe daily maximum (minimum) temperatures during a month, andtotal precipitation) were downscaled to a resolution of 0.5\u0001 \u0003 0.5\u0001.ClimGEN was used to produce projected monthly time series for30-year periods centred on 2020 (i.e. 2011e2040), 2050, and 2080,These were then averaged to produce representative monthly cli-mates for each 30-year period, using the above mentioned fourindicators. This approach was necessary because GCMs have notbeen run for the mitigation scenarios which were needed for thisproject.Emission scenarios used in this analysis included a baseline eSRES A1B (Nakicenovic and Swart, 2000), and several mitigationscenarios developed for the AVOID project (Gohar and Lowe, 2009),which initially follow the baseline scenario before transitioningover seven years so that emissions peak globally in either 2016 or2030. They are then reduced subsequently at rates of between 2and 5% annually, until they reach a hypothetical lower limitdesigned to represent emissions thatmight be dif\ufb01cult to eliminate,e.g. from the agricultural system. These scenarios are combinedwith seven alternative GCM-derived change patterns, producing 42projected climates consistent with the IPCC (PCMDI, 2009).The downscaled climate data were post-processed to produceeight bioclimatic indices: average maximum temperature of thewarmest month, the average minimum temperature of the coldestmonth, annual mean temperature, temperature seasonality, totalPlease cite this article in press as: Hallgren, W., et al., The Biodiversity anEnvironmental Modelling & Software (2015), http://dx.doi.org/10.1016/j.eannual rainfall, rainfall seasonality, rainfall of the wettest quarterand rainfall of the driest quarter. These were calculated directlyfrom the average climates of the aforementioned 30-year periods.Refer to Appendix A for further details on the methodology.2.1.2. ExperimentsThe Experiments section of the BCCVL allows users to access asuite of statistical modelling and analytical tools. There arecurrently \ufb01ve different types of experiments users can undertake:a) Species Distribution Modelling Experiments identify the po-tential distribution of a species given current climateconditions;b) Climate Change Experiments project a current species distri-bution into the future based on a climate projection, for oneor more emission scenarios;c) Biodiverse Experiments calculate biodiversity statistics (spe-cies richness and endemism) based on species distributionmodelling results;d) Species Trait Modelling (STM) Experiments identify futuredistributions of a particular species trait (e.g. Leaf AreaIndex);e) Ensemble Modelling Experiments enable the utilization ofmultiple models (SDMs, STMs or climate models) or sce-narios to reduce some of the uncertainty inherent in thesingle-model/scenario approach.An example of how to use the BCCVL to implement two of theseexperiment types; (a) species distribution modelling and (b) pro-jecting a species distribution model into the future with a climatemodel projection, is given in Appendix B. The SDMs are the corefunctionality of the BCCVL as their results are used in most of theother components. There are currently 17 SDM algorithms avail-able, as well as \ufb01ve algorithms employed in species trait modelling.These include the popular and well-known MaxEnt (Phillips et al.,2006) and Arti\ufb01cial Neural Networks (Hilbert and Van DenMuyzenberg, 1999), as well as simpler and more easily compre-hensible algorithms, such as Bioclim (Nix, 1986). All algorithmsemployed in the BCCVL are described in Table B.1 (Appendix B). Ad Climate Change Virtual Laboratory: Where ecology meets big data,nvsoft.2015.10.025W. Hallgren et al. / Environmental Modelling & Software xxx (2015) 1e54demonstration of a Species Distribution Modelling Experiment isalso available in Appendix C.Supplementary data related to this article can be found online athttp://dx.doi.org/10.1016/j.envsoft.2015.10.025.The algorithms that are currently implemented in species traitmodelling within the BCCVL include widely used statistical meth-odologies such as Generalized Additive Models, Generalized LinearModels (as for species distribution modelling), but also othercommon statistical methodologies such as linear models, analysisof variance, and multivariate analysis of variance. The BCCVLautomatically facilitates modelling experiments at multiple scales:currently the range of resolutions available for modelling experi-ments is 90 m to 50 km.2.1.3. Knowledge baseThe Knowledge Base is designed as a repository of informationabout many facets of the BCCVL. It includes a glossary, backgroundinformation on all modelling algorithms, and links to key refer-ences and papers. In development are a user-tested, expert-informed 'decision support tool' to guide and inform users as theyproceed through the steps to undertake species distributionmodelling and other experiments offered by the BCCVL, and anopen online course which provides theoretical and practical infor-mation on many aspects of the BCCVL experiments. These featuresof the BCCVL will also be improved by continuing user input andfeedback.2.2. Technical detailsThe BCCVL utilises a variety of open source software packages,which are operated on the Australian National eResearch Collabo-ration Tools, and Resource Project (NeCTAR) Research Cloud. TheBCCVL's architecture is designed to handle large datasets, processdata through experiments, display experiment outputs andsecurely share data within a cloud-based setting.The BCCVL is novel in its utilisation of cloud-based technologiesto perform modelling functions traditionally reserved for clusterservices or purpose built High Performance Computing. Cloudbased technologies are often designed using commodity hardware(compute, storage and networking) and achieve scale and resiliencethrough the ability to easily add and replace individualcomponents.The BCCVL utilises the NeCTAR Research Cloud, which willprovide 35,000 cores of processing capacity hosted at eight nodes(data centres) distributed across Australia. Utilisation of cloud-based technologies enables the BCCVL to easily scale to meet newdemands for processing or storage capacity. For example, the BCCVLuses the SWIFT object storage package to handle and replicate largedatasets in a cloud environment. It allows the BCCVL to robustlyhandle duplication and storage of large datasets with appropriatesafeguards to mitigate against data loss and corruption.Within the application, the BCCVL is composed of six discretecomponents, comprising: (i) visualizer, (ii) front-end user inter-face, (iii) back-end manager, and (iv) data mover components, aswell as (v) job execution and worker node, and (vi) swift objectstorage components. These components communicate throughcommon Application Programming Interfaces (APIs) such asSOAP, JSON and XL-RPC to enable modularity and the ability toadd additional resources or features to the BCCVL whilst inoperation. Appendix D provides a schematic of the major com-ponents and information architecture of the BCCVL (Fig. D1), aswell as further technical details on the six components thatconstitute the BCCVL.All code is open source and available on GitHub at https://github.com/BCCVL. The biodiversity experiments arePlease cite this article in press as: Hallgren, W., et al., The Biodiversity anEnvironmental Modelling & Software (2015), http://dx.doi.org/10.1016/j.eimplemented using the Biodiverse platform (Laffan et al., 2010;http://purl.org/biodiverse).3. ConclusionsThe BCCVL is a unique tool for the facilitation of research intoBiodiversity and the impact of Climate Change. Strong feedbackfrom researchers in the \ufb01rst few months after the launch of theBCCVL con\ufb01rms that the goals of lowering the technical re-quirements for conducting research into climate impacts onbiodiversity, as well as reducing the time it takes to do suchresearch, have been met.These two factors are designed to feed into productivity gainsfor individual researchers, andwill likely propel the \ufb01eld forward interms of the number of species, and species response traits whichwill be the subject of biodiversity-climate change modelling ex-periments and analyses. As such, we believe that the BCCVL rep-resents a signi\ufb01cant step forward for the species distribution andspecies trait modelling community, and will likely broaden thecomplexity of the experimental design and the scope of theresearch undertaken in this \ufb01eld in the future.Future development of the BCCVL will focus on adding moreenvironmental datasets, and potentially other species distributionmodels, species trait models, and post-modelling analytical tools.AcknowledgementsFunding for the BCCVL comes from the Australian NationaleResearch Tools and Resources Project (NeCTAR). A complete list ofparticipating institutions, development team members, andgovernance and advisory committee members can be found atwww.bccvl.org.au. No co-authors have any con\ufb02icts of interests todeclare.Appendix A. Supplementary dataSupplementary data related to this article can be found at http://dx.doi.org/10.1016/j.envsoft.2015.10.025.ReferencesGohar, L., Lowe, J., 2009. Summary of the Committee on Climate Change's 2016 PeakEmission Scenarios. Work Stream 1, Report 1 of the AVOID Programme (AV/WS1/D1/R01). Available online at: www.avoid.uk.net. http://www.metof\ufb01ce.gov.uk/media/pdf/5/l/AVOID_WS1_D1_01_20090205.pdf.Hilbert, D.W., Van Den Muyzenberg, J., 1999. Using an arti\ufb01cial neural network tocharacterize the relative suitability of environments for forest types in a com-plex tropical vegetation mosaic. Divers. Distributions 5, 263e274.Laffan, S.W., Lubarsky, E., Rosauer, D.F., 2010. Biodiverse, a tool for the spatialanalysis of biological and related diversity. Ecography 33 (4), 643e647.Lowe, J.A., Huntingford, C., Raper, S.C.B., Jones, C.D., Liddicoat, S.K., Gohar, L.K., 2009.How dif\ufb01cult is it to recover from dangerous levels of global warming. Environ.Res. Lett. 4, 014012.Mitchell, T.D., 2003. Pattern scaling e an examination of the accuracy of the tech-nique for describing future climates. Clim. Change 60, 217e242.Mitchell, T.D., Jones, P.D., 2005. An improved method of constructing a database ofmonthly climate observations and associated high-resolution grids. Int. J. Cli-matol. 25, 693e712.Nakicenovic, N., Swart, R., 2000. ISBN 0521804930. In: Nakicenovic, Nebojsa,Swart, Robert (Eds.), Special Report on Emissions Scenarios (SRES). (WorkingGroup III of the Intergovernmental Panel on Climate Change, 2000). CambridgeUniversity Press,, Cambridge, UK, p. 612.Nix, H.A., 1986. A biogeographic analysis of Australian elapid snakes. In: atlas ofelapid snakes of Australia. Series Number 7. In: Longmore, R. (Ed.), AustralianFlora and Fauna. Australian Government Publishing Service, Canberra, pp. 4e15.Osborn, T.J., 2009. A User Guide for ClimGen: a Flexible Tool for Generating MonthlyClimate Data Sets and Scenarios. Climatic Research Unit. University of EastAnglia, Norwich.PCMDI, 2009. IPCC Model Output. http://www.pcmdi.llnl.gov/ipcc/about_ipcc.php,26 September 2009.Peters, D.P.C., Havstad, K.M., Cushing, J., Tweedie, C., Fuentes, O., Villanueva-Rosales, N., 2014. Harnessing the power of big data: infusing the scienti\ufb01cd Climate Change Virtual Laboratory: Where ecology meets big data,nvsoft.2015.10.025W. Hallgren et al. / Environmental Modelling & Software xxx (2015) 1e5 5method with machine learning to transform ecology. Ecosphere 5 (6), 67.Phillips, S.J., Anderson, R., Schapire, R.E., 2006. Maximum entropy modeling ofspecies geographic distributions. Ecol. Model. 190, 231e259.Vitolo, C., Elkhatib, Y., Reusser, D., Macleod, C.J.A., Buytaert, W., 2015. Web tech-nologies for environmental big data. Environ. Model. Softw. 63, 185e198.Warren, R., de la Nava Santos, S., Arnell, N.W., Bane, M., Barker, T., et al., 2008.Please cite this article in press as: Hallgren, W., et al., The Biodiversity anEnvironmental Modelling & Software (2015), http://dx.doi.org/10.1016/j.eDevelopment and illustrative outputs of the Community Integrated AssessmentSystem (CIAS), a multi-institutional modular integrated assessment approachfor modelling climate change. Environ. Model. Softw. 23, 592e610.Wigley, T.M.L., Raper, S.C.B., 2001. Interpretation of high projections for global-mean warming. Science 293, 451e454.d Climate Change Virtual Laboratory: Where ecology meets big data,nvsoft.2015.10.025",
    "source": "'Elsevier BV'"
  },
  {
    "id": 7091437,
    "doi": "10.1007/s00382-013-1708-x",
    "title": "Comparing the spatial structure of variability in two datasets against each other on the basis of EOF-modes",
    "abstract": "In analysis of climate variability or change it is often of interest how the spatial structure in modes of variability in two datasets differ from each other, e.g. between past and future climate  or between models and observations. Often such analysis is based on Empirical Orthogonal Function (EOF) analysis or other simple indices of large-scale spatial structures. The present analysis lays out a concept on how two datasets of multi-variate climate variability can be compared against each other on basis of EOF analysis and how the differences in the multi-variate spatial structure between the two datasets can be quantified in terms of explained variance in the leading spatial patterns. It is also illustrated how the patterns of largest differences between the two datasets can be defined and interpreted.\n\nWe illustrate this method on the basis of several well-defined artificial examples and by comparing our approach with examples of climate change studies from the literature. These literature examples include analysis of changes in the modes of variability under climate change for the Sea Level Pressure (SLP) of the North Atlantic and Europe, the SLP of the Southern Hemisphere, the Surface Temperature of the Northern Hemisphere, the Sea Surface Temperature of the North Pacific and for Precipitation in the tropical Indo-Pacific. The discussion of the literature examples illustrates that the method introduced here is at least partly more sensitive than the approaches used in the literature and it allows a better quantification of the changes in the modes of variability",
    "fullText": "Comparing the spatial structure of variability in two datasets \nagainst each other on the basis of EOF-modes\nTobias Bayr and Dietmar Dommenget\nIntensification of a pattern\nIntensification of  a pattern:\n\u2022 One or more eigenvalues will be more dominant in one dataset relative to the \nother dataset.\n\u2022 One (only one!) DEOF-mode will be significantly dominant in the one dataset \nrelative to the other dataset.\nPattern shift:\n\u2022 One or more eigenvalues will be more dominant in both datasets relative to the \nother dataset.\n\u2022 One (only one!) DEOF-mode will be significantly dominant in each of the two \ndatasets. \n\u2022 The DEOF-modes peak at the locations where the variance is increased most \nrelative to the other dataset, marking the location shift.\nChange in the multivariate structure:\n\u2022 Most leading eigenvalues will be more dominant in one dataset relative to the \nother dataset.\n\u2022 The higher-ranked eigenvalues of the other data set maybe more dominating \nthan in the first dataset.\n\u2022 Two or more DEOF-modes will be significantly dominant in the first dataset \nrelative to the other dataset.\n\u2022 More than one large-scale leading EOF-mode will be more dominant than in the \nother dataset.\nContact: Tobias Bayr (tbayr@geomar.de)\nPattern shift Change in the \nmultivariate structure\nFigure 1: a) Forcing pattern in dataset \u0001 and in b) ratio of the standard deviation of the\nsecond dataset divided by the first dataset\nFigure 2: a-b) EOF patterns of dataset \u0002, c-d) EOF patterns of dataset \u0001, explained variance is\ngiven in brackets; e) the explained variances of the eigenvalues of dataset \u0002 (black) and\nexplained variances of dataset \u0002 projected onto the eigenvalues of dataset \u0001 (red dashed); f)\nDEOF\u0002\u2192\u0001 patterns; the explained variances \b\t\n\u0002\u2192\u0001\u000b\u0002\f and\t\b\t\n\u0002\u2192\u0001\u000b\u0001\f are given in the\nheader in brackets; g) same as (e), but here showing the eigenvalues of dataset \u0001 (red) and the\nexplained variances of dataset \u0001 projected onto the eigenvalues of dataset \u0002 (black dashed), h)\nsame as f), but for the DEOF\u0001\u2192\u0002 patterns .\nFigure 3: Same as Fig. 2, but here for North Pacific SST in the period 1950-1999 (20C) compared\nwith the period 2050-2099 of the A1B scenario (21C) of a CMIP3 multimodel ensemble.\nFigure 4: a) Forcing pattern in dataset \u0002, in b) forcing pattern in dataset \u0001, in c) the\ndifference between the forcings \u0001-\u0002 and in d) ratio of the standard deviation of the\nsecond dataset divided by the first dataset\nFigure 5: Same as Fig. 2, but here for the two forcings in Fig. 4.\nFigure 6: Same as Fig. 3, but here for winter SLP over the North Atlantic region.\nFigure 7: Same as Fig. 2, but here with no forcing patterns, but different decorrelation lengths\n(about 7 grid point in dataset \u0002 and about 10 grid points in dataset \u0001).\nFigure 8: Same as Fig. 3, but here for precipitation over the Tropical Indo-Pacific region.\nReference: Bayr, T., and D. Dommenget (2013), Comparing the spatial structure of variability in two datasets against \neach other on the basis of EOF-modes, Climate Dynamics, doi:10.1007/s00382-013-1708-x. \nAbstract\nIn analysis of climate variability or change it is often\nof interest how the spatial structure in modes of\nvariability in two datasets differ from each other, e.g.\nbetween past and future climate or between models\nand observations. Often such analysis is based on\nEmpirical Orthogonal Function (EOF) analysis or other\nsimple indices of large-scale spatial structures.\nHere we illustrate how the Distinct EOF (DEOF)\nmethod reveals changes in the modes of variability,\nlike intensification of one pattern, a shift of a pattern\nand a change in the multivariate structure, each on\nthe basis of a well-defined artificial example of\nisotropic diffusion and an example of climate change.\nThese climate change studies are about the North\nPacific SST, the North Atlantic SLP and the tropical\nIndo-Pacific precipitation.\nHow to compare the spatial structure of\nvariability in two datasets against each\nother on the basis of EOF-modes?\n1. Define anomalies for both datasets.\n2. Do EOF-analysis for both datasets (e.g. Fig. 2a-d\nor Fig. 3a-b).\n3. Define the EOF-modes of one dataset as the\nreference modes.\n4. Project the reference EOF-modes onto the other\ndataset \u0001 projected explained variances (e.g.\ndashed line in EV-spectrum in Fig. 2e or Fig. 3c).\n5. Compute the DEOF-modes by pairwise rotation to\nmaximize the differences in explained variance of\nthis mode in the two data sets (e.g. DEOF in Fig. 2f\nor Fig. 3e).\n6. Repeat steps 3 to 5 with the other dataset as the\nreference modes (Fig. 3d,f) .\n7. Compare the results with idealized examples\n(Fig. 2, 5 and 7) to understand the nature of the\ndifferences.\n",
    "source": ""
  },
  {
    "id": 24764869,
    "doi": "10.1016/j.wace.2016.02.001",
    "title": "Quantile-based bias correction and uncertainty quantification of extreme\n  event attribution statements",
    "abstract": "Extreme event attribution characterizes how anthropogenic climate change may\nhave influenced the probability and magnitude of selected individual extreme\nweather and climate events. Attribution statements often involve quantification\nof the fraction of attributable risk (FAR) or the risk ratio (RR) and\nassociated confidence intervals. Many such analyses use climate model output to\ncharacterize extreme event behavior with and without anthropogenic influence.\nHowever, such climate models may have biases in their representation of extreme\nevents. To account for discrepancies in the probabilities of extreme events\nbetween observational datasets and model datasets, we demonstrate an\nappropriate rescaling of the model output based on the quantiles of the\ndatasets to estimate an adjusted risk ratio. Our methodology accounts for\nvarious components of uncertainty in estimation of the risk ratio. In\nparticular, we present an approach to construct a one-sided confidence interval\non the lower bound of the risk ratio when the estimated risk ratio is infinity.\nWe demonstrate the methodology using the summer 2011 central US heatwave and\noutput from the Community Earth System Model. In this example, we find that the\nlower bound of the risk ratio is relatively insensitive to the magnitude and\nprobability of the actual event.Comment: 28 pages, 4 figures, 3 table",
    "fullText": "Generated using version 3.0 of the official AMS LATEX template\nQuantile-based Bias Correction and Uncertainty Quantification of\nExtreme Event Attribution Statements\nSoyoung Jeon\nEarth Sciences Division, Lawrence Berkeley National Laboratory\nBerkeley, California, USA\nsoyoungj82@gmail.com\nChristopher J. Paciorek\nDepartment of Statistics, University of California\nBerkeley, California, USA\npaciorek@stat.berkeley.edu\nMichael F. Wehner\nComputational Research Division, Lawrence Berkeley National Laboratory\nBerkeley, California, USA\nmfwehner@lbl.gov\n1\nar\nX\niv\n:1\n60\n2.\n04\n13\n9v\n1 \n [s\ntat\n.M\nE]\n  1\n2 F\neb\n 20\n16\nABSTRACT\nExtreme event attribution characterizes how anthropogenic climate change may have influ-\nenced the probability and magnitude of selected individual extreme weather and climate\nevents. Attribution statements often involve quantification of the fraction of attributable\nrisk (FAR) or the risk ratio (RR) and associated confidence intervals. Many such analyses\nuse climate model output to characterize extreme event behavior with and without anthro-\npogenic influence. However, such climate models may have biases in their representation of\nextreme events. To account for discrepancies in the probabilities of extreme events between\nobservational datasets and model datasets, we demonstrate an appropriate rescaling of the\nmodel output based on the quantiles of the datasets to estimate an adjusted risk ratio. Our\nmethodology accounts for various components of uncertainty in estimation of the risk ratio.\nIn particular, we present an approach to construct a one-sided confidence interval on the\nlower bound of the risk ratio when the estimated risk ratio is infinity. We demonstrate the\nmethodology using the summer 2011 central US heatwave and output from the Community\nEarth System Model. In this example, we find that the lower bound of the risk ratio is\nrelatively insensitive to the magnitude and probability of the actual event.\n1\n1. Introduction\nThe summer of 2011 was extremely hot in Texas and Oklahoma, producing a record\nof 30.26\u25e6C for the average June-July-August (JJA) temperature (3.24\u25e6C above the 1961-\n1990 mean) as measured in the CRU observational dataset (CRU TS 3.21, Harris et al.\n(2014)). In a previous study of the 2011 Texas heat wave by Hoerling et al. (2013), a\nmajor factor contributing to the magnitude of 2011 heat wave was the severe drought over\nTexas resulting from the La Nin\u02dca phase of the ocean state. However, the analysis found\na substantial anthropogenic increase in the chance of an event of this magnitude. As in\nmost mid-latitude land regions, the probability of extreme summer heat in this region has\nincreased due to human-induced climate change (Min et al. 2013). However, as Stone et al.\n(2013) note, depending on spatial extent of the region analyzed, observed summer warming\nis low in Texas in 2011 and traceable to the so-called \u201cwarming hole\u201d (Meehl et al. 2012).\nExtreme event attribution analyses attempt to characterize whether and how the proba-\nbility of an extreme event has changed because of external forcing, usually anthropogenic, of\nthe climate system. As with traditional detection and attribution of trends in climate vari-\nables (Bindoff et al. 2013), climate models must play an important role in the methodology\ndue to the absence of extremely long observational records. The fraction of attributable risk\n(FAR) or the risk ratio (RR) are commonly-used measures that quantify this potential hu-\nman influence (Palmer 1999; Allen 2003; Stott et al. 2004; Jaeger et al. 2008; Pall et al. 2011;\nWolski et al. 2014). Following the notation used in Stott et al. (2004), let pA be the proba-\nbility in a simulation using all external (anthropogenic plus natural) forcings of an event of\nsimilar magnitude, location and season to the actual event and pC be the probability of such\nan event under natural forcings. The FAR is defined as FAR = 1 \u2212 pC/pA while the RR\nis defined as RR = pA/pC , with each quantity a simple mathematical transformation of the\nother. We note that the commonly used term \u201crisk ratio\u201d is more precisely a \u201cprobability\nratio\u201d (Fischer and Knutti 2015) but we will stick to the RR nomenclature in this study\u2014in\npart because RR is well-established terminology.\nIn the seminal study of the 2003 European heat wave by Stott et al. (2004), their climate\nmodel did remarkably well in simulating both European mean summer temperature and its\ninterannual standard deviation. However, this is not generally the case for the entirety of\navailable climate model outputs nor for the wide range of extreme events of current interest\n(Peterson et al. 2012, 2013; Herring et al. 2014). Hence there is a need to correct model\noutput, particularly in the tail of its distribution, to more realistically estimate both pA and\npC . Quantile-based mapping is often used to reduce such climate model biases in statistical\ndownscaling studies of future climate change projections. Such methods match quantiles\nof climate model outputs to observed data for monthly GCM temperature and precipita-\ntion (Wood et al. 2004). For instance, quantile-based corrections to the transfer function\nbetween the coarse mesh of the global models and the finer downscaled mesh have been\nobtained by using cumulative distribution functions (CDFs) to match percentiles between\nthe model outputs and observations over a specified base period (Maurer and Hidalgo 2008).\nLi et al. (2010) proposed an adjustment of the traditional quantile matching method (Panof-\nsky and Brier 1968) to account for time-dependent changes in the distribution of the future\nclimate and suggested that the quantile-matching method is a simple and straightforward\nmethod for reducing the scale differences between simulations and observations, for the tails\n2\nof the distribution as well. The quantile mapping approach of Li et al. (2010) has been\npreviously used to empirically estimate annual and decadal maximum daily precipitation in\nan attribution study of an early season blizzard in western South Dakota (Edwards et al.\n2014).\nThis paper is concerned with developing a formal statistical methodology using extreme\nvalue analysis combined with quantile mapping to adjust for model biases in event attribu-\ntion analyses. We apply the methodology to the 2011 central US heatwave as a case study,\nusing an ensemble of climate model simulations. In Section 2, we describe the observed\nand simulated data for the central US heatwave analysis. Section 3 presents our statistical\nmethodology, describing the use of extreme value methods combined with the quantile bias\ncorrection to estimate the risk ratio. We describe several approaches for estimating uncer-\ntainty in the risk ratio, focusing on the use of a likelihood ratio-based confidence interval\nthat provides a one-sided interval even when the estimated risk ratio is infinity. In Section\n4 we present results from using the methodology for event attribution for the central US\nheatwave, showing strong evidence of anthropogenic influence.\n2. Case Study: Summer 2011 Central USA Heatwave\nFor a representative case study of extreme temperature attribution, we define a central\nUnited States region bordered by 90\u25e6W to 105\u25e6W in longitude and 25\u25e6N to 45\u25e6N in latitude,\nchosen to encompass the Texas and Oklahoma heatwave that occurred in summer 2011 (see\nFigure 1). For this region, we calculated summer (June, July, August [JJA]) average tem-\nperature anomalies for the time period 1901-2012 by averaging daily maximum temperatures\nfor grid cells falling within the study region. Anomalies are computed using 1961-1990 as\nthe reference period.\nThe observational data in this study are obtained from the gridded data product (CRU\nTS 3.21, Climatic Research Unit Time Series) available on a 0.5\u25e6\u00d70.5\u25e6 grid provided by the\nClimatic Research Unit (Harris et al. 2014). This dataset provides monthly average daily\nmaximum surface air temperature anomalies. Similarly, monthly averaged daily maximum\nsurface air temperatures were obtained from the CMIP5 database through the Earth System\nGrid Federation (ESGF) archive. For both the observations and model output, spatial aver-\nages over the cells covering the land surface of the region were calculated, resulting in simple\n1-dimensional time series. In this study, we use a single climate model, the fourth version\nof the Community Climate System Model (CCSM4) with a resolution of 1.25\u25e6\u00d70.94\u25e6 grid.\nTo more fully explore the structural uncertainty in event attribution statements, additional\nmodels would need to be included in the analysis. While that topic is outside the scope of\nthis paper, our methodology is also relevant for analyses that use multiple models that will\neach have their own biases.\nThe CCSM4 ensemble consists of multiple simulations, each initialized from different\ntimes of a control run; we treat the ensemble members as independent realizations of the\nmodel\u2019s possible climate state. For the actual scenario with all forcings included, we use\nan ensemble of five members, constructed by concatenating the period 1901-2005 from the\nCMIP5 \u201chistorical\u201d forcings experiment and the period 2006-2012 from the matching RCP8.5\nemissions scenario experiment. As a representation of a world without human interference\non the climate system, we construct a counterfactual scenario by producing an ensemble of\n3\n12 100-year segments drawn from the preindustrial control run. In this scenario, greenhouse\ngases, aerosols and stratospheric ozone concentrations are set at pre-industrial levels, but\nother external natural forcings such as solar variability and volcanoes are not included.\nWe use this counterfactual scenario as a proxy for the natural climate system without any\nexternal forcing factors.\nAn important consideration in event attribution analyses is whether the climate model(s)\nreasonably represent the magnitudes and frequencies of the event of interest (Christidis et al.\n2013). Figure 2 shows that summer temperatures vary more in the CCSM4 output than in\nthe observations. The record observed extreme value in our central US region in 2011 was\n2.467\u25e6C above the 1961-1990 average (represented by the large black dot); even this extreme\nis somewhat lower than the observed values over just the states of Texas and Oklahoma.\nHowever, this value is not particularly rare in either model scenario dataset. Due to this scale\nmismatch in temperature variability, the climate model incorrectly estimates the probabilities\nof extreme events of this magnitude in both scenarios. In light of this model bias, a quantile\nmapping procedure to scale the extreme values of either the model or the observations to the\nother is warranted to more consistently relate the model\u2019s risk ratio to the real world. More\nprecisely, we define the event according to observations, even in the presence of observational\nerror, and calibrate the model to the observations with the quantile-based method described\nin this paper. The methodology presented in Section 3 implements such a scaling by first\nestimating the probability, p\u02c6O, of reaching or exceeding the actual event magnitude from the\nobservations. Then, the magnitude, z\u02c6A, of an event in that time with the same probability,\np\u02c6O(= p\u02c6A), is estimated from the actual scenario of the model. The risk ratio can then be\nestimated from the probability, p\u02c6C , of an event of magnitude z\u02c6A from the counterfactual\nscenario of the model as R\u0302R = p\u02c6A/p\u02c6C .\nImplicit in this estimation of RR is an assumption that the asymptotic behavior of the all\nforcings model ensemble is similar to the observations. Indeed, it is not clear how to validate\nthat assumption given the limited observational data availability and the rarity of the events\nof interest in attribution studies. However, it is clear that errors from estimating RR directly\nfrom the model without a quantile mapping correction would be larger, because probability\nestimates would be drawn from a different part of the distribution. In this case study, such\nprobabilities would not be representative of the tail of the distribution. Furthermore, in\nother cases, the model may underestimate variability, and the probability in the model of\nan event of the actual magnitude may be zero due to the boundedness of the distribution\nfunction. We return to the implications of bounded distributions for uncertainty estimates in\nSection 3. There is a risk that bias correction could mask serious model errors in simulating\nthe processes responsible for the extreme event in question. This risk is also present in more\ncommonly-used bias correction techniques such as the use of anomalies based on subtracting\noff or dividing by a reference value. In the present example, a complete assessment of the\nrobustness of the results would also include analysis of CCSM4\u2019s ability to reproduce the\ntype of large-scale meteorological patterns leading to central US heatwaves as well as its\nsimulation of ENSO.\n4\n3. Methodology\na. Quantile Bias Correction\nHere we describe a quantile mapping methodology to adjust for the difference in scales\nbetween observations and model outputs; we call this methodology quantile bias correction.\nThe methodology seeks to estimate adjusted probabilities pA and pC and the corresponding\nRR. From this point forward, since we will work exclusively with the adjusted probabilities,\nwe will simply use pA and pC to refer to the adjusted probabilities rather than introduce\nadditional notation to distinguish adjusted and unadjusted probabilities. The steps of the\nmethod are as follows:\n(1) observe some extreme event, e.g., the extreme value of 2.467\u25e6C for the 2011 central US\nheatwave, and estimate the probability, p\u02c6O, of the observed event using appropriate\nextreme value statistical methods,\n(2) use extreme value methods applied to the model output under the actual scenario\nto estimate the magnitude, z\u02c6A, associated with the probability p\u02c6O, thereby defining\npA = pO,\n(3) use extreme value methods applied to the model output under the counterfactual\nscenario to estimate the probability p\u02c6C of exceeding the value z\u02c6A, and\n(4) calculate the estimated risk ratio R\u0302R = p\u02c6A/p\u02c6C .\nStep 2 is the critical bias adjustment, where the method adjusts the magnitude of the extreme\nevent considered in the model output to be of the same rarity in the model under the all\nforcings scenario as the actual extreme event is in the observations. This correction in the\ntail of the distribution is likely to be very different than a simple adjustment of the model\nmean and/or variance and more appropriate to event attribution studies. Figure 3 illustrates\nthe quantile bias correction method and demonstrates the steps with cumulative distribution\nfunctions for the 2011 central US heatwave analysis.\nb. Using Extreme Value Statistics to Estimate Event Probabilities\nThe probabilities, pO and pC , can be estimated using a variety of techniques. For in-\nstance, in studies using ensembles with tens of thousands of model realizations (Pall et al.\n2011), probabilities of very rare events can often be estimated simply using the proportion\nof realizations in which the event was observed. However, in our case study, as will be the\ncase in many other analyses, there are only a few simulations and the tail of the distribution\nis not well sampled. Extreme value statistical methods involve fitting a three parameter\nextreme value distribution function to a subset of the available sample and are well suited\nto estimating such probabilities. After estimating the distribution\u2019s parameters, step 2 can\nbe accomplished by inverting the distribution to estimate the magnitude of z\u02c6A in the form\nof a return value for the period 1/p\u02c6O.\nIn the current study, we use a point process (PP) approach to extreme value analysis\n(Smith 1989; Coles 2001; Katz et al. 2002; Furrer et al. 2010). This approach involves\nmodeling exceedances over a high threshold and is described in detail in the Appendix. The\n5\nsimplest formulations of extreme value models assume that the distribution of the extremes\ndoes not change over time, an assumption of stationarity. The PP approach can be extended\nto non-stationary cases in which the parameters of the model, \u00b5, \u03c3, and \u03be, are allowed to\nbe (arbitrary but often linear) functions of covariates. Covariates are chosen to incorporate\nadditional physical insight into the statistical model. A common practice is to represent\nnonstationarity through only the location parameter, \u00b5, and take \u03c3 and \u03be to be constant\n(Coles 2001; Kharin and Zwiers 2005). For example, one could represent the location of the\nextreme value distribution \u00b5t to depend on time t as a function of time-varying covariates\nxkt:\n\u00b5t = \u03b20 +\nK\u2211\nk=1\n\u03b2kxkt. (1)\nThe model under the actual scenario, as seen in Figure 2, is non-stationary due to the\neffects of anthropogenic climate change. Rather than try to directly develop a covariate\nas an explicitly nonlinear function of time, it is simpler to use a more physically-based\n\u201ccovariate\u201d as a linear source of non-stationarity. A simple choice is a temporally-smoothed\nglobal mean temperature anomaly (xt). A 13-point filter (Solomon et al. 2007) removes\nsome of the natural modes of variability that may affect central US summer temperature\nbut retains the anthropogenic warming signal. This function is then a non-linear proxy for\ntime that we can use as a covariate in a linear representation of the location parameter,\n\u00b5t = \u03b20 + \u03b21xt. We note that adding additional covariates to account for other known\nphysical dependencies, such as an El Nin\u02dco/La Nin\u02dca index, may improve the quality of the\nfitted distribution but as such is outside the scope of this study. Finally, as the model\nunder the counterfactual scenario is presumed to be stationary, we do not use a covariate\nin fitting that dataset. In this study, we computed the Akaike Information Criterion (AIC)\nto compare stationary and non-stationary models for the observations and actual scenario\noutput, where the model with the smaller AIC value is preferred. For the actual scenario,\nthe non-stationary model was strongly preferred based on AIC. However, we found that the\nAIC for the stationary model for observations (152.93) was slightly smaller than the AIC for\nthe non-stationary model for the observations (154.14). This is a consequence of the very\nsmall observed warming trend in the selected region. Despite this preference for omitting\nthe covariate, we use the non-stationary model for the observational data to be consistent\nwith the statistical representation for the actual scenario output.\nThe PP model requires the choice of an arbitrary threshold, with only data above the\nthreshold used to fit the model, as described in the Appendix. There are few rigid guidelines\nfor how high the threshold should be. It must be high enough to be in the \u2018asymptotic\u2019\nregime, i.e., that the assumptions of the extreme value statistical theory are satisfied, but low\nenough that enough points from the original sample are retained to reduce the uncertainty\nin estimating the parameters of the statistical model. Here we use the 80th percentile of the\nvalues in each dataset. Standard diagnostics (Coles 2001; Scarrott and MacDonald 2012),\nincluding mean residual life plots shown in Figure 4, suggest this is a reasonable choice.\nGiven the choice of a threshold and covariates, the PP-based extreme value distribution\nis straightforward to fit using maximum likelihood methods, providing estimates of \u00b5t (i.e.,\n\u03b20 and \u03b21), \u03c3, and \u03be. To fit the model, we use the fevd routine of the R package, extRemes\n6\n(Gilleland and Katz 2011). Note that for seasonal data such as for this case study, the\ntime.units argument should be specified to be \"m/year\", where m is the number of obser-\nvations in each block of data. It is useful to treat a \u2018block\u2019 as a year so that return levels\ncan be considered to be the value exceeded once in 1/p years. When using an ensemble\nof model runs, we have multiple replicates for each year, so m is the number of ensemble\nmembers (e.g., m = 5 for the all forcings ensemble). To implement steps 2 and 3 of the\nquantile bias correction method, we need to be able to calculate both a return level given a\nspecified probability z\u02c6A(p\u02c6O) and a probability given a specified return level, p\u02c6C(z\u02c6A). Both of\nthese values are obtained from the estimated parameter values as shown in the Appendix,\nequations (A2) and (A3).\nc. Uncertainty Quantification of the Risk Ratio\nWe have presented an approach to estimating the RR using the quantile bias correction\nmethod. We turn now to accounting for the various sources of uncertainties in the estimate of\nRR produced by this method. Here we focus on uncertainty from statistical estimation of the\nvarious probabilities; structural uncertainty that arises from using model simulations in place\nof the real climate system is of course important but is beyond the scope of our work. More\nprecisely, the uncertainties in estimating the risk ratio can be separated into three sources:\nuncertainty in estimating pO using the observations (step 1), uncertainty in estimating zA\nusing the actual scenario model output (step 2), and uncertainty in estimating pC using the\ncounterfactual scenario output (step 3). In this section we quantify the uncertainty in the\nrisk ratio considering the second and third sources of uncertainty. With regard to the first\nsource, for now we consider the magnitude of the extreme event to be a given, as a precise\nestimate of pO will be shown to not be absolutely necessary to make a confident attribution\nstatement. Rather, we believe the sensitivity of the estimate of RR to a defensible range of\nzO values (and pO) is critical to confident extreme event attribution.\nIn our uncertainty analysis below, we condense our notation of the fitted extreme value\ndistributions to \u03b8A = (\u03b20A , \u03b21A , \u03c3A, \u03beA) and \u03b8N = (\u00b5C , \u03c3C , \u03beC), where A again indicates the\nmodel under the actual scenario and C the model under the counterfactual scenario. We\nconsider several approaches to deriving a confidence interval for the RR. Given that the RR\nis non-negative and its sampling distribution is likely to be skewed, we work on the base-2\nlogarithmic scale.\nA standard approach to estimating the standard error of a non-linear functional of pa-\nrameters in a statistical model is to use the delta method and then derive a confidence\ninterval using a normal approximation (Sections 5.5.4 & 10.4.1, Casella and Berger (2002)).\nAnother possibility is to use the bootstrap to either estimate the standard error or directly\nestimate a confidence interval (Section 10.1.4, Casella and Berger (2002)). However, both of\nthese methods fail when the estimated RR is infinity. The bootstrap uncertainty estimate\nwill also pose difficulties if some of the bootstrap datasets produce estimated risk ratios\nthat are infinity. This outcome is quite likely if the extreme value distribution of the model\noutput under the counterfactual scenario is bounded and the magnitude of z\u02c6A is close to that\nbound. Therefore, after a brief discussion of the delta method and the bootstrap, we develop\nan alternative confidence interval by inverting a likelihood ratio test (LRT) and propose this\nis as a general approach to estimating a lower bound of RR.\n7\n(i) Delta Method\nIn this uncertainty analysis, we estimate the log risk ratio and logRR = f(\u03b8) as a function\nof the parameter vector \u03b8 = (\u03b8A, \u03b8C). The delta method uses an analytic approximation by a\nfirst-order Taylor series expansion: f(\u03b8\u02c6) \u2248 f(\u03b8) +Of(\u03b8)T (\u03b8\u02c6\u2212 \u03b8), where Of is a vector of the\npartial derivatives of f and \u03b8\u02c6 is the maximum likelihood estimate of \u03b8. Taking the variance\nof both sides of the Taylor approximation above, the delta method gives that\nV\u0302ar(log R\u0302R) = V\u0302ar[f(\u03b8\u02c6)] \u2248 Of(\u03b8\u02c6)TCov(\u03b8\u02c6)Of(\u03b8\u02c6). (2)\nThe variance-covariance matrix of \u03b8\u02c6, Cov(\u03b8\u02c6) is based on the matrix of second derivatives\nof the likelihood function. The standard error is s.e.(log R\u0302R) =\n\u221a\nV\u0302ar(log R\u0302R) and the\ncorresponding 95% confidence interval for logRR is(\nlog R\u0302R\u2212 1.96 s.e.(log R\u0302R), log R\u0302R + 1.96 s.e.(log R\u0302R)). (3)\nThe delta method relies on the approximate linearity represented by the Taylor approxi-\nmation and approximate normality of the distribution of the maximum likelihood estimates.\nIn particular, the delta method will not perform well when the sampling distribution for\nlog R\u0302R is skewed, which will be a particular concern for large values of R\u0302R, as the sampling\ndistribution of p\u02c6C is bounded below by zero.\n(ii) Bootstrap Method\nOur bootstrap procedure attempts to reflect the structure of the climate model outputs\nin the resampling procedure that produces bootstrapped datasets. To generate a boot-\nstrap dataset, we first resample with replacement from the set of ensemble members, as the\nensemble members are independent realizations of the climate state. In addition, for each\nresampled ensemble member, we resample years with replacement from the years represented\nin the dataset. This second type of resampling is a block bootstrap that is justified by the\nlow correlation in seasonal climate from year to year. Note that by resampling both ensemble\nmembers and years, we reduce the discreteness in approximating the sampling distribution\nthat would occur from only resampling from the small number of ensemble members. How-\never, note that in our example, results were similar when either excluding or including the\nresampling of years.\nBy repeating the resampling procedure, we produce bootstrap datasets, D1, \u00b7 \u00b7 \u00b7 , DB\nwhere B is the bootstrap sample size, e.g., B = 500. For example, for the actual sce-\nnario, we resample with replacement from the five ensemble members and with replacement\nfrom the 112 years and the associated smoothed global temperature values. We obtain boot-\nstrap samples with analogous resampling for the counterfactual scenario. The return levels,\nz\u02c6\n(1)\nA , z\u02c6\n(2)\nA , \u00b7 \u00b7 \u00b7 , z\u02c6(B)A , are computed from the bootstrapped samples for the actual scenario for\nthe fixed probability p\u02c6O. Pairing each bootstrapped return level estimate from the actual\nscenario with a bootstrapped dataset from the counterfactual scenario, we obtain boot-\nstrapped probabilities p\u02c6\n(1)\nC (z\u02c6\n1\nA), p\u02c6\n(2)\nC (z\u02c6\n2\nA), \u00b7 \u00b7 \u00b7 , p\u02c6(B)C (z\u02c6BA ) of exceeding the bootstrapped return\nlevels. We can then calculate log R\u0302R\n(1)\n, log R\u0302R\n(2)\n, \u00b7 \u00b7 \u00b7 , log R\u0302R(B), which allows us to estimate\nthe sampling distribution of log R\u0302R. From this, one can obtain a bootstrap standard error or\n8\nconfidence interval for the logRR via standard methods. For the basic bootstrap confidence\ninterval of logRR, we use the 2.5th and 97.5th percentiles of the bootstrapped values for\nlog R\u0302R\n(b)\n, b = 1, . . . , B, to compute the 95% confidence interval:(\nlog R\u0302R\u2212 (log R\u0302R(b).975 \u2212 log R\u0302R), log R\u0302R\u2212 (log R\u0302R\n(b)\n.025 \u2212 log R\u0302R)\n)\n. (4)\n(iii) Method of Inverting a Likelihood Ratio Test\nThe delta method fails when p\u02c6C = 0 (R\u0302R = \u221e) as it relies on asymptotic normality,\nand the bootstrap method fails for p\u02c6C = 0 and can fail to varying degrees when p\u02c6C is very\nsmall and one obtains log(R\u0302R)(b) = \u221e for one or more bootstrap samples. Hansen et al.\n(2014) discussed the case of p\u02c6C = 0 under the counterfactual scenario in the context of event\nattribution and suggested a one-sided confidence interval for attributable risk using station-\nary Poisson processes in the setting where probabilities are estimated simply by empirical\nproportions. Here we propose a likelihood ratio test-based method to find a lower bound\nfor RR that can be employed when extreme value statistics are used. We note that a lower\nbound is actually more relevant for making an attribution statement than a point estimate\nof RR as it encapsulates both the potential magnitude of the risk ratio and our uncertainty\nin estimating it.\nA standard approach to finding a confidence interval is to invert a test statistic (Casella\nand Berger 2002). The basic intuition is that for a hypothesized parameter value, \u03b80, if we\ncannot reject the null hypothesis that \u03b8 = \u03b80 based on the data, then that \u03b80 is a plausible\nestimate for the true value of \u03b8 and should be included in a confidence interval for \u03b8. A\nconfidence interval is then constructed by taking all values of \u03b80 such that a null hypothesis\ntest of \u03b8 = \u03b80 is not rejected.\nThe likelihood ratio test (Sections 9.2.1 & 10.3.1, Casella and Berger (2002)) compares\nthe likelihood of the data based on the MLE (i.e., the maximized likelihood estimate) to the\nlikelihood of the data when restricting the parameter space (which in the notation above can\nbe expressed as setting \u03b8 = \u03b80). If the null hypothesis is true then as the sample size goes\nto infinity, twice the log of the ratio of these two likelihoods has a chi-square distribution\nwith \u03bd degrees of freedom. \u03bd is equal to the difference in the number of parameters when\ncomparing the original parameter space to the restricted space. The hypothesis test of \u03b8 = \u03b80\nis rejected when twice the log of the likelihood ratio exceeds the 1 \u2212 \u03b1 quantile of the chi-\nsquare distribution, which would be the 95th percentile (i.e., \u03b1 = 0.05) for a 95% confidence\ninterval.\nSpecifically, we are interested in the plausibility of RR = pA\npC\n= r0 versus the alternative\nthat RR = pA\npC\n> r0 where r0 is a non-negative constant, so it would be natural to derive a\none-sided confidence interval, RR \u2208 (RRL,\u221e), that gives a lower bound, RRL, on the risk\nratio. The likelihood ratio test we use here is one where the restricted parameter space sets\nRR = r0. Under this null hypothesis, which is equivalent to pC = pA/r0, we construct the\nconstrained likelihood function by letting \u03b20A , \u03b21A , \u03c3A, \u03beA, \u03c3C and \u03beC be free parameters\nand setting\n\u00b5C = zA(\u03b20A , \u03b21A , \u03c3A, \u03beA) +\n\u03c3C\n\u03beC\n{\n1\u2212 (\u2212 log(1\u2212 pA/r0))\u2212\u03beC},\n9\nwhere zA is the return level corresponding to probability of exceedance under the actual\nscenario and pA is based on p\u02c6O or chosen in advance without directly making use of the\nobservations. This likelihood ratio test has one degree of freedom, corresponding to the\nrestriction on \u00b5N in the constrained likelihood. The joint likelihood for the model output\nfrom both the actual scenario and counterfactual scenario can be expressed as\nL(\u03b8A, \u03b8C) \u221d exp\n{\n\u2212 1\nnyA\nnA\u2211\ni=1\n[\n1 + \u03be\n(\nu\u2212 \u00b5tiA\n\u03c3A\n)]\u22121/\u03beA\n+\n} mA\u220f\ni=1\n\u03c3\u22121A\n[\n1 + \u03beA\n(\nxi \u2212 \u00b5tiA\n\u03c3A\n)]\u22121/\u03beA\u22121\n+\n\u00d7 exp\n{\n\u2212 nC\nnyC\n[\n1 + \u03beC\n(\nu\u2212 \u00b5C\n\u03c3C\n)]\u22121/\u03beC\n+\n} mC\u220f\nj=1\n\u03c3\u22121C\n[\n1 + \u03beC\n(\nxj \u2212 \u00b5C\n\u03c3C\n)]\u22121/\u03beC\u22121\n+\nwhere mA is the number of exceedances (out of the total of nA observations) for the actual\nscenario and mC the analogous quantity for the counterfactual scenario. Thus, the lower\nbound of RRL = minRR is found by finding the smallest value r0 such that\n2[logL(\u03b2\u02c60A , \u03b2\u02c61A , \u03c3\u02c6A, \u03be\u02c6A, \u00b5\u02c6C , \u03c3\u02c6C , \u03be\u02c6C ;x)\u2212 logL(\u03b2\u02c60A , \u03b2\u02c61A , \u03c3\u02c6A, \u03be\u02c6A, \u03c3\u02c6C , \u03be\u02c6C ;x,RR = r0)] < 3.841,\n(5)\nwhere 3.841 is the 95th percentile of a chi-square distribution with one degree of freedom.\nNumerically this can be solved by one dimensional minimization subject to the constraint\nfor the condition (5). The simplest way to do this is to move the constraint into the objective\nfunction and minimize an unconstrained problem. The new unconstrained objective function\nis\nr0 + c \u00b7 I(\u03bb(r0) > 3.841)\nwhere c is set to be a large number (mathematically c =\u221e), \u03bb(\u00b7) is twice the log of the likeli-\nhood ratio, and I(\u00b7) is an indicator function that evaluates to one if the inequality is satisfied\nand zero if not. The resulting objective function is not continuous, hence many standard\noptimization techniques are not applicable. One that can be used here is \u201cgolden section\nsearch\u201d (particularly if the objective function is modified slightly to be unimodal \u2013 albeit still\ndiscontinuous). In R, we use the optimize function. This function is designed for continuous\nobjective functions as it combines golden section search with parabolic interpolation, but it\nseems to work reasonably well in our analyses.\n4. Results\nIn this section we apply our proposed methodology to the central US heatwave event. The\nanalysis relies on estimation of the probabilities pO and pC and the adjusted event magnitude\nzA. As described in the previous section, we use the smoothed global mean temperature\nanomaly as a covariate to account for non-stationarity in temperature extremes in both the\nobservations and the model output under the all forcings scenarios. The smoothed global\nmean temperature anomalies are plotted on Figure 2. Table 1 gives the parameter estimates\nfrom fitting the PP model to observations and to the model output from both scenarios.\nNote that the estimated shape parameters (\u03be\u02c6) are all negative, indicating that the fitted\ndistributions are bounded.\nAs shown in Table 1, the estimated probability, p\u02c6O, of exceeding the observed extreme\nvalue of 2.467 is 0.032. Following the proposed quantile bias correction method, we set\n10\np\u02c6A = 0.032 and, based on the fitted PP model for the actual scenario, estimate the return\nlevel as z\u02c6A = 4.842. Then, using the fitted PP model for the counterfactual scenario, the\nestimated probability, p\u02c6C , of an event as or more extreme than zC = 4.842 is 1.5e-08. The\ncorresponding estimated logarithm of risk ratio is 21.0 (or RR \u2248 2, 100, 000), indicating a\nvery large increase in probability of a heatwave due to human influence. Figure 3 graphically\nillustrates the quantile bias correction methodology for this particular case study. Without\nthe bias correction, one would obtain p\u02c6A = 0.657 and p\u02c6C = 0.132, giving an estimated RR\nof approximately 5, which is quite different than the estimate with the bias correction. Note\nthat the observed event is not extreme in the model simulations under the actual scenario,\nwhich suggests that without bias correction we would be inappropriately be estimating a\nRR from a different part of the distribution than is of interest based on the observations.\nThe uncertainty in estimating RR with the quantile bias correction is quantified using\nthree methods: the delta method, the bootstrap, and our suggested likelihood ratio test-\nbased interval; Table 2 shows 95% confidence intervals for logRR from each method. As\ndiscussed in Section 3c, both the delta method and the bootstrap face difficulties when the\nestimated probability under counterfactual scenario is near zero, as it is here. In this example,\nthe bootstrap resamples often produce estimates of large return levels under the actual\nscenario that correspond to estimating probabilities of zero under counterfactual scenario.\nThe result is that many of the bootstrap datasets (246 of the 500) have estimates of logRR\nthat are infinity, but these bootstrap estimates cannot be sensibly included in the estimate\nof the bootstrap confidence interval. Hence, the confidence interval in Table 2 is calculated\nbased only on the finite values, but we cannot expect this to provide a reliable estimate of\nthe uncertainty.\nInstead, we focus on the likelihood ratio-based interval described in the previous section.\nWe apply our method by inverting a LRT in two ways. First we ignore uncertainty in z\u02c6A\nand consider only uncertainty in p\u02c6C , and second we consider uncertainty in both z\u02c6A and p\u02c6C\n(note that when we consider only uncertainty in p\u02c6C , one can derive a LRT-based interval\nanalogously to that derived in Section 3c).\nThe estimated lower bound, when considering both sources of uncertainties, is 4.0 (i.e.,\n16.1 on the original scale of the risk ratio), which indicates strong evidence that the true risk\nratio is substantially elevated under actual scenario compared to counterfactual scenario. As\nexpected, the lower bound is lower (4.0) when considering both sources of uncertainty than\nwhen considering only uncertainty in p\u02c6C (4.3).\nIn Section 3c, we argued that a precise event magnitude and corresponding pO is not\nnecessary to making confident event attribution statements. Rather, the sensitivity of the\nrisk ratio to a plausible range of extreme event definitions is essential. Table 3 shows the\nsensitivity of the risk ratio and its lower bound to various values of pO = pA. Critically, while\nthe estimate of the risk ratio varies dramatically as one varies the event definition, with the\nestimated risk ratio as large as infinity, the lower bound from the one-sided confidence interval\nis quite stable for a wide range of event definitions. This is a critically important component\nto the confident event attribution statement: \u201cFor the summer 2011 central US heat wave,\nanthropogenic changes to the atmospheric composition caused the chance of the observed\ntemperature anomaly to be increased by at least a factor of 16.1.\u201d Of course this statement\nis conditional on the climate model accurately representing relative changes in probabilities\nof extreme events under the different scenarios after the quantile-based correction.\n11\n5. Conclusion\nWe present an approach to extreme event attribution that addresses differences in the\nscales of variability between observations and model output using the methodology of quantile-\nbased bias correction in the context of a formal statistical treatment of uncertainty. The\ncorrection rescales matching quantiles between the observations and the models to obtain an\nevent in realistically-forced climate model simulations of corresponding rarity to the actual\nextreme weather or climate event of interest. We develop a procedure for estimation and for\nquantifying uncertainty in the risk ratio, a measure of the anthropogenic effect on the change\nin the chances of an extreme event. In particular we calculate a lower bound on the risk\nratio by inverting a likelihood ratio test statistic that can be used even when the estimated\nprobability of the event is zero or near-zero in climate model simulations of a hypothetical\nworld without anthropogenic climate change. This lower bound provides the key element\nin constructing confident attribution statements about the human influence on individual\nextreme weather and climate events.\nWe caution that bias correction can mask serious errors and is not a replacement for\nexpert judgment and physical insight into the source of the bias between model and ob-\nservation. For instance in our case study, it is well known that extreme temperatures in\nTexas and Oklahoma are associated with the La Nin\u02dca phase of ocean surface temperatures.\nThe statistical methods presented here could account for this source of bias by including an\nEl Nin\u02dco/La Nin\u02dca index as a covariate in the statistical model for event probabilities in the\nmodel dataset (see Section 3b) and bias correct the index rather than directly bias correcting\nthe distribution for the variable of interest. Pursuing such ideas is beyond the scope of our\nwork here but could lead to an approach that offers more insight into the source of bias and\nprovide a physically-based justification for the bias correction.\nThe lower bound on the risk ratio estimated using our proposed method implies a substan-\ntial increase in the probability of reaching or exceeding the observed extreme temperature\nof 2011 central US heatwave event under human-influenced climate change. However the\nprecise probability and magnitude of the observed extreme event is not a key component\nin extreme event attribution analyses. We explored the sensitivity of the lower bound of\nthe risk ratio to various definitions of the event (i.e., probabilities corresponding to different\nmagnitudes of extreme events) and found that the lower bound of the risk ratio confidence in-\nterval is more stable than point estimates of the risk ratio. As a result, confident attribution\nstatements about the minimum amount of anthropogenic influence on extreme events are\nmore readily constructed than statements about the most likely amount of anthropogenic\ninfluence. We also maintain that such more conservative statements are more consistent\nwith the vast literature of attribution statements about the human influence on trends in\nthe average state of the climate.\nAcknowledgments.\nThis research was supported by the Director, Office of Science, Office of Biological and\nEnvironmental Research of the U.S. Department of Energy under Contract No. DE-AC02-\n05CH11231 as part of their Regional and Global Climate Modeling Program and used re-\nsources of the National Energy Research Scientific Computing Center (NERSC), also sup-\nported by the Office of Science of the U.S. Department of Energy, under Contract No.\n12\nDE-AC02-05CH11231. This document was prepared as an account of work sponsored by\nthe United States Government. While this document is believed to contain correct infor-\nmation, neither the United States Government nor any agency thereof, nor the Regents of\nthe University of California, nor any of their employees, makes any warranty, express or\nimplied, or assumes any legal responsibility for the accuracy, completeness, or usefulness of\nany information, apparatus, product, or process disclosed, or represents that its use would\nnot infringe privately owned rights. Reference herein to any specific commercial product,\nprocess, or service by its trade name, trademark, manufacturer, or otherwise, does not nec-\nessarily constitute or imply its endorsement, recommendation, or favoring by the United\nStates Government or any agency thereof, or the Regents of the University of California.\nThe views and opinions of authors expressed herein do not necessarily state or reflect those\nof the United States Government or any agency thereof or the Regents of the University of\nCalifornia.\n13\nAPPENDIX\nBackground for Modeling of Extreme Values\nExtreme value theory (EVT) provides a statistical theory of extreme values that models\nthe tail of a probability distribution. Univariate extreme value theory to study so-called\nblock maxima (e.g., annual or seasonal maxima of daily data) is well-developed. The theory\nshows that the distribution of the maxima converges to a distribution function G,\nG(x;\u00b5, \u03c3, \u03be) = exp\n{\n\u2212\n(\n1 + \u03be\nx\u2212 \u00b5\n\u03c3\n)\u22121/\u03be\n+\n}\n, (x+ = max(0, x)) (A1)\nthat is known as the generalized extreme value (GEV) distribution. The parameters \u00b5, \u03c3, and\n\u03be are known as the location, scale and shape parameters, respectively. The shape parameter,\n\u03be, determines the type of tail behavior \u2014 whether the tail is heavy (\u03be > 0), light (\u03be \u2192 0), or\nbounded (\u03be < 0), implying a short-tailed distribution. For example, analysts usually obtain\na negative estimated shape parameter for temperature data and a non-negative estimated\nshape parameter for precipitation data.\nReturn levels are quantiles \u2014 a return level z such that P (Z > z) = p implies that the\nlevel z is expected to be exceeded once every 1/p years on average. The probability p of\nexceeding z is easily obtained in closed form, given \u00b5, \u03c3, and \u03be, based on the distribution\nfunction (A1),\np = 1\u2212 P (Z \u2264 z) = 1\u2212 exp\n{\n\u2212\n(\n1 + \u03be\nz \u2212 \u00b5\n\u03c3\n)\u22121/\u03be\n+\n}\n. (A2)\nAs a counterpart to this, given p, the return level is obtained by solving the equation P (Z >\nz) = p, which gives\nz = \u00b5\u2212 \u03c3\n\u03be\n{\n1\u2212 (\u2212 log(1\u2212 p))\u2212\u03be} (\u03be 6= 0). (A3)\nHowever, the block maxima approach only uses the maximum (or analogously the mini-\nmum when analyzing extreme low values) of blocks in time series data. An alternative that\ncan make use of more of the data is the peaks over threshold (POT) approach (Coles 2001;\nKatz et al. 2002). POT modeling is based on the observations above a high threshold, u. The\ndistribution of exceedances over the threshold is approximated by a generalized Pareto dis-\ntribution (GPD) as u becomes sufficiently large. In this approach, the limiting distribution\nof threshold exceedances is characterized by the following: for x > u,\nP (X \u2264 x|X > u) = 1\u2212\n(\n1 + \u03be\nx\u2212 u\n\u03c3u\n)\u22121/\u03be\n+\n. (A4)\nThe scale parameter \u03c3u > 0 depends on the threshold. As with the GEV distribution the\nshape parameter, \u03be, determines the tail behavior.\n14\nThe point process (PP) provides a closely-related alternative peaks over threshold ap-\nproach to the GPD that is convenient because the PP parameters can be directly related to\nthe GEV parameters and then the GEV equations above can be used to calculate return val-\nues and return probabilities. The corresponding likelihood of the threshold excesses can be\napproximated by a Poisson distribution with the intensity measure depending on \u00b5, \u03c3, and \u03be,\nwhere \u00b5, \u03c3, and \u03be are location, scale, and shape parameters equivalent to those in the GEV\ndistribution (A1). More precisely, for a vector of n observations X1, X2, \u00b7 \u00b7 \u00b7 , Xn standard-\nized under the conditions of GEV distribution, the point process on regions of (0, 1)\u00d7 [u,\u221e)\nconverges to a Poisson process with the intensity measure given by\n\u039b\n(\n[t1, t2]\u00d7 (x,\u221e)\n)\n= (t2 \u2212 t1)\n[\n1 + \u03be\n(\nx\u2212 \u00b5\n\u03c3\n)]\u22121/\u03be\n+\n. (A5)\nTaking m to be the number of observations above the threshold u (out of the total of n\nobservations), the likelihood function is\nL(\u03b8;x1, x2, \u00b7 \u00b7 \u00b7 , xn) \u221d exp\n{\n\u2212 n\nny\n[\n1+\u03be\n(\nu\u2212 \u00b5\n\u03c3\n)]\u22121/\u03be\n+\n} m\u220f\ni=1\n\u03c3\u22121\n[\n1+\u03be\n(\nxi \u2212 \u00b5\n\u03c3\n)]\u22121/\u03be\u22121\n+\n(A6)\nwhere ny is number of observations per year (e.g., ny = 5 for the all forcings ensemble and\nny = 12 for the counterfactual ensemble).\n15\nREFERENCES\nAllen, M. R., 2003: Liability for climate change. Nature, 421 (6926), 891\u2013892.\nBindoff, N., et al., 2013: Detection and Attribution of Climate Change: from Global to\nRegional In: Climate Change 2013: The Physical Science Basis. Contribution of Working\nGroup I to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change\n[Stocker, T.F., D. Qin, G.-K. Plattner, M. Tignor, S.K. Allen, J. Boschung, A. Nauels,\nY. Xia, V. Bex and P.M. Midgley (eds.)]. Cambridge University Press, Cambridge, United\nKingdom and New York, NY, USA.\nCasella, G. and R. L. Berger, 2002: Statistical Inference. 2d ed., Thomson Learning, Australia\nPacific Grove, CA.\nChristidis, N., P. A. Stott, A. A. Scaife, A. Arribas, G. S. Jones, D. Copsey, J. R. Knight,\nand W. J. Tennant, 2013: A new HadGEM3-A-Based system for attribution of weather-\nand climate-related extreme events. Journal of Climate, 26, 2756\u20132783.\nColes, S. G., 2001: An Introduction to Statistical Modeling of Extreme Values. Springer\nVerlag, New York.\nEdwards, L. M., M. J. Bunkers, J. T. Abatzoglou, D. P. Todey, and L. E. Parker, 2014:\nOctober 2013 Blizzard in western South Dakota [in \u201cExplaining Extremes of 2013 from a\nClimate Perspective\u201d]. Bulletin of the American Meteorological Society, 95 (9), S23\u2013S26.\nFischer, E. M. and R. Knutti, 2015: Anthropogenic contribution to global occurrence of\nheavy-precipitation and high-temperature extremes. Nature Climate Change, 5 (6), 560\u2013\n564.\nFurrer, E. M., R. W. Katz, M. D. Walter, and R. Furrer, 2010: Statistical modeling of hot\nspells and heat waves. Climate Research, 43, 191\u2013205.\nGilleland, E. and R. W. Katz, 2011: New software to analyze how extremes change over\ntime. Eos, 92 (2), 13\u201314.\nHansen, G., M. Auffhammer, and A. R. Solow, 2014: On the attribution of a single event to\nclimate change. J. Climate, 27, 8297\u20138301.\nHarris, I., P. D. Jones, T. J. Osborn, and D. Lister, 2014: Updated high-resolution grids of\nmonthly climatic observations \u2013 the CRU TS3.10 Dataset. Int. J. Climatol., 34, 623\u2013642,\ndoi:10.1002/joc.3711.\nHerring, S. C., M. P. Hoerling, T. C. Peterson, and P. A. Stott, (Eds.) , 2014: Explaining Ex-\ntreme Events of 2013 from a Climate Perspective. Bulletin of the American Meteorological\nSociety, 95 (9), S1\u2013S96.\nHoerling, M., et al., 2013: Anatomy of an extreme event. J. Climate, 26, 2811\u20132832.\n16\nJaeger, C. C., J. Krause, A. Haas, R. Klein, and K. Hasselmann, 2008: A method for\ncomputing the fraction of attributable risk related to climate damages. Risk Analysis,\n28 (4), 815\u2013823, doi:10.1111/j.1539-6924.2008.01070.x, URL http://dx.doi.org/10.\n1111/j.1539-6924.2008.01070.x.\nKatz, R. W., M. B. Parlange, and P. Naveau, 2002: Statistics of extremes in hydrology.\nAdvances in Water Resources, 25, 1287\u20141304.\nKharin, V. V. and F. W. Zwiers, 2005: Estimating extremes in transient climate change\nsimulations. J. Climate, 18, 1156\u20131173.\nLi, H., J. Sheffield, and E. F. Wood, 2010: Bias correction of monthly precipitation\nand temperature fields from Intergovernmental Panel on Climate Change AR4 mod-\nels using equidistant quantile matching. J. of Geophys. Res., 115 (D10101), doi:\n10.1029/2009JD012882.\nMaurer, E. P. and H. G. Hidalgo, 2008: Utility of daily vs. monthly large-scale climate data:\nan intercomparison of two statistical downscaling methods. Hydrol. Earth Syst. Sci., 12,\n551\u2013563.\nMeehl, G. A., J. M. Arblaster, and G. Branstator, 2012: Mechanisms contributing to the\nwarming hole and the consequent U.S. East\u2013West differential of heat extremes. Journal\nof Climate, 25, 6394\u20136408.\nMin, S.-K., X. Zhang, F. Zwiers, H. Shiogama, Y.-S. Tung, and M. Wehner, 2013: Multi-\nmodel detection and attribution of extreme temperature changes. Journal of Climate, 26,\n7430\u20137451.\nPall, P., T. Aina, D. A. Stone, P. A. Stott, T. Nozawa, A. G. J. Hilberts, D. Lohmann, and\nM. R. Allen, 2011: Anthropogenic greenhouse gas contribution to flood risk in England\nand Wales in autumn 2000. Nature, 470 (7334), 382\u2013385.\nPalmer, T. N., 1999: A nonlinear dynamical perspective on climate prediction. J. Climate,\n12, 575\u2013591.\nPanofsky, H. A. and G. W. Brier, 1968: Some Applications of Statistics to Meteorology. The\nPennsylvania State University, University Park, PA, USA, 224 pp.\nPeterson, T. C., M. P. Hoerling, P. A. Stott, and S. Herring, (Eds.) , 2013: Explaining Ex-\ntreme Events of 2012 from a Climate Perspective. Bulletin of the American Meteorological\nSociety, 94 (9), S1\u2013S74.\nPeterson, T. C., P. A. Stott, and S. Herring, 2012: Explaining extreme events of 2011 from\na climate perspective. Bulletin of the American Meteorological Society, 93, 1041\u20131067,\ndoi:10.1175/BAMS-D-12-00021.1.\nScarrott, C. and A. MacDonald, 2012: A review of extreme value threshold estimation and\nuncertainty quantification. REVSTAT - Statistical Journal, 10 (1), 33\u201360.\n17\nSmith, R. L., 1989: Extreme value analysis of environmental time series: An application to\ntrend detection in ground-level ozone (with discussion). Statistical Science, 4, 367\u2013393.\nSolomon, S., D. Qin, M. Manning, Z. Chen, M. Marquis, K. B. Averyt, M. Tignor, and\nH. L. Miller, 2007: In Climate Change 2007: The Physical Science Basis. Contribution\nof Working Group I to the Fourth Assessment Report of the Intergovernmental Panel\non Climate Change. Cambridge University Press, Cambridge, United Kingdom and New\nYork, NY, USA.\nStone, D. A., C. J. Paciorek, P. Prabhat, P. Pall, and M. F. Wehner, 2013: Inferring the\nanthropogenic contribution to local temperature extremes. Proc. Natl. Acad. Sci. USA,\n110 (17), E1543.\nStott, P. A., D. A. Stone, and M. R. Allen, 2004: Human contribution to the European\nheatwave of 2003. Nature, 432 (7017), 610\u2013614.\nWolski, P., D. Stone, M. Tadross, M. Wehner, and B. Hewitson, 2014: Attribution of floods\nin the Okavango basin, Southern Africa. Journal of Hydrology, 511, 350\u2013358, doi:10.1016/\nj.jhydrol.2014.01.055.\nWood, A., L. R. Leung, V. Sridhar, and D. P. Lettenmaier, 2004: Hydrologic implications\nof dynamical and statistical approaches to downscaling climate model outputs. Climatic\nChange, 62, 189\u2013216.\n18\nList of Tables\n1 Parameter estimates from the point process model fitted to observations (top,\n1901-2012), actual scenario model output (middle, 1901-2012), and counter-\nfactual scenario model output (bottom, 100 years). The right column gives\nthe estimated return levels and/or probabilities calculated in the steps of the\nquantile bias correction method. The threshold, u, is the 80th percentile of\nvalues for each given dataset. 20\n2 Estimated logRR and corresponding confidence intervals using delta method,\nbootstrap resampling (B=500), and the proposed likelihood ratio test (LRT)-\nbased method giving a lower bound for the risk ratio. For the bootstrap, 246\nof the 500 bootstrap samples are excluded as the bootstrapped RR estimate\nis infinity. For the LRT-based approach, we consider two cases of uncertainty\nquantification: first uncertainty only in estimating pC , and second uncertainty\nin estimating both zA and pC . 21\n3 Sensitivity of results to definition of the event, i.e., different values of pO = pA. 22\n19\nTable 1. Parameter estimates from the point process model fitted to observations (top,\n1901-2012), actual scenario model output (middle, 1901-2012), and counterfactual scenario\nmodel output (bottom, 100 years). The right column gives the estimated return levels and/or\nprobabilities calculated in the steps of the quantile bias correction method. The threshold,\nu, is the 80th percentile of values for each given dataset.\nObservation location scale shape\nu = 0.856 \u03b2\u02c60 \u03b2\u02c61 \u03c3\u02c6 \u03be\u02c6 p\u02c6O = P (Z > 2.467)\nglobal mean tmp -0.802 0.404 1.250 -0.239 0.032\nModel (actual scenario) location scale shape\nu = 1.405 \u03b2\u02c60A \u03b2\u02c61A \u03c3\u02c6A \u03be\u02c6A z\u02c6A\nglobal mean tmp 1.263 1.382 0.926 -0.197 4.842\nModel (counterfactual) location scale shape\nu = 0.811 \u00b5\u02c6C \u03c3\u02c6C \u03be\u02c6C p\u02c6C = P (Z > 4.842)\nno trend (K = 0) 1.415 0.638 -0.179 1.503e-08\n20\nTable 2. Estimated logRR and corresponding confidence intervals using delta method,\nbootstrap resampling (B=500), and the proposed likelihood ratio test (LRT)-based method\ngiving a lower bound for the risk ratio. For the bootstrap, 246 of the 500 bootstrap samples\nare excluded as the bootstrapped RR estimate is infinity. For the LRT-based approach, we\nconsider two cases of uncertainty quantification: first uncertainty only in estimating pC , and\nsecond uncertainty in estimating both zA and pC .\nlog2 R\u0302R 21.0\nDelta method\n[16.8, 25.2]\nBootstrap method\n[12.2, 39.4]\nLRT-based method\nUQ for p\u02c6C [4.3, \u221e)\nUQ for z\u02c6A and p\u02c6C [4.0, \u221e)\n21\nTable 3. Sensitivity of results to definition of the event, i.e., different values of pO = pA.\npA z\u02c6A p\u02c6C log2 R\u0302R one-sided CI for log2RR lower bound of RR\npO = pA (\u03b1 = .05)\n0.200 3.7 2.8e-03 6.1 [3.0, \u221e) 8.0\n0.100 4.2 1.9e-04 9.1 [3.6, \u221e) 11.7\n0.050 4.6 3.1e-06 14.0 [3.9, \u221e) 14.8\n0.032 4.8 1.5e-08 21.0 [4.0, \u221e) 16.1\n0.023 5.0 0 \u221e [4.1, \u221e) 16.8\n0.010 5.3 0 \u221e [4.1, \u221e) 16.9\n22\nList of Figures\n1 Central United States region, 90\u25e6W to 105\u25e6W in longitude and 25\u25e6N to 45\u25e6N\nin latitude (bold rectangular area), covering the states of Texas and Oklahoma. 25\n2 Illustration of the mismatch in scales between observations and model output\nfor central US summer temperatures. Observed values for 1901-2012 (blue),\nmodel output under actual scenario for 1901-2012 (red) and model output\nunder counterfactual scenario for 100-year time period (green). The vertical\nlines show the 5-95% range of values for the different datasets. The larger black\ndot represents the observed value of 2.467 for 2011. The blue and red lines\nrepresent smoothed global mean temperature anomalies used as observational\nand actual scenario model output covariates, respectively. 26\n3 Demonstration of the quantile bias correction applied to the central US heat-\nwave example, showing estimated cumulative distribution functions of ob-\nserved (blue) and modeled datasets under actual scenario (red) and counter-\nfactual scenario (green). The blue dashed line shows the observed event, with\nthe horizontal red dashed line translating the observed event magnitude to\nthe equivalent magnitude under the actual scenario, holding p\u02c6O = p\u02c6A. For the\nevent magnitude indicated by the vertical red dashed line, the green dashed\nline indicates the probability under counterfactual scenario. The three colored\ndots represent the upper bounds of each distribution function, which occurs\nbecause with a negative shape parameter (as is estimated in these cases), the\nextreme value distribution has a finite upper bound. 27\n23\n4 Mean residual life plot for each dataset: (a) observations, (b) model output\nunder actual scenario, and (c) model output under counterfactual scenario.\nRed dashed lines represent the 75th, 80th, and 85th percentiles, respectively,\nas possible choices of thresholds. We chose the 80th percentile as a reasonable\nthreshold beyond which there are relatively linear trends. 28\n24\n-105 -100 -95 -90\n25\n30\n35\n40\n45\nLongitude\nLa\ntit\nud\ne Oklahoma\nTexas\nFig. 1. Central United States region, 90\u25e6W to 105\u25e6W in longitude and 25\u25e6N to 45\u25e6N in\nlatitude (bold rectangular area), covering the states of Texas and Oklahoma.\n25\n1900 1920 1940 1960 1980 2000\n-4\n-2\n0\n2\n4\n6\nYear\nA\nno\nm\nal\ny\nObs (1901-2012)\nACT (1901-2012)\nCF (001-100)\nFig. 2. Illustration of the mismatch in scales between observations and model output for\ncentral US summer temperatures. Observed values for 1901-2012 (blue), model output under\nactual scenario for 1901-2012 (red) and model output under counterfactual scenario for 100-\nyear time period (green). The vertical lines show the 5-95% range of values for the different\ndatasets. The larger black dot represents the observed value of 2.467 for 2011. The blue and\nred lines represent smoothed global mean temperature anomalies used as observational and\nactual scenario model output covariates, respectively.\n26\n-2 0 2 4 6\n0.\n0\n0.\n2\n0.\n4\n0.\n6\n0.\n8\n1.\n0\nX value\nP\nro\nba\nbi\nlit\ny\np^C=P(X>4.842)=1.503e-08\np^A=P(X>2.467)=0.032\n2.467 4.842\nObs\nACT\nCF\nFig. 3. Demonstration of the quantile bias correction applied to the central US heatwave ex-\nample, showing estimated cumulative distribution functions of observed (blue) and modeled\ndatasets under actual scenario (red) and counterfactual scenario (green). The blue dashed\nline shows the observed event, with the horizontal red dashed line translating the observed\nevent magnitude to the equivalent magnitude under the actual scenario, holding p\u02c6O = p\u02c6A.\nFor the event magnitude indicated by the vertical red dashed line, the green dashed line\nindicates the probability under counterfactual scenario. The three colored dots represent\nthe upper bounds of each distribution function, which occurs because with a negative shape\nparameter (as is estimated in these cases), the extreme value distribution has a finite upper\nbound.\n27\n-2 -1 0 1 2 3\n0.\n0\n0.\n5\n1.\n0\n1.\n5\n2.\n0\n2.\n5\nu\nM\nea\nn \nE\nxc\nes\ns\n(a)\n-2 0 2 4\n0\n1\n2\n3\n4\nu\nM\nea\nn \nE\nxc\nes\ns\n(b)\n-3 -2 -1 0 1 2 3 4\n0.\n0\n0.\n5\n1.\n0\n1.\n5\n2.\n0\n2.\n5\n3.\n0\nu\nM\nea\nn \nE\nxc\nes\ns\n(c)\nFig. 4. Mean residual life plot for each dataset: (a) observations, (b) model output under\nactual scenario, and (c) model output under counterfactual scenario. Red dashed lines\nrepresent the 75th, 80th, and 85th percentiles, respectively, as possible choices of thresholds.\nWe chose the 80th percentile as a reasonable threshold beyond which there are relatively\nlinear trends.\n28\n",
    "source": "'Elsevier BV'"
  },
  {
    "id": 17485099,
    "doi": null,
    "title": "Political attention to environmental issues: Analyzing policy punctuations in the Netherlands",
    "abstract": "One of the most dramatized features in Al Gore's movie The Inconvenient Truth is the effects of a rising sea-level in the Netherlands. The film is an example of how the mobilization of bias in the Netherlands resulted in sudden high levels of attention for climate change problems. We analyze agenda setting on Dutch environmental policy, using various policy issue datasets about parliamentary activities, media, and expert organizations and focusing on the interrelations between these policy venues. All datasets are coded by the same topic codebook. The findings show that interest in environmental issues is largely determined by the state of the economy, unexpected incidents, and the competition for attention with other issues in the political arena. We show that political interest in environmental issues has initially been flagging, since the environment was mostly seen as a European topic, and Europe has not been popular since the referendum on a European Constitution. However, once the climate change problem was translated to a national problem, popular attention increased enormously. We conclude that climate change framed as a European problem does not increase attention, nationalization of the problem does",
    "fullText": "1 \n \n \n \n \n \n \n \nPaper prepared for the 2nd Annual Meeting of the \nComparative Policy Agendas Conference \nThe Hague, 18-19 June 2009 \n \n \n \nPolitical Attention to Environmental Issues: \nAnalyzing Policy Punctuations in The Netherlands \n \n \nGerard Breeman, Wageningen University, Netherlands \nArco Timmermans, Montesquieu Institute and Leiden University, Netherlands \nJouke de Vries, Leiden University Campus Den Haag, Netherlands \n \n \n \n2 \n \nIntroduction \n \nWhen in 2007 Al Gore toured through Europe to promote his Inconvenient Truth, he found an \nattentive public in the Netherlands. After years of declining attention for the environment, \nGore\u2019s film on global warming created a spike of interest. Al Gore became so much a hype in \nthis country, that an inconvenient truth about his son caught in the possession of drugs \nreceived almost equal media coverage as the film. But attention to the film\u2019s message did not \npersist when the economy went down. In early 2009, opinion polls showed a decline in public \nconcern with the environment, despite an increasing urgency of the global warmng problem \nreported by international experts such as Nicolas Stern and national organizations for \nenvironmental policy advice. \nThe environment is not unique in this respect. Attention for other policy problems \nsuch as unemployment, crime and social tensions over immigration also rises and spreads \npublicly and politically and then drops and becomes a topic of much smaller communities of \nactors with professional stakes in the issue. Indeed, rather than unique, the phenomenon of \nrising and declining attention for the environment is itself related to the attention to other \nproblems. Such patterns of attention become more visible as the time perspective extends to \nlonger periods. A longer time perspective may reveal how different policy themes and more \nspecific subtopics come and go together, or crowd each other out. \nThus, for most issues considered over longer periods of time, broad attention is an \nexception rather than a rule. While this idea has informed studies of agenda setting in the \nUnited States and other countries since the work of Schattschneider (1960), Bachrach and \nBaratz (1962), Downs (1972), Cobb and Elder (1983) and Kingdon (1984), less is known \nabout how attention for an issue travels from one type of agenda to the other, and how such \ntravelling happens for larger sets of issues at the same time. Moreover, some venues of \nagenda setting may facilitate dramatization of issues and games of high politics, while others \nmay provide institutional equipment for attempts at depoliticization and formulation of \ntechnical definitions of the problem. \n3 \n \nThe most ambitious and comprehensive approach to study the process and content of \nagenda setting following this early work is the theory of punctuated equilibrium and the \nextensive empirical analysis of policy agendas developed by Baumgartner and Jones (1993; \nJones and Baumgartner 2005). Typically, this work does not focus on single issues alone, but \nconsiders the whole range of problems that publics and governments face, and analyzes how \ndifferent policy agendas in the spheres of politics, the media and the public expand and \ncontract over time. While initially this approach to policy agendas was confined to the United \nStates, recent contributions focus also on European countries and take a comparative \nperspective (Baumgartner et al 2006; Brouard et al 2009). \nThis paper follows this line of theoretical and empirical research on policy agendas \nand monitors the attention for environmental problems in the Netherlands, a country that has \nbeen known as an active agenda setter in this policy domain in Europe. The central question \nin our analysis is how much attention environmental issues received in public and political \narenas in the past two decades, and what conditions rises and declines of attention to this \ntheme. The empirical analysis includes the agenda of the national executive, parliament, the \nmedia, reports of expert organizations focusing on environmental issues, and environmental \nlegislation as output. We also map environmental attention relative to other main themes of \npublic policy. From an agenda-theoretical perspective, we analyze mechanisms producing ups \nand downs in attention to environmental problems over time, and changes from technical \nproblem definitions to more political and dramatic definitions or vice versa. In this theoretical \nperspective, we include not only policy venues for attention pushing and puilling, but also \nevents and incidents that may trigger agenda dynamics. One part of the analysis contains the \nlong term pattern of environmental attention relative to other main themes of public policy. \nThen we move on to focusing on more specific issues of environmental policy and analyze \nhow these issues travel through policy agendas, and with what effects in the policy making \nmachinery. \n \n \n4 \n \nA Theoretical Lens on Environmental Attention \n \nIn his early and often cited theoretical model of environmental attention, Downs (1972) \nposited that attention patterns are cyclical. Writing in the early 1970s, he predicted that the \nrising prominence of ecology in public and political debates in the United States would be \ntemporal and be followed by a decline. In Down\u2019s issue attention cycle, a \u2018pre-problem stage\u2019 \nis followed by discovery and political actors claiming they are able to solve the problem, and \nthen a stage of decreasing enthusiasm as problems appear to be more intractable than \nexpected or portrayed, and a public that becomes more concerned with other problems. As \nDowns put it, in this \u2018post-problem stage\u2019, \u201can issue that has be replaced at the center of \npublic concern moves into a prolonged limbo \u2013 a twilight realm of lesser attention or \nspasmodic recurrences of interest.\u201d While attention thus drops after a loss of public and \npolitical interest, the problem is latent until events or incidents trigger renewed attention. \nAttention thus often recycles over a longer period of time. \n Not all problems are equally sensitive to such ups and downs in attention. Problems \nmay not be directly visible, effects may \u2018creep\u2019 rather than \u2018crash\u2019 onto the agenda (Princen \nand Rhinard 2006), and some issues represent deeper social cleavages and are easier to \ndramatize and keep the public interested than others. Moreover, some problems are external \neffects of social or economic activities that most people would not readily sacrifice for their \nresolution. According to Downs, environmental problems have characteristics of issues for \nwhich attention is cyclic. In later work mapping environmental attention in the United States \nuntil recent times, the prediction of Downs appeared only partly true: public interest has \nshown clear peaks and declines (Guber 2001), but political attention did not disappear and \nsome stability in environmental policy production occurred after the initial build up of \ninstitutions endowed with this task (Baumgartner and Jones 1993: 87; Baumgartner 2006). \nDowns may have overstated the effects of opportunistic behavior of politicians in response to \nthe public mood, and understated the significance of institutionalization. \n \n5 \n \nAttention levels may depend in part on the nature of the problem, as Down argued, but \ndefining problems is itself a key element of agenda setting (Rocheford and Cobb 1994). As \nBaumgartner and Jones (1993) say, accounting for attention patterns requires the theoretical \nand empirical inclusion of different venues. Strategies of agenda access or denial are \nemployed for problem definition. The added theoretical value of Baumgartner and Jones\u2019s \nwork is that it links two key elements of attention cycles: the venues of agenda setting, and \nthe constructions of topic and tone made within these venues (Baumgartner and Jones 1993). \nPolicy venues are institutional sites of agenda setting, not only formal political arenas such as \nlegislatures and executives, but also the media, public opinion, bureaucracies, and fora for \nscientific expertise. Typically, venues and policy arenas have their own rules of access and \ninformation, and in this way they may facilitate a particular emphasis on topic and tone in \nagenda setting. Some may facilitate the spread of a popular and dramatic image of a problem, \nothers lead to more technical approaches to policy problems. Thus rising or declining \nattention and the substantive portrayal of problems go together with (and are the result of) \nshifts in prominence of policy venues of high and low politics. Focusing events in the external \nenvironment can trigger such shifts, but their impact depends on how actors within arenas \nattach value and meaning to them (Birkland 1997). For example, scientific alarm over threats \nto the environment in itself does not provoke immediate political attention \u2013 and still less \npolitical action. Such signals usually require repeated interaction between experts and policy \nmakers and amplification by the media to transcend what Downs called the \u2018pre-problem \nstage\u2019 and be taken home onto the political agenda. One reason why some problems are slow \nto move from low to high politics (or vice versa) is that relevant actors usually do not engage \nin extensive venue shopping and focus their agenda setting attempts only at particular arenas \n(Pralle 2003). \nThe idea that different venues of agenda setting facilitate or promote a particular \nconception of a policy problem is crucial to the evolution of environmental attention and the \ncyclical pattern that may (or may not) become visible over time. As Baumgartner and Jones \n(1993) argue in their theory of punctuated equilibrium, the stabilization and destabilization of \n6 \n \n\u2018policy monopolies\u2019 happens within and between venues. If we say that venues facilitate a \nparticular type of policy conception, this means that these venues have institutional properties \nconducive to the replication and aggregation of particular problem frames. These institutional \nproperties thus may produce a degree of friction in the responses to input signals, so that it \ntakes time before policy attention becomes visible and formal and priorities change (Jones \nand Baumgartner 2005). Issue attention in political systems at large or in smaller policy \nsubsystems requires the transfer of individual to collective frames (Baumgartner 2007). \nInstitutional conditions within policy venues regulate how particular individual frames of \nactors are turned into collective frames, just as institutions structure collective perceptions and \nsocial behavior more generally. When such shared frames resonate in other venues, a \ncascading of topic and tone takes effect. \nFor analyzing environmental policy agendas, the distinction between technical and \ndramatic frames is particularly useful. This distinction was presented by Nisbet and Huge \n(2006) in their analysis of the role of the media in framing agricultural biotechnology. It \nconnects well to the concepts of limitation and expansion of the scope of conflict in \nSchattschneider\u2019s original work (1960), and to the more institutional notions of \u2018low\u2019 and \n\u2018high\u2019 politics and the venues of attention associated to them. Technical frames stress rational \nand expert-oriented approaches to problems, and facilitate their decomposition for resolution. \nThey downplay or ignore political and emotional dimensions and present policy talk that is \nlimited to a group of experts and professionals with direct stakes in the issue. By contrast, \ndramatic frames play on collective emotion and expanded public and political debate over \nnormative arguments and causal stories of disaster, they link rather than disconnect sensitive \nissues, and they involve broad mobilization of popular support, for which the media are an \nimportant venue. \n \nThe theoretical lens in this paper thus combines a long term view of evolving environmental \nattention and the institutional sites where this attention is produced and frames for problem \ndefinition are applied and reproduced. As noted, analysis of the United States in the thirty \n7 \n \nyears after Downs shows punctuated equilibria in environmental attention, but with more \nemphasis on policy consolidation and frequent public upsurges than on steep declines \n(Baumgartner 2006). As in the United States, the policy history of environmental attention in \nthe Netherlands is more recent than the legacy of political attention in some other main policy \nfields. The analysis below provides systematic data on environmental policy agendas in order \nto see how attention has developed and whether technical and dramatic frames have become \nvisible in the portrayal of issues of ecology. More specifically, the data are used to determine \nwhether, as in the United States, policy punctuations in the Netherlands are most visible in \nupward movement, and are followed by periods of institutional consolidation. \nThe hypothesis on this is that institutional consolidation is visible through the \nproduction of environmental policy even at times of declining public and media attention. \nThis hypothesis not only draws on empirical findings on the United States but also on the \ninstitutional literature in which the Netherlands is characterized as a country with a tradition \nof political accommodation and depoliticization and corporatist policy making structures and \nclosed policy networks (refs.). This literature suggests that political responses after \u2018alarmed \ndiscovery\u2019 involve systematic attempts at depoliticization and limitation of the scope of \ndebate. \n \n \n \nEmpirical Patterns of Environmental Attention \n \nMedia Attention \nFigure 1 shows the changing attention for environmental issues in one the Dutch prominent \nnewspapers: the NRC. On average, the attention for environmental problems is 1.8 percent of \nall articles in our sample, but more interesting is the development visible in the figure: first a \nsteady decline since 1990, and since 2006 a clear spike. No surprise, this rise in 2006 was \ncaused by the presentation of An Inconvenient Truth. Media attention only started to take off \n8 \n \nafter the premiere of the film in the Netherlands. Earlier presentations of the film in the U.S. \nor in Cannes hardly had effect on attention. \n \nFigure 1 here \n \nThe general trend is clear and shows one major policy change, which was triggered by a \nmajor focusing event: a dramatizing film. The film was coded as a climate change subtopic \nand the disaggregated graph shows clearly that this particular subtopic accounts to the overall \nincrease of the attention for environmental issues. However, this has not been the only major \nincrease of attention over the period 1990-2007. Figure 2 shows all changes in percentages \nbetween environmental subtopics from year to year. The high peak in the centre indicates that \nattention for many subtopics did not change much from one year to the next. The graph\u2019s tails \nshow the major changes in attention: the left tail represents declining attention, the right rising \nattention. Thus, there have been more major shifts, other than the two year increasing \nattention of climate change, which was triggered by Al Gore\u2019s film. \n \nFigure 2a here \n \nFigure 2b here \n \nMore important than changes in subtopics within the main theme of environmental policy are \nchanges in attention for the environment relative to other themes reported in the NRC. Figure \n2b shows this proportional attention. There has been, for instance, one major shift of 1.5 \npercent, which was the increased attention between 2006 and 2007 due to Gore\u2019s film. The \nkurtosis is 2.90, which means that the distribution is somewhat peeked (a normal distribution \nhas a kurtosis of 0). One can also do the same at disaggregated level, which means that all \nchanges per subtopic are calculated against the total amount of newspaper articles per year. In \nthat case the kurtosis is 3.28 (n=270). \n9 \n \nIn punctuated equilibrium theory, kurtosis is used as an important indicator. \nCharacteristic for periods with long term policy stability are the large numbers of incremental  \npolicy changes. This results in high spikes in the middle of a frequency distribution. The \nperiods with major policy shifts, the punctuations, on the other hand, result in small numbers \nof large percentage changes on either side of the spike (Jones and Baumgartner 2005). Hence, \nhigher kurtosis means that policies do not change easily (due to different types of friction), \nbut if they change, this is drastic. \nBesides the increasing attention to climate change policies triggered by Gore\u2019s film, \nalso UN climate conferences and specific dramatic events were relevant. The spike in \n1991/1992 was caused by the (preparations of the) conference in Rio de Janeiro. The \nincreased attention in 1995 was caused by widespread protests of Dutch farmers against \nmanure policies. The relative spike in 1997 was triggered by the Kyoto conference. The \nelevated attention during the period 1999-2000 was induced by the UN climate change \nconferences in Bonn (2000) and The Hague (2001). The rising attention in 2003 was caused \nby a couple of incidents in that year with polluted asbestos vessels. And the attention in 2005 \nwas a result of law suits of an environmental NGO against the government for not solving the \nproblem of the most polluted street in the Netherlands, which was the city of The Hague, the \nlocation of the national government. \nThese observations support the expectation that media attention is triggered largely \nby events that are easily dramatized, as the film and the following hype around Al Gore , the \nasbestos incidents, and the air pollution events at local level illustrate. In addition, we also \nobserve that attention for environmental issues in the media was triggered if Dutch political \npersonalities took a leading position in international conferences. Figure 3 shows the total \namount of attention in the NRC to U.N. conferences, showing that the less important \nconferences in Bonn and The Hague were displayed more extensively in the national media \nthan the broad and ambitious climate change conferences in Rio de Janeiro, Kyoto, and Bali. \nBonn and The Hague obtained more attention in the Dutch media because they were chaired \nby a former Dutch minister of development aid, Jan Pronk. \n10 \n \n \nFigure 3 here \n \nPunctuations and Stability in Political Attention \nCharacteristic of punctuated equilibria in political attention and policy is the alternation \nbetween periods of stability and short episodes of relatively drastic change. The analysis in \nthe previous section has shown how media attention for environmental issues follows this \npattern of periodic punctuation. Below we move on to analyzing the level of punctuation of \nthe political agenda. \nFigure 4 shows the results of the proportion of attention to environmental issues in \ncoalition agreements between 1963 and 2007. Coalition agreements are are written at the \nbeginning of a new governmental term, and in case a government collapses, a new agreement \nis made (since 1967 this always goes together with new parliamentary elections). The early \nagreements did not mention the environment, 1971 was the first year in which a new \ngovenment devoted attention to it, largely in response to the international emergence of issues \nof environment and nature on the agenda. This appeared to be an isolated spike, however, as \nthe following government displayed far less policy ambition at the outset \u2013 that is, when it \nbegan its term in office in 1973. The agreement of 1977 carried the effects of the first oil \ncrisis and also made reference to the report Limits to Growth by the Club of Rome. The main \npolitical attention spike however occurred in 1989, a key year in the history of Dutch \nenvironmental policy, and it resulted from the presentation of the first National \nEnvironmental Plan, a coalition crisis over an environmental policy issue, and the first UN \nclimate change conference in the Netherlands. But again attention slipped away from the next \ngovernment formation table in 1994, nor did it reach the level of 1989 in following years. The \nups and downs were modest, shifting between some 1 and 3 percent of total policy attention \nin coalition agreements. In 1998 and 2003 there was some increased attention for CO2 \nproblems and climate change. \n \n11 \n \nFigure 4 here \n \nFigure 5 shows the results of the amount of attention for environmental issues in Queen\u2019s \nspeeches presented annually and containing also plans for governmental action. The figure \nshows three spikes of attention in the 1970s, one major rise in 1989/1990, and a period of \nincreased attention between 1995 and 1998. After the initial neglect in the 1973 agreement, a \nseries of focusing events in the 1970s led to increases in attention which however did not \npersist. A period of low key attention in Queen\u2019s speeches occurred from the late 1970s to the \nlate 1980s. As noted above, the sharp rise high in attention in the late 1980s peaking in 1989 \nwas the result of different national and international events, and so was the rerising attention \nin the mid 1990s. After 2000, political attention expressed in annual Queen\u2019s speeches \nremained rather low key. This point is taken up further below. \n \nFigure 5 here \n \nThe Legislative Agenda \nFigure 6 shows an increased attention in law production for the environment in 1992, 1995, \n2004 and 2007. The increased attention in 1992 consisted of a set of new taxes on fuels and \nother environmentally harmful products. In 1995, the government instituted new \nenvironmental agencies and introduced several bills for protecting open waters against oil \npollution. In 2004 the international trade system for emission rights was accepted and in 2007 \nthe Dutch government accepted a series of EU directives on environmental policy. While \nthere are ups and downs, legislative attention between 1990 and 2007 appears to drop not a \nlow as environmental attention in governmental policy plans. \n \nFigure 6 here \n \n12 \n \nFigure 7 shows the extent to which in this body of environmental legislation explicit reference \nwas made to European environmental policy, and the figure indicates this explicitness has \ngrown with the relative increase in environmental legislation as a proportion of all legislative \noutput. The government does not talk much about the European Union, but in environmental \nlegislation European influences are acknowledged. \n \nFigure 7 here \n \nAnalyzing the ups and downs in environmental attention on different policy agendas such as \nthe media, the government and the legislative output it produces shows that the attention cycle \nposited by Downs is visible, but environmental problems do not entirely disappear from the \nagenda in the Netherlands. Moreover, legislation on this theme became more important in \nrelative terms when the policy talk was fading. The distributions of changes in attention \nsuggest that some venues in agenda setting are more sensitive to focusing events than others. \nBelow we analyze this point further, by considering what drives ups and downs in \nenvironmental attention. The conditions we consider relate to party influence, the flow of \nalarming signals and their spread into agenda setting venues, and the role of the European \nagenda in this domain. We also discuss how rises and declines in attention in public and \npolitical arenas relate to the type of frame used for portraying environmental problems. \n \n \nIs Environmental Attention Programmatic or Reactive? \n \nDespite frequent attempts by governments to portray ambitious goals of economic growth or \nrecovery and ecological sustainability as compatible, the Netherlands is no exception to the \ngeneral phenomenon that environmental attention follows the economic trend. If the economy \ngoes down, environmental issues become less prominent on public and political agendas. \nEnvironmental attention thus seems to be in part a matter of economic affordability. Further, \n13 \n \ncompared to other main policy themes that made their way in public and political arenas since \nthe 1960s (social affairs, health, education, rights and immigration), the ups and downs for \nenvironmental attention are quite profiled. This pattern suggests that mechanisms of trade off \nare operative. But what factors actually drive the up- and downward trend in political \nattention? \n \nGovernments and Policy Planning \nGovernments, and certainly those in parliamentary systems, usually develop programs and \nplans for their legislative term. While in a coalition system such as the Netherlands, these \ngovernment programs may not always follow electoral mandates as even the composition of \nthese governments is sometimes hard to predict from elections, they may however reflect the \npriorities and preferences of participant parties. In this way, governments may be expected to \nact on preformulated party platforms, and thus give more or less emphasis to problems and \nissues as their platforms indicate. This is the central premise of mandate theory and median \nmandate theory (McDonald and Budge 2005). While ecology is not a onedimensional \nconcept, political parties in a multiparty system such as the Netherlands can be mapped on an \nenvironmental policy dimension from left to right (refs.). This leads to the hypothesis that \nparties of the left lay more emphasis on this theme than parties of the right. As all Dutch \ncoalition governments since 1945 contained at least one center party, the relevant distinction \nhere is between center-left and center-right governments. When relating these types of \ngovernments to the pattern of attention visible from our data on coalition agreements, the \nenvironmental attention trend does not appear to correspond to the left or right swings of \ngovernments over time. The first spike of attention occurred when a center-right government \ntook office in 1971, and when the most leftist government since 1945 took office, in 1973, \nthis involved a sharp drop in attention in the governmental program. This recurred in the early \n1980s and in the 1990s and beyond, when a low level of attention existed during both types of \ngovernments. The only clear instance of a change in government composition involving an \nattention shift was 1989, when the Christian Democrats (CDA) changed the Liberal \n14 \n \nConservatives (VVD) for the Social Democrats (PvdA). But even this specific attention rise \nin the coalition agreement of 1989 actually followed after this theme had acquired \nprominence in the previous center-right government, which launched an ambitious and path-\nbreaking National Environmental Policy Plan but later collapsed over an environmental issue. \nIncidentally, that government had started out in 1986 with the lowest expression of \nenvironmental interest of all governments since the theme had appeared on the political \nagenda. In the Netherlands, issues triggering coalition collapse usually receive broad attention \nduring the formation of the next government. \nAnnual policy agenda correction for which Queen\u2019s speeches may be used also does \nnot consistently display the hypothesized ups and downs connected to center-left and center-\nright governments. The rise of attention in the late 1960s happened during a more \nconservative government than that actually placing the environment on the coalition agenda in \n1971. A correction did happen during the center-left government in office between 1973 and \n1977, but this was just a one year spike in 1974. As said, the low initial interest in \nenvironmental problems in the 1980s gave way to annual corrections leading to the high peak \nof 1989. But the decline in following does not testify to an image of a center left government \npromoting continued environmental attention. Likewise, the annual levels of attention during \nfollowing governments show some correction, but this attention correction itself went both up \nand down within similarly composed coalitions (1994-1998, 1998-2002). Particularly striking \nis the ongoing decline in the 1990s down to the level of the 1960s, when environmental \nproblems were just being discovered by politicians. Thus both indicators of policy \nprogramming by Dutch governments do not speak to a clear direct effect of government \ncomposition, the programmatic hypothesis has to be rejected. This conclusion fits a more \ngeneral pattern in which the allocation of attention in coalition agreements and annual \nQueen\u2019s speeches in the Netherlands does not clearly relate to new government\u2019s beginning a \nterm in office (Breeman et al 2008; Breeman et al 2009). \n \nPolitical Attention as National Reaction \n15 \n \nThe intrusion of new information may alter attention to environmental problems, and the lack \nof support for the programmatic attention hypothesis suggests this happens across \ngovernments of different party political orientation and at irregular intervals. Thus the rival \nhypothesis expects environmental attention to be much more reactive, following the flow of \ninformation signals. How does this information find its way into policy venues and change \npriorities in agenda setting? The discovery of environmental problems in the late 1960s was \npreceded by scientific alarm in what Downs called the pre-problem stage, and analysis of the \ndelivery of scientific reports on environmental issues between 1995 and 2006 suggests this \ntime element remained typical to the attention pattern in the Netherlands since those early \nyears (Breeman and Timmermans 2008). In this sense, the agenda effect of environmental \nindicators produced by expert organizations includes considerable delay in discovery and is \nmostly indirect. The production of what expert organizations would qualify as \u2018scientific facts \nthat none can deny\u2019 thus is not a sufficient stimulus to public and political attention. Agenda \nsetting always involves information selection and ignorance, and only in a few cases do \nalarming indicators receive immediate attention. Moreover, expertise may not only feed \nexpansion of debate and a rise in agenda prominence, it also is used to limit participation and \ntransfer problems away from political and public arenas. While expert organizations may \nacquire primacy in environmental policy making, our analysis suggests this happened more \noften in periods of declining attention than when issues spiralled up in public and political \narenas. This pattern is more widespread across other policy domains (Timmermans and \nScholten 2006). \nAttention spikes thus must involve other factors than the release of expert knowledge. \nAlarming environmental indicators or other information find their way onto the political \nagenda when linked to major events and reported by the media. The occurrence of focusing \nevents and the development of media attention over time relate closely to the upsurges in \npolitical attention since the 1960s. Thus the media are important amplifiers of information and \nplay a central role in attention cascades within this policy domain \u2013 much like they do in other \npolicy domains. The continous competition for attention in the media however also leads to \n16 \n \nshort pront page attention waves. Further, within the space of the media agenda for the \nenvironment, specific suptopics also compete for primacy in attention. Events such as the \npresentation by the government of the first National Environmental Policy Plan (1989), \nconferences over environmental problems such as climate change, new provocative actions by \npressure groups, or events such as the premiere of An Inconvenient Truth have lead to broad \nattention to the issues to which they were connected. Particularly the problem of climate \nchange and the international events organized for mobilizing attention received media \ncoverage in recent years, and crowded out attention for other environmental issues. In short, \nthe findings in this study provide empirical evidence for the hypothesis that governmental \nattention to environmental problems rises mostly in reaction to information signals, and is \nmuch less a matter of party political programming. \nOf course, the distinction between programmatic and reactive attention to \nenvironmental problems should not be overstated. Political parties in government do react to \ninformation signals. Coalition agreements or annual policy plans are not cast in stone, not are \nenvironmental issues usually decisive when forming and maintaining governments (the \ncoalition breakdown in 1989 over an environmental issue was an exception \u2013 it followed \nshortly after the first national environmental policy plan was presented). But the pattern of \nrising and declining political attention suggests two things: one, governments do not typically \nsteer a party political course in this, and two, when they change attention towards the \nenvironment, they do not consolidate this attention within the political arenas but soon move \nto other matters instead. \n \nLegislative Productivity by Stealth \nThe relative volatility of environmental attention in arenas of political agenda setting differs \nfrom the pattern of legislative productivity in this domain. Taking into account a time lag of \nabout three years in producing legislation after bills are proposed, the rise of legislative \nproduction since 2000 contrasts with the decline in attention in governmental programs \npresented every four years (coalition agreements) and annually (Queen\u2019s speeches). This \n17 \n \nrising regulatory activity does not seem to be a response to national political attention. The \nonly national agenda to which this recent trend corresponds is the media agenda. The \nlegislative agenda also shows much more attention to specific environmental issues such as \neffects of agriculture, chemical and nuclear waste, water quality, coastal areas, soil and air \npollution. This attention distribution emerged in the 1970s and institutionalized in what is \nreferred to as \u2018sectoral\u2019 environmental policy: regulation is divided into separate components \naddressing soil, water and air issues in environmental policy. As with the policy talk, the \npattern of legislative output does not reflect any particular government type and thus \nunderlines the reactive nature of environmental attention in the Netherlands. \nThis increasing legislative responsiveness during a period of declining political \nattention is related to the European environmental policy agenda. In the 1990s, national \nenvironmental legislation contained relatively few references to European policy (never more \nthan in 25 percent of the cases), but in recent years this changed to more than 50 percent of \nenvironmental legislation mentioning European policy \u2013 either formally in situations of \ntransposition of directives, or more informally when European impulses were mentioned. This \ncorresponds to findings in other research, where estimates are that today some 65 percent of \nnational environmental regulation follows from European policy (the higher percentage in this \nlast study may stem from a measurement difference: we counted only explicit references) \n(Asser Institute 2006). \nThe salience of the European Union to national environmental legislation may \nexplain both the rise in legislative output and the decline on the governmental agenda:  issues \nso strongly connected to European influence have become matters of high political risk since \nEuropean integration politicized in the Netherlands. Not only for environmental issues but for \nall main topics of public policy on the governmental agenda, references to Europe declined to \nthe lowest level in 25 years. \n \nAlternating Problem Frames \n18 \n \nEnvironmental laws thus do not appear at high tides of political attention, and they seem to \nhave become less related to national attention more generally. In recent years, frequent \nreferences to Europan policy indicate an external source of attention that is mostly reactive, \nand in earlier years, legislation mostly ensued with a time lag after an attention spike. This \npattern of rise and decline has its own expression in tone: political agenda setting and issue \nexpansion involve a dramatic frame, amplified by media attention mentioning focusing \nevents, whereas legislative production and its institutionalization since the 1970s speak more \nthe language of a technical frame, employed by professionals and experts in a policy \ncommunity. These two frames not only alternate over time with the rise and fall of attention \nin political arenas, they also exist in parallel but with changes in primacy. The spikes in \npolitical attention discussed in the previous section showed dramatization, but competition \nfrom other issues made that political alarm talk soon died out and was replaced by the more \ntechnical language in which environmental legislation became institutionally locked in. The \nalarmed rediscovery of global warming in recent years induced policy entrepreneurs such as \nenvironmental NGOs to try and expand issue attention and advocate a comprehensive \n\u2018climate law\u2019. The continued decline in political attention however suggests this is a national \nsiren call. \n \n \nConclusion \n \nInterest in environmental issues in arenas of governmental agenda setting is triggered largely \nby national and international events receiving media coverage. Alarmed discovery of \nenvironmental issues is not usually a matter of scientific information per se; media attention \npropels such discovery after what Downs called the \u2018preproblem stage\u2019. The pattern of \nattention to environmental problems shows ups and downs over time, but the fall of public \nand overt political attention in \u2018high politics\u2019 which mostly occurs soon after its rise involves \na shift to a sphere of  \u2018low politics\u2019 in which legislative production takes place. Since the \n19 \n \n1970s, this legislative production has institutionalized and it has become less and less driven \nby domestic public and political attention, and increasingly determined by European \nenvironmental policy. This Europeanization has further increased the discrepancy between \novert attention and national policy production in this domain. Since the rejection by national \nreferendum of the European Constitutional treaty on 1 June 2005, national politicians avoided \nEuropean topics, especially in public venues such as government formation in which coalition \nagreements are negotiated and annual Queen\u2019s speeches presented to the Joint Houses of \nParliament. Even when Al Gore hit the media headlines in 2007, political attention in the \naforementioned venues remained relatively low key. \n While these political and institutional forces are characteristic of the Dutch case, the \noverall pattern of attention produced in the different venues of agenda setting displays \npunctuated equilibria. This means that in a political system that differs from the U.S. polity \nwith its formal separation of powers and multiple venues related to federalism, the same basic \nmechanisms seem to be operative in directing environmental attention and the legislative \nagenda. The findings in this analysis of the Dutch case suggest that different in institutional \narrangements can have similar effects. This conclusion however is preliminary, and it needs \nfurther systematic analysis including a broader range of policy topics. \n \n \nReferences: PM\n20 \n \n \n \nFigures \n \nFigure 1: media attention to environmental issues\n0%\n1%\n1%\n2%\n2%\n3%\n3%\n4%\n4%\n5%\n1990\n1992\n1994\n1996\n1998\n2000\n2002\n2004\n2006\n2008\nenvironmental research issues \nopen w aters\nsoil pollution\nseaw ater pollution\nprotection endangered species\nasbestos\nrecycling\nair pollution traffic\nsour rain\nair pollution \nsound- and light issues\nchemical and nuclear w aste\nw aste disposal and sew erage\ndrinking w ater and groundw ater\nglasshouse issues, climate\nchange, CO2\n \n21 \n \nFigure 2a: distribution of changes between environmental issues in media attention \n(Kurtosis 6.49, N=255)\n0\n10\n20\n30\n40\n50\n60\n70\n-18 -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16 18 20\npercentage change\nam\nou\nn\nt\n \n2006 and 2007: An \nInconvenient truth \n2005 law suits most \npolluted streets \n22 \n \n \nFigure 2b distribution of changes in attention to the envrionment related to all topics in \nthe media (1990-2008; N=18; K=2.90)\n0\n1\n2\n3\n4\n-1,5 -1,3 -1,1 -0,9 -0,7 -0,5 -0,3 -0,1 0,1 0,3 0,5 0,7 0,9 1,1 1,3 1,5\npercentage change\nam\nou\nn\nt\n \nFigure 3: references to UN climate conferences in the media (N=264)\n0%\n5%\n10%\n15%\n20%\n25%\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n \n2006 and 2007: An \nInconvenient truth \n23 \n \nFigure 4: attention to environmental issues in coalition agreements\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n1963\n1965\n1967\n1971\n1973\n1977\n1981\n1982\n1986\n1989\n1994\n1998\n2002\n2003\n2007\nother\nenvironmental research issues \nsoil pollution\nseawater pollution\nrecycling\nglasshouse issues, climate\nchange, CO2\nchemical and nuclear waste\nwaste disposal and sewerage\ndrinking water and\ngroundwater\ngeneral\n \nFigure 5: attention to environmental issues in Queens' speeches\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n1945\n1949\n1953\n1957\n1961\n1965\n1969\n1973\n1977\n1981\n1985\n1989\n1993\n1997\n2001\n2005\nenvironmental research issues \nsoil pollution and open w aters\nseaw ater pollution\nprotection endangered species\nasbestos\nrecycling\nglasshouse issues, climate change,\nCO2\nchemical and nuclear w aste\nw aste disposal and sew erage\ndrinking w ater and groundw ater\ngeneral\n \n24 \n \nFigure 6: attention to envrironmental issues in laws\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\n1990 1992 1994 1996 1998 2000 2002 2004 2006\nother\nenvironmental research issues \nsoil pollution\nseaw ater pollution\nprotection endangered species\nrecycling\nglasshouse issues, climate change,\nCO2\nchemical and nuclear w aste\nw aste disposal and sew erage\ndrinking w ater and groundw ater\ngeneral\n \nFigure 7: environmental laws in the Netherlands\n0%\n1%\n2%\n3%\n4%\n5%\n6%\n7%\n8%\n9%\n10%\n1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007\nno reference to EU referring to EU\n \n \n",
    "source": ""
  },
  {
    "id": 2252395,
    "doi": null,
    "title": "Comment on \"Spatio-temporal filling of missing points in geophysical data sets\" by D. Kondrashov and M. Ghil, Nonlin. Processes Geophys., 13, 151\u2013159, 2006",
    "abstract": "Kondrashov and Ghil (2006) (KG hereafter) describe a method for imputing missing values in incomplete datasets that can exploit both spatial and temporal covariability to estimate missing values from available values. Temporal covariability has not been exploited as widely as spatial covariability in imputing missing values in geophysical datasets, but, as KG show, doing so can improve estimates of missing values. However, there are several inaccuracies in KG\u2019s paper. Since similar inaccuracies have surfaced in other recent papers, for example, in the literature on paleo-climate reconstructions, I would like to point them out here",
    "fullText": "Nonlin. Processes Geophys., 14, 1\u20132, 2007\nwww.nonlin-processes-geophys.net/14/1/2007/\n\u00a9 Author(s) 2007. This work is licensed\nunder a Creative Commons License.\nNonlinear Processes\nin Geophysics\nComment on \u201cSpatio-temporal filling of missing points in\ngeophysical data sets\u201d by D. Kondrashov and M. Ghil, Nonlin.\nProcesses Geophys., 13, 151\u2013159, 2006\nT. Schneider\nCalifornia Institute of Technology, Pasadena, CA, USA\nReceived: 10 July 2006 \u2013 Revised: 1 October 2006 \u2013 Accepted: 22 December 2006 \u2013 Published: 15 January 2007\nKondrashov and Ghil (2006) (KG hereafter) describe a\nmethod for imputing missing values in incomplete datasets\nthat can exploit both spatial and temporal covariability to es-\ntimate missing values from available values. Temporal co-\nvariability has not been exploited as widely as spatial covari-\nability in imputing missing values in geophysical datasets,\nbut, as KG show, doing so can improve estimates of miss-\ning values. However, there are several inaccuracies in KG\u2019s\npaper. Since similar inaccuracies have surfaced in other re-\ncent papers, for example, in the literature on paleo-climate\nreconstructions, I would like to point them out here.\n(i) In estimating covariance matrices, KG treat an incom-\nplete dataset with imputed values filled in as if it were a\ncomplete dataset. Possible variations of the missing values\naround the imputed values are ignored, leading to biased\nestimates of covariance matrices (Little and Rubin, 2002).\nThe expectation-maximization (EM) algorithm takes varia-\ntions of the missing values around the imputed values into\naccount, which is essential to obtain maximum likelihood es-\ntimates of parameters such as covariance matrices with their\nattendant optimality properties (Dempster et al., 1977). Reg-\nularized variants of the EM algorithm (Schneider, 2001) like-\nwise take variations of the missing values around the imputed\nvalues into account in the estimation of variances and covari-\nances, such that they reduce, in the limit of no regularization,\nto the EM algorithm for Gaussian data. The same could be\ndone in KG\u2019s method by adding estimated covariance matri-\nces of the imputation error to the sample covariance matrix\nof the completed dataset. This would improve the accuracy\nof KG\u2019s method, for example, in estimating variability. Ac-\ncurate variability estimates are particularly important for es-\ntimating higher-order statistics, such as extreme value statis-\ntics, which can be strongly biased if variations of missing\nvalues around imputed values coming from the center of a\ndistribution of possible values are not taken into account.\nCorrespondence to: T. Schneider\n(tapio@caltech.edu)\n(ii) Except for neglecting the variations of the missing\nvalues around the imputed values and for an unusual order\nof iterations \u2013 iteratively re-estimating individual principal\ncomponents \u2013 KG\u2019s method is similar to the regularized EM\nalgorithm exploiting spatial and stationary temporal covari-\nability described in Schneider (2001). (A principal compo-\nnent technique similar to that of KG and Beckers and Rixen\n(2003) but with the more usual order of iterations \u2013 itera-\ntively re-estimating covariance matrices and all relevant prin-\ncipal components \u2013 was presented by Everson and Sirovich\n(1995).) KG\u2019s principal component technique for imputing\nmissing values corresponds to an orthogonal or truncated to-\ntal least squares (auto-)regression (Fierro et al., 1997) and\ncan be used in a regularized EM algorithm as discussed in\nSchneider (2001). As KG\u2019s method, a regularized EM algo-\nrithm with truncated total least squares regression uses lead-\ning principal components based on the entire dataset, includ-\ning all records and variables with missing and available val-\nues. An innovation in KG\u2019s method is to make the time lag\nup to which temporal covariability is exploited an adaptive\nparameter.\n(iii) As a result of the similarity of KG\u2019s method and a\nregularized EM algorithm exploiting spatio-temporal covari-\nability with truncated total least squares regressions, several\nof KG\u2019s claims of how their method differs from regularized\nEM algorithms are incorrect. For example, KG\u2019s contrast-\ning of their method as being \u201cnon-parametric\u201d as opposed to\nthe \u201cparametric\u201d regularized EM algorithm is incorrect. The\nEM algorithm for Gaussian data yields maximum likelihood\nestimates of mean values, covariance matrices, and missing\nvalues, with their attendant optimality properties, but it and\nits regularized variants can also be justified under weaker as-\nsumptions (as least squares methods or regularized variants).\nKG\u2019s method is just as parametric as the regularized EM al-\ngorithm.\nPublished by Copernicus GmbH on behalf of the European Geosciences Union and the American Geophysical Union.\n2 T. Schneider: Comment on \u201cSpatio-temporal filling of missing points . . . \u201d\n(iv) There are other inaccuracies, particularly where KG\ncontrast their method with other methods. For example, it is\nnot correct that an \u201cEM-based method . . . [relies] on the ran-\ndomness in time of the missing values.\u201d The EM algorithm\nand regularized variants rely on the assumption that miss-\ning values are missing at random, which does not mean that\nvalues are missing randomly in time or in space but that the\nprobability that a value is missing is independent of the miss-\ning value \u2013 the central necessary condition for the mecha-\nnisms responsible for missingness to be ignorable (Little and\nRubin, 2002). KG\u2019s method relies on the same assumption.\nWhile using different terms and concepts may create\nthe impression that methods used to estimate statistics\nfrom incomplete data differ more strongly than they do,\nactual methodological differences, even if small, may be\nimportant in determining the performance of the methods. A\nsystematic exploration of the advantages and disadvantages\nof different methods is desirable, including methods such as\nthat of KG that exploit spatio-temporal covariability.\nEdited by: B. D. Malamud\nReviewed by: B. D. Malamud\nReferences\nBeckers, J. M. and Rixen, M.: EOF calculations and data fill-\ning from incomplete oceanographic datasets, J. Atmos. Oceanic\nTechnol., 20, 1839\u20131856, 2003.\nDempster, A. P., Laird, N. M., and Rubin, D. B.: Maximum like-\nlihood estimation from incomplete data via the EM algorithm\n(with discussion), J. Roy. Stat. Soc. B, 39, 1\u201338, 1977.\nEverson, R. and Sirovich, L.: Karhunen-Loe`ve procedure for gappy\ndata, J. Opt. Soc. Am. A, 12, 1657\u20131664, 1995.\nFierro, R. D., Golub, G. H., Hansen, P. C., and O\u2019Leary, D. P.: Reg-\nularization by truncated total least squares, SIAM J. Sci. Com-\nput., 18, 1223\u20131241, 1997.\nKondrashov, D. and Ghil, M.: Spatio-temporal filling of missing\npoints in geophysical data sets, Nonlin. Processes Geophys., 13,\n151\u2013159, 2006,\nhttp://www.nonlin-processes-geophys.net/13/151/2006/.\nLittle, R. J. A. and Rubin, D. B.: Statistical Analysis with Missing\nData, Series in Probability and Mathematical Statistics, Wiley,\nNew York, 2nd edn., 2002.\nSchneider, T.: Analysis of incomplete climate data: Estimation of\nmean values and covariance matrices and imputation of missing\nvalues, J. Climate, 14, 853\u2013871, 2001.\nNonlin. Processes Geophys., 14, 1\u20132, 2007 www.nonlin-processes-geophys.net/14/1/2007/\n",
    "source": "European Geosciences Union"
  },
  {
    "id": 45819251,
    "doi": null,
    "title": "Training on IRI Climate Data Tools and developing a method for integrating climate data in Kigali, Rwanda",
    "abstract": "For more than five years, the Rwanda Meteorological Agency (M\u00e9t\u00e9o Rwanda) and the International Research Institute for Climate and Society (IRI) have been working together to implement the IRI\u2019s Enhancing National Climate Services initiative (ENACTS) in Rwanda. The ENACTS initiative brings climate knowledge into national decision- making by improving availability, access and use of climate information. M\u00e9t\u00e9o Rwanda staff have received a number of trainings on the different aspects of generating the datasets and developing climate information products. However, the tools used to generate ENACTS datasets have been evolving through\r\naddition of several new features. As a result, it is necessary to revise the training and to expose the staff to the new version of the tools. On the other hand, M\u00e9t\u00e9o Rwanda has started operating a network of automatic weather stations (AWS) and a weather radar. The integration of these datasets to ENACTS datasets is very important to improve the quality of climate data in Rwanda. Thus, the current training activities had three major components: (1) make a full use of the IRI Climate Data Tools (CDT) to create and analyze ENACTS datasets; (2) develop quality control procedures and create scripts to integrate data from the AWS network into ENACTS datasets; and (3) create scripts to process and adjust the radar-based precipitation estimates with data from the AWS network and integrate the processed data into ENACTS datasets",
    "fullText": " \n \n \nWorkshop\treport:\tTraining\ton\tIRI\tClimate\t\nData\tTools\tand\tdeveloping\ta\tmethod\tfor\t\nintegrating\tclimate\tdata\t\nKigali,\tRwanda\t\t\nAugust 2017 \n \nRija Faniriantsoa \n \n \n \n \n \n 2 \n \n \n \nTraining on IRI Climate Data \nTools and developing a \nmethod for integrating \nclimate data \n \nKigali, Rwanda, August 2017 \n \nWorkshop Report \n \nCGIAR Research Program on Climate Change, \nAgriculture and Food Security (CCAFS) \n \nRija Faniriantsoa \n \n \n  \n 3 \nCorrect citation:  \nFaniriantsoa, R. 2017. Training on IRI Climate Data Tools and developing a method for integrating \nclimate data in Kigali, Rwanda. CCAFS Workshop Report. Wageningen, Netherlands: CGIAR \nResearch Program on Climate Change, Agriculture and Food Security (CCAFS). Available online at: \nwww.ccafs.cgiar.org \n \n \nCCAFS Workshop Reports aim to disseminate interim climate change, agriculture and food security \nresearch and practices and stimulate feedback from the scientific community. \n \nPublished by the CGIAR Research Program on Climate Change, Agriculture and Food Security \n(CCAFS). \n \nThe CGIAR Research Program on Climate Change, Agriculture and Food Security (CCAFS) is a \nstrategic partnership of CGIAR and Future Earth, led by the International Center for Tropical \nAgriculture (CIAT). The Program is carried out with funding by CGIAR Fund Donors, Australia \n(ACIAR), Ireland (Irish Aid), Netherlands (Ministry of Foreign Affairs), New Zealand Ministry of \nForeign Affairs & Trade; Switzerland (SDC); Thailand; The UK Government (UK Aid); USA \n(USAID); The European Union (EU); and with technical support from The International Fund for \nAgricultural Development (IFAD). \n \n \n \nContact: \nCCAFS Program Management Unit \u2013 Wagenigen University & Research, Lumen Building, \nDroevendaalsesteeg 3a, 6708 PB Wageningen, the Netherlands. Email: ccafs@cgiar.org  \n \n \nCreative Commons License \n \nThis Workshop Report is licensed under a Creative Commons Attribution \u2013 NonCommercial\u2013\nNoDerivs 3.0 Unported License. \n \nArticles appearing in this publication may be freely quoted and reproduced provided the source is \nacknowledged. No use of this publication may be made for resale or other commercial purposes. \n \n\u00a9 2017 CGIAR Research Program on Climate Change, Agriculture and Food Security (CCAFS). \n \n \nCover photo by Rija Faniriantsoa. \n \n \n \n \n \n \n \nDISCLAIMER: \nThis Workshop Report has been prepared as an output for the Flagship 4: Climate Services and Safety \nNets (previously Flagship 2: Climate Risk Management) under the CCAFS program and has not been \npeer reviewed. Any opinions stated herein are those of the author(s) and do not necessarily reflect the \npolicies or opinions of CCAFS, donor agencies, or partners. The geographic designation employed and \nthe presentation of material in this publication do not imply the expression of any opinion whatsoever \non the part of CCAFS concerning the legal status of any country, territory, city or area or its authorities, \nor concerning the delimitation of its frontiers or boundaries. All images remain the sole property of \ntheir source and may not be used for any purpose without written permission of the source.\n  4 \nAbstract  \nFor more than five years, the Rwanda Meteorological Agency (M\u00e9t\u00e9o Rwanda) and \nthe International Research Institute for Climate and Society (IRI) have been working \ntogether to implement the IRI\u2019s Enhancing National Climate Services initiative \n(ENACTS) in Rwanda. The ENACTS initiative brings climate knowledge into \nnational decision- making by improving availability, access and use of climate \ninformation. M\u00e9t\u00e9o Rwanda staff have received a number of trainings on the different \naspects of generating the datasets and developing climate information products. \nHowever, the tools used to generate ENACTS datasets have been evolving through \naddition of several new features. As a result, it is necessary to revise the training and \nto expose the staff to the new version of the tools. On the other hand, M\u00e9t\u00e9o Rwanda \nhas started operating a network of automatic weather stations (AWS) and a weather \nradar. The integration of these datasets to ENACTS datasets is very important to \nimprove the quality of climate data in Rwanda. Thus, the current training activities \nhad three major components: (1) make a full use of the IRI Climate Data Tools (CDT) \nto create and analyze ENACTS datasets; (2) develop quality control procedures and \ncreate scripts to integrate data from the AWS network into ENACTS datasets; and (3) \ncreate scripts to process and adjust the radar-based precipitation estimates with data \nfrom the AWS network and integrate the processed data into ENACTS datasets. \nKeywords \nClimate information services; Rwanda; Climate forecast; Climate data; Adaptation; \nENACTS; Training \n 5 \nAbout the authors  \nRija Faniriantsoa is a Staff Associate working on evaluating and improving climate \ndatasets at the International Research Institute for Climate and Society (IRI), \nColumbia University. Contact: rijaf@iri.columbia.edu  \n \n \n \n \n \n  \n  6 \nAcknowledgements  \nThis report is an output of the Rwanda Climate Services for Agriculture Project, \nsupported by the U.S. Agency for International Development (USAID) \u2013 Rwanda and \nthe CGIAR Research Program on Climate Change, Agriculture and Food Security \n(CCAFS). The opinions expressed herein are those of the authors, and do not \nnecessarily reflect the view of the USAID and donors. \nCCAFS receive support from CGIAR Fund Donors and through bilateral funding \nagreements. For details please visit https://ccafs.cgiar.org/donors. The views \nexpressed in this document cannot be taken to reflect the official opinions of these \norganizations. \n \n 7 \nContents \nIntroduction .................................................................................................................... 9 \nTraining on IRI Climate Data Tools (CDT) ................................................................ 10 \nSession I: Introduction to CDT ................................................................................ 10 \nSession II: Data manipulation .................................................................................. 11 \nSession III: Data analysis ......................................................................................... 11 \nSession IV: Quality control ...................................................................................... 15 \nSession V: Merging data .......................................................................................... 16 \nIntegrating AWS data into ENACTS datasets ............................................................. 18 \nThe AWS network ................................................................................................... 18 \nData processing ........................................................................................................ 19 \nIntegrating Radar data into ENACTS datasets ............................................................ 21 \nTITAN Precipitation estimation .............................................................................. 22 \nAdjustment of radar-based precipitation accumulations .......................................... 24 \nConclusions and future work ....................................................................................... 26 \nAppendix 1: Training Agenda ..................................................................................... 27 \nAppendix 2: Participant List ........................................................................................ 28 \n \n  8 \nAcronyms \nAWS  Automatic Weather Stations \nCDT  IRI Climate Data Tools \nCIDD Cartesian Interactive Data Display \nENACTS  Enhancing National Climate Services initiative \nIRI  The International Research Institute for Climate and Society \nJRA55  Japan 55 Years Re-analysis \nMDV Meteorological Data Volume \nPICSA  Participatory Integrated Climate Services for Agriculture \nQC  Quality Control  \nQPE quantitative precipitation estimates \nSOND September-December rainy season \nTAMSAT  Tropical Applications of Meteorology using SATellite data and ground-\nbased observations \nTITAN  Thunderstorm Identification Tracking Analysis and Nowcasting \n\u2028 \nIntroduction \nThe Rwanda Meteorological Agency (M\u00e9t\u00e9o Rwanda), in collaboration with the \nInternational Research Institute for Climate and Society (IRI), has embarked on a \nunique multi-faceted initiative called Enhancing National Climate Services \n(ENACTS). The initiative aims to bring climate knowledge into national decision-\nmaking by improving availability, access and use of climate information. Availability \nof climate data is improved by combining quality controlled data from the national \nobservation network, with satellite estimates for rainfall and elevation maps and \nreanalysis products for temperature. These new data sets have been used to develop \ninformation products that are available through M\u00e9t\u00e9o Rwanda's web page. \nM\u00e9t\u00e9o Rwanda staff have received several trainings by IRI experts on the different \naspects of generating the data sets and developing climate information products. \nHowever, due to the recent reorganization at M\u00e9t\u00e9o Rwanda, new staff have been \nhired. Thus, M\u00e9t\u00e9o Rwanda has requested refresher training for existing staff as well \nas training new staff. There have also been some new developments in methodology \nand tools. The purpose of the training was to provide the necessary basis use of the \nIRI Climate Data Tools (CDT) to generate and exploit ENACTS datasets. \nOver a seven-day period, the participants had the opportunity to practice CDT and \nwere able not only to generate the data sets, but also to make full use of these data \nthrough this tool to develop elaborated products. The training program (Appendix 2) \nhad the following main components: 1) Introduction to CDT; 2) Data manipulation; 3) \nData analysis; 4) Quality control; and 5) Merging data. \nIn order to enhance their meteorological observational network, M\u00e9t\u00e9o Rwanda has \ninstalled 41 automatic weather stations and 100 automatic rain gauges. The automatic \nstations provide regular and more objective measurements. In order to acquire and \nprovide accurate data records, a quality control procedure is required. Thus, an \nautomated quality control procedure was developed to ensure quality of the data and \nintegration into ENACTS datasets. \n  10 \nWeather Radar offers weather forecasters, scientists and the public with an alternative \nmeans of measuring precipitation. Although its measurements are indirect, it remains \nthe best alternative gauge measurements to capture the spatial variability of \nprecipitation at high temporal and spatial resolutions. Thus, accurately measuring the \namount of precipitation is becoming increasingly more important for the end-user\u2019s \napplication. Thus, a computer program was developed to improve the existing radar \nprecipitation field and to integrate the processed data to ENACTS datasets. \nTraining on IRI Climate Data Tools (CDT) \nThis training was designed to provide the participants with strong skills in utilizing \nCDT and manipulate ENACTS gridded datasets that can be used in technical reports. \nThe training was divided into 5 sessions; the first three sessions were for all of the \nparticipants and focused on data processing and analysis. Sessions IV and V were \ndesigned to provide hands-on training on data quality control and generating \nENACTS datasets, and were specifically for staff responsible for the operational \nproduction of ENACTS data. The following is a brief description of the five sessions. \nSession I: Introduction to CDT \nCDT1 is an R-based software package with a set of modules accessible via a \nGraphical User Interface for data quality control, homogenization and analysis of \nclimatological time series (mainly air temperature and precipitation). In addition, \nCDT offers an easy way to combine station data with satellite rainfall estimates \nproducts and reanalysis for temperature. Users have full control over each step of the \nprocess.  This session trained the participants on how to install and configure CDT on \ntheir computers: acquisition and installation of all required software and cloning CDT \nfrom GitHub repository. This session also provided the participants with hands-on \ntroubleshooting tips during installation. After CDT installation, an introduction to the \nmain menu of CDT was presented. The session provided an overview of the various \nfeatures of CDT. \n \n \n1 https://github.com/rijaf/CDT \n 11 \nSession II: Data manipulation \nThis session provided more practice on the different components of CDT data \norganization and acquisition so that participants are familiar with the following tasks: \n\u00a7 Convert stations data to CDT data format; \n\u00a7 Download and extract satellite rainfall estimates and reanalysis data; and \n\u00a7 Download auxiliary\u2019s data such as Digital Elevation Model and administrative \nboundaries. \nThe second half of this session focused on time series aggregation. It also explored the \ndifferent ways that CDT aggregate stations or gridded NetCDF data. This session \nincluded several exercises where the participants calculated the monthly or seasonal \namount of rainfall, the monthly and seasonal average temperature and the number of \ndays above or below a given threshold.  A practical exercise on operational \napplications was also carried out: computing the coldest, hottest, driest and wettest \nmonth or season, the frequency of a frost day or tropical night. \nSession III: Data analysis \nThis session was designed to provide the participants with in-depth knowledge on the \nmain components of CDT\u2019s data analysis and familiarity with the different features. \nThe session started with the extraction of gridded ENACTS data over a defined \ngeometry support (points, rectangle, polygons), then aggregated the data to other time \nstep or compute climatologies, anomalies or standardized anomalies. The output can \nbe exported to different formats used by different software such as the IRI Climate \nPredictability Tool (CPT). \nThis session also trained the participants on how to generate a historical climate data \nand graphs which can be used by the Participatory Integrated Climate Services for \nAgriculture (PICSA) approach2.  An overview of the computing of the historical onset \nand cessation of the rainy season was presented. Definitions were presented: on the \nonset of September-December (SOND) rainy season and how to set the different \n \n \n2 http://www.walker.ac.uk/projects/participatory-integrated-climate-services-for-agriculture-picsa/ \n  12 \ncriterions depending on the geographic/climatological region, the rainy season and the \ndefinition in the context of agriculture. \nHaving been introduced to the method used to calculate the onset and cessation of the \nrainy season, the participants began to work with the data. The exercises allowed the \nparticipants to navigate through the different options of CDT and get familiar with the \ncreation and interpretation of graphs and maps that would be useful for PICSA. Below \nare some examples of the several graphs and maps that participants could generate \nand interpret during the exercises (Figures 1-7). \n \nFigure 1: Average onset date of the SOND rainy season for a few locations for the period \n1981-2016 \n \nFigure 2: Average cessation date of the SOND rainy season for a few locations for the \nperiod 1981-2016 \n 13 \n \n \n \nFigure 3: Average duration of the SOND rainy season from a few locations for the period \n1981-2016 \nFigure 4: Onset date time series graph of the SOND rainy season for Kanama \nFigure 5: Seasonal rainfall amounts during the SOND rainy season defined by the onset \nand cessation date for Kanama \n  14 \nFigure 6: Seasonal number of rainy days probability of exceedance for Kanama (SOND \nseason defined by the onset and cessation date)  \nFigure 7: Seasonal number of dry spells (5 or more consecutive dry days) probability of \nexceedance for each ENSO phase at Kanama (SOND season defined by the onset and \ncessation date)  \n \nThis session also included exercises with the CDT menu where the participants were \ninvited to calculate the climate extremes indices defined by the ETCCDI (Expert \nTeam on Climate Change Detection and Indices)3. The session also included a \npractical exercise on a climatological analysis. This exercise allowed participants to \ncalculate totals, averages, medians, trends, anomalies and other functions for a given \nperiod of time.  \n \n \n \n3 http://www.climdex.org/indices.html \n 15 \nSession IV: Quality control \nThis session trained M\u00e9t\u00e9o Rwanda\u2019s staff on how to conduct a quality control of \nrainfall and temperature time series with CDT. The importance of using quality \ncontrolled was emphasized. Inaccurate data can cause many problems for users when \ncreating reliable climate information products. The participants were introduced to the \ndifferent types and sources of error that could be found in the climate datasets, \nincluding: \n\u00a7 Change or malfunction of sensor/instrument \n\u00a7 Change in the environment around the station, in the station location, or in the \ntime of observation \n\u00a7 Data entry errors and typos, or change of units \n\u00a7 Mixture of data with decimal points and data not yet converted to decimal points \n\u00a7 Confusion between variables, e.g. minimum temperature instead of maximum \ntemperature  \n\u00a7 Confusion between missing and actual values, e.g. missing value set to zero \n\u00a7 Files format conversion \nThe training mainly focused on practical work on M\u00e9t\u00e9o Rwanda's own data. Most of \nthis was done outside the training hours because there was not enough time to do it as \npart of the regular training. \nThe typical workflow for quality control with CDT is as follows: \n\u00a7 Stations geographical coordinates verification \n\u00a7 False zeros for daily rainfall, which is the case where missing values are set to \nzero.  \n\u00a7 Outliers detection for rainfall and temperatures: \n\u00a7 Internal consistency check \n\u00a7 Temporal outliers check \n\u00a7 Spatial outliers check \n\u00a7 Reporting suspicious values and correcting errors: \n\u00a7 Problems might be identified but more difficult to correct. \n  16 \n\u00a7 Throwing out valid extreme values can cause errors as easily as keeping \nerroneous extreme values. \n\u00a7 Undetected observation errors are always possible even using objective \nmethods. \n\u00a7 The biggest problem is usually the lack of accessible metadata. \nSession V: Merging data \nThis session provided more practice on combining stations t data with gridded \ndatasets using CDT. The workflow used to merge stations measurement data with \nsatellite rainfall estimates is as follows: \n\u00a7 Use the data from 1981 to 2016 (station and satellite) to calculate a gridded \nclimatological bias factor for each day (days 1 to 365) or dekad (dekads 1 to 36). \n\u00a7 Apply the bias correction to satellite rainfall estimates time series from 1981 to \npresent. \n\u00a7 Merge the bias corrected rainfall satellite estimates with the stations data for each \ntime step. \nFig. 8 compares station data with satellite, the bias corrected satellite and combined \nstation-satellite products. \nFigure 8: Comparisons of station observation (top left), TAMSATv3 (top right), \nTAMSATv3 bias corrected (bottom left) and combined product (bottom right) for the 1st \ndekad of May 1987. \n 17 \nAs there are no satellite temperature estimates going back 30 years, reanalysis data are \nused as a proxy. Reanalysis products are climate data generated by systematically \ncombining climate observations (analyses) with climate model forecasts using data \nassimilation schemes and climate models. The Japanese 55-year Reanalysis (JRA55)4 \nis used to generate a gridded temperature time series for the period 1961-2016. This \nproduct has a coarse spatial resolution of about 50km. Thus, the reanalysis data are \ndownscaled to 5km spatial resolution using station observations and elevation maps.  \nFigure 9: Comparisons of station observation (top left), JRA55 reanalysis (top center), \ndownscaled JRA55 (top right), bias corrected data (bottom left) and combined product \nof minimum temperature (bottom center) for the 3rd dekad of February 2015. \nThe workflow used to generate gridded temperature time series by combining stations \nmeasurement data with reanalysis is as follows: \n\u00a7 Calculate the coefficients for the downscaling using the elevation and the station \ntemperature data. \n\u00a7 Downscale reanalysis data from 50km to 4km resolution for the period 1961-2016 \nusing the coefficients estimated above. \n \n \n4 http://jra.kishou.go.jp/JRA-55/index_en.html#about \n  18 \n\u00a7 Use the data from 1961 to 2010 (station and downscaled data) to calculate a \ngridded climatological bias factor for each day (days 1 to 365) or dekad (dekads 1 \nto 36). \n\u00a7 Apply the bias correction to the downscaled data from 1981 to 2016. \n\u00a7 Merge the bias corrected data with the stations data for each time step. \nAll the steps listed are shown in Fig. 9. \nIntegrating AWS data into ENACTS datasets \nSo far, Rwanda\u2019s ENACTS datasets have been produced using manual meteorological \nstations network. During the last 4 years, M\u00e9t\u00e9o Rwanda has developed and operates a \nnetwork of automatic weather stations (AWS) across Rwanda. A major advantage \noffered by the AWS network is that it has the ability to gather and disseminate greater \nquantities of data at more frequent intervals than a conventional meteorological \nstation network. Thus, the main objective of this work was to maximize the use of \nthese data, and integrate it into ENACTS datasets.  \nThe AWS network \nAt present, M\u00e9t\u00e9o Rwanda operates a network of 141 AWS providing real-time data \nat 10 minute intervals. The network is composed of 41 automatic weather stations \nrecording meteorological parameters such as pressure, temperature, relative humidity, \nwind speed and direction, radiation and precipitation, and 100 automatic rain gauge \nstations. Fig. 10 shows the geographical distribution of the 141 stations. The blue \nasterisks in the figure represent the location of the automatic rain gauges while the red \ncrosses are the locations of the AWS. The network covers the totality of the Rwanda \nterritory with a more or less homogeneous geographic distribution in some places. \nIn the quasi-totality of the stations, data are recorded at 10 minute intervals, and they \nare transferred to a server at M\u00e9t\u00e9o Rwanda office, dedicated to exploitation and \narchiving. There are two servers used to manage the data. The data from 119 stations \nare stored in a LSI LASTEM GIDAS database, which is a Microsoft SQL Server \ncompatible database for storage of data acquired and elaborated through the \ninstruments. A viewer program allows the user to make a query, re-process, extract \n 19 \nand display the data stored using Microsoft SQL Server Gidas. Apart from the direct \nmeasurements of the main meteorological parameters, additional information is \nprovided, such as wind rose data and daily minima and maxima of all parameters. \n \n \nFigure 10: Geographical distribution of the AWS network over Rwanda \nThe data from the other 22 stations use a Linux MySQL database. Those stations have \na dedicated webpage which provides the most recent measurements, diagrams of \nmeasured parameters for a selected period, and some basic statistical information. \nThe two servers have a different program to processed the data. The elaborated data \nare sent to an FTP server. \nData processing \nAll of the data from the AWS were already subject to a basic quality control. It is a \nQuality Control (QC) performed on the raw data (signal measurements) inside the \nAWS system, which eliminates the technical devices errors, including sensors, \nmeasurement errors (systematic or random), errors inherent in measurement \nprocedures and methods. However, the basic quality control procedures do not \n  20 \nremove the erroneous values from the AWS data completely; there are still some \nerrors undetectable by the AWS QC system. Thus, those data need to be extensively \nquality controlled before being used. \nThe workflow used to process the AWS data before used to generate ENACTS \ndatasets is as follows: \n\u00a7 Download 10-min AWS data from FTP servers every hour. \n\u00a7 Apply a first QC to the 10-min data for each meteorological variable during the \nelapsed one hour in order to detect and flag data of questionable quality. The \nerroneous data detected in this QC level are recorded to a log file, and are replaced \nby a missing value and not used in further data processing. This QC procedures \ncheck for:  \n\u00a7 conformance to the operational range of each sensor; \n\u00a7 exceedance of predefined extreme values for each variable;  \n\u00a7 suspiciously persistent values; \n\u00a7 internal consistency; \n\u00a7 Aggregate 10-min data to hourly, a minimum number of 10-min data is required \nto compute the hourly data (5 for precipitation and 4 for the other variables). If \nminimum number is not present, then the value is replaced by missing. \n\u00a7 Apply a second QC to the hourly data. A spatial check is performed, the value of \nthe target station is compared with the closest stations observations by taking into \naccount the elevation and relief. The suspicious data detected in this QC are saved \nto a log files, and replaced by a missing value and not used in further data \nprocessing. \n\u00a7 Aggregate hourly data to daily, as the case of hourly data a minimum number of \nhourly data is required to compute the daily data (23 for precipitation and 18 for \nthe other variables). \n\u00a7 Apply a third QC to the daily data. A spatial and temporal check are performed. \nAll the processed and quality controlled data are saved to a disk. The hourly \nprecipitation will be used to adjust the radar-based rainfall estimate. The daily data \n 21 \nwill be used in combination with data from the conventional meteorological station \nnetwork to produce ENACTS datasets. \nAll procedures used to process the data should be fully automated by using a task \nscheduler and an appropriate configuration on the computer to be used to run the \nscripts. The data acquisition from the FTP servers, QC for 10-min, hourly aggregation \nand QC will be executed every hour, while the daily aggregation and QC will be \nexecuted daily. Unfortunately, there was not enough time, so the configuration of the \ncomputer to run the scripts has not been fully tested and the procedures are not yet \ncompletely automated. \nDevelopment of an applications to produce specific data formats, summaries, and \ncharts from the output data should also be added to help M\u00e9t\u00e9o Rwanda to compute \nweekly, dekadal, and monthly climate summaries and bulletin. It is highly \nrecommended that M\u00e9t\u00e9o Rwanda establish a well-documented metadata information \nfor all AWS. All information about the operation of the stations must be meticulously \nnoted. These metadata include: \n\u00a7 Detailed description of each station related to the location, coordinates and ID; \n\u00a7 Detailed description of the dates of regular maintenance, problems encountered, \nand the calibration of sensors; and \n\u00a7 Reporting of dates of sensor replacements, due to failure or ageing. \nIntegrating Radar data into ENACTS datasets \nM\u00e9t\u00e9o Rwanda has weather radar, which is mainly used by forecasters to track the \nevolution of storm systems over time and monitor severe weather events. Following \nthe storm/event, the weather radar data are then stored and are no longer used. \nWeather radars give quantitative precipitation estimates (QPE) over large areas with \nhigh spatial and temporal resolutions not achieved by conventional rain gauge \nnetworks. Therefore, the integration of these data into ENACTS datasets is highly \nrelevant for later analysis and applications. \n \n  22 \nThe radar is an ARC C250P C-band polarimetric doppler weather operated by M\u00e9t\u00e9o \nRwanda located at the latitude 02\u00b009'29.07\" S, longitude 30\u00b006'44.43\" E and altitude \n1616 m (Fig. 11). Every 5 minutes, the radar performs 11 azimuthal scans of 360\u00b0 \naround a vertical axis at beam elevation angles of 0.5\u00b0, 1.5\u00b0, 2.5\u00b0, 3.5\u00b0, 4.5\u00b0, 6.0\u00b0, \n8.0\u00b0, 11.0\u00b0, 15.0\u00b0, 22.0\u00b0 and 32.0\u00b0 with maximum range 250 km. More technical \nspecifications about the radar can be found on Advanced Radar Company\u2019s website5.  \n \n \nFigure 11: Location of the weather radar, with the red circles represent the rings of \nrange with a spacing of 50 km \nThe data from the radar are archived in a data format called Meteorological Data \nVolume (MDV), and the software to process and display the data is Thunderstorm \nIdentification Tracking Analysis and Nowcasting (TITAN)/Cartesian Interactive Data \nDisplay (CIDD) developed by the National Center for Atmospheric Research \n(NCAR). \nTITAN Precipitation estimation \nTo compute the precipitation rate, the TITAN configuration installed in Rwanda \ncurrently uses the Marshall-Palmer relationship Z = a Rb, where Z is the reflectivity \n \n \n5 http://advancedradarcompany.com/products/arc-c250p/ \n 23 \nfactor and R is rainfall intensity. The coefficients are set as a = 200, b = 1.6 in the \ncurrent configuration. \nClutter are non-meteorological echoes. They are caused by the radar beam hitting \nobjects on the earth's surface or in the air. Clutter causes a problem for precipitation \naccumulation calculations because a single clutter point will give a large amount of \nfalse precipitation value when the clutter reflectivity is converted into precipitation \nrate and integrated over time. Thus, clutter must be removed before computing the \nprecipitation rate. TITAN standard clutter removal technique consists of computing a \nclutter map from a number of MDV volumes containing \"clear air\" data, a sample of \n30 to 40 volumes are used to create the map. The median reflectivity values for these \nclear-air scans are computed then stored as a clutter map in MDV format. The clutter \nmap is used to remove clutter from radar volumes. Only reflectivity which exceeds \nthe median value by a specified amount (typically 3 or 5 dB) will be retained. Fig. 12 \nshows the steps for clutter removal. \n \n \nFigure 12: TITAN standard clutter removal technique diagram. Source: ARC TITAN \nSystem Manual \nTITAN has a specific application, PrecipAccum, to compute precipitation rate and the \naccumulated precipitation by integrating the rate over time. Fig. 13 shows the diagram \nusing PrecipAccum. A single-scan rate is computing first, then a running \naccumulation totals for 1-hour, 3-hour, and 24-hour products are performed. \n \n  24 \n \nFigure 13: TITAN precipitation estimation application. Source: ARC TITAN System \nManual \nTITAN configuration installed at M\u00e9t\u00e9o Rwanda computes a single-scan depth, a 1-\nhour and 24-hour running accumulation totals precipitation. A 2-dimensional MDV \nfile with the surface precipitation estimates in millimetres of rain is produced by the \napplication. The precipitation values can be extracted and adjusted with surface gauge \nobservations. \nAdjustment of radar-based precipitation accumulations \nAdjustment normally refers to using rain gauge observations on the ground to correct \nfor errors in the radar-based precipitation estimation. The advantage with rain gauge \nmeasurements is their ability to directly measure the precipitation on the ground. It is \noften regarded as the true precipitation for a particular point of measurement. \nHowever, as rain gauge measurements are point observations, this limits their ability \nto capture the spatial variability of precipitation. It is difficult to interpolate or \nextrapolate rain gauge data in any accuracy or significant detail. At shorter time scales \nand distances the accuracy of the measurement are also dependent on rainfall type. \nTherefore, the radar-based rainfall estimates are extracted at rain gauge locations. \nThe general idea is to quantify the error of the radar-based rainfall estimate at the rain \ngauge locations. A mixed error model is used to adjust the raw radar rainfall \nestimates. This model assumes that the error to be heterogeneous in space and have \nboth a multiplicative and an additive error term. The mixed error model is defined as: \nR_gauge = R_radar * (1 + delta) + epsilon \n 25 \nR_gauge is the amount of rain from the rain gauges and R_radar is the amount of rain \nfor the corresponding radar pixel. delta and epsilon have to be assumed to be \nindependent and normally distributed. This implementation is based on a Least \nSquares estimation of delta and epsilon for each rain gauge location. delta and epsilon \nare then interpolated to the radar grid data and used to correct the radar rainfall \nestimates. The advantage of this approach is that epsilon dominates the adjustment for \nsmall deviations between radar and gauge while delta dominates in case of large \ndeviations. \nThe hourly rainfall accumulations data from AWS network (Fig. 10) and the hourly \naccumulations from TITAN outputs are used to compute the coefficients delta and \nepsilon of the error model. Fig. 14 shows the results of the adjustment process. The \nautomatic rain gauges have been extensively quality controlled. However, there may \nbe some problematic measurement still as it is difficult to account for all the \nassociated errors with the datasets. The estimated rainfall data from radar already \nshow similarity with the rain gauges.  \n \nFigure 14: Hourly rainfall depths for 2017-09-28 16:00 at local time. Results are shown \nfor the automatics rain gauges (left), radar radar-based rainfall estimation, and the \nadjusted rainfall.  \nAs is in the case of AWS, all radar data processing should be automated. In addition, \ndevelopment of specific data formats and maps should be developed to help M\u00e9t\u00e9o \nRwanda make the best use of their radar. \n \n  26 \nConclusions and future work \nThe training activity built M\u00e9t\u00e9o Rwanda\u2019s capacity to take full advantage of \nENACTS datasets. The first part of training allowed M\u00e9t\u00e9o Rwanda's Staff to make \nfull use of their data with CDT, while the second part trained the staff to generate, \nmaintain and update ENACTS datasets. An automated quality control procedures was \nimplemented in order to ensure the accuracy of M\u00e9t\u00e9o Rwanda's AWS network, and a \ndata processing scripts was developed to integrate these data into ENACTS datasets. \nPart of the work done at M\u00e9t\u00e9o Rwanda included the creation of scripts to process the \nradar-based precipitation estimation and the adjustment of these data with quality-\ncontrolled data from the AWS network. Further development is needed to improve the \nrobustness of the procedures currently being implemented and to evaluate the outputs. \nDevelopment of an application to produce specific data formats, summaries and charts \nfrom the output data should be added to help M\u00e9t\u00e9o Rwanda to compute a daily, \nweekly, dekadal, and monthly climate summaries and bulletins. \n 27 \nAppendix 1: Training Agenda \nWeek Training title Date Training objectives \n1 Training on updated CDT Monday, August 14, 2017 \u2022 Meet with the participants \n\u2022 Software installation \n\u2022 Introduction to IRI Climate Data Tools \n(CDT) main menu \nTuesday, August 15, 2017 \u2022 Data organization \n\u2022 CDT data manipulation and conversion \n\u2022 Time series aggregation \nWednesday, August 16, \n2017 \n\u2022 Time series aggregation (continue) \n\u2022 Data extraction \nThursday, August 17, \n2017 \n\u2022 Climate extremes indices calculations \n\u2022 Climatological analysis \n\u2022 Generate PICSA climate data and graphs \nFriday, August 18, 2017 \u2022 Quality control Rainfall \n\u2022 Quality control Temperature \n2 Merging data Monday, August 21, 2017 \u2022 A review of merging data \n\u2022 Merge Rainfall data \n\u2022 Operational updates of dekadal rainfall \ndata \nTuesday, August 22, 2017 \u2022 Downscaling reanalysis \n\u2022 Merge temperature data \nIncorporation of AWS \ndata into ENACTS \ndatasets \nWednesday, August 23, \n2017 \n\u2022 System and server overview \n\u2022 Review of all available AWS data at M\u00e9t\u00e9o \nRwanda \n\u2022 Implement a Verification and Quality \ncontrol procedures and scripts \n\u2022 Create scripts to format AWS data to CDT \n\u2022 Create batch scripts for tasks scheduling \nand automation \nThursday, August 24, \n2017 \nFriday, August 25, 2017 \n3 Incorporation of AWS \ndata into ENACTS \ndatasets \nMonday August 28, 2017 \u2022  Continue above activities \nIncorporation of radar \ndata into ENACTS \ndatasets \nTuesday, August 29, 2017 \u2022 Review of all available radar data at \nM\u00e9t\u00e9o Rwanda \n\u2022 Writing scripts for processing radar data \nand adjusting radar rainfall estimates with \nrain gauge observations \nWednesday, August 30, \n2017 \nThursday, August 31, \n2017 \nFriday, September 1, \n2017 \n \n  \n  28 \nAppendix 2: Participant List \nName Title Institute Gender \nAmos Uwizeye Data Quality Control M\u00e9t\u00e9o Rwanda M \nErnest Bagambiki System Administrator M\u00e9t\u00e9o Rwanda M \nFid\u00e8le Maniraguha Senior Radar and Remote Sensing Eng M\u00e9t\u00e9o Rwanda M \nJoyce K Rusaro Meteorological App Officer M\u00e9t\u00e9o Rwanda F \nFelix Mucyo Observations Supervisor M\u00e9t\u00e9o Rwanda M \nJacqueline Uwimbabazi Meteorological App Officer M\u00e9t\u00e9o Rwanda F \nPeace Bamurange Meteorological App Officer M\u00e9t\u00e9o Rwanda F \nVedaste Iyakaremye Observations Processing Officer M\u00e9t\u00e9o Rwanda M \nGodfrey Musafiri Meteorological App Officer M\u00e9t\u00e9o Rwanda M \nEmmanuel Rukundo Observations Supervisor M\u00e9t\u00e9o Rwanda M \nSerge H. Senyana Meteorological App Officer M\u00e9t\u00e9o Rwanda M \nClarrisse Mukazarukundo Observations Officer M\u00e9t\u00e9o Rwanda F \nHerv\u00e9 Murenzi Observations Officer M\u00e9t\u00e9o Rwanda M \nJoseph Ndakize Sabaziga Forecasting Officer M\u00e9t\u00e9o Rwanda M \nFloribert Vuguziga Senior Meteorological App Officer M\u00e9t\u00e9o Rwanda M \nRija Faniriantsoa Trainer IRI M \n \n \n",
    "source": ""
  },
  {
    "id": 23052499,
    "doi": null,
    "title": "Establishing links between organizational climate, employee well-being and historical patient outcomes",
    "abstract": "This research undertaken in collaboration with Queensland Health analysed the links between dimensions of workplace climate/employee well-being contained in a number of\n\nQueensland Health databases, including the Patient Satisfaction Survey, the Clinical Incident database, the compliments and complaints database, the Variable Life Adjusted Display (VLAD) Database and the Better Workplaces\n\nStaff Opinion Survey database. Queensland Health sought to identify in what ways workplace climate is related to patient outcomes using existing datasets collected within the Queensland Health Centre for Healthcare Improvement. The process of establishing links involved matching aggregated data for specific facilities (where possible), or failing that, larger facilities (e.g. Hospital), or the Health Service District. Once the datasets had been matched on location or facility, correlations were calculated between the aggregated scores. The results demonstrated links between the data sets. These links showed that a better workplace climate is associated with greater reported numbers of clinical incidents, especially \u201cno harm\u201d clinical incidents. There was also a link between workplace climate and patient compliments/complaints which show that unsolicited compliments received from patients and their families are clearly related to a number of positive aspects of workplace climate (workplace morale, role clarity, and appraisal and recognition) and individual\n\nmorale. The results linking workplace climate and patient satisfaction showed that there is a strong positive relationship between overall patient satisfaction and role clarity, and a negative relationship between overall patient satisfaction and both workplace distress and\n\nexcessive work demands. While these results relate to historical data and therefore should not be construed to reflect the current state of operation within Queensland Health, they are still indicative of some very important\n\nrelationships. This is the first study to demonstrate that more positive clinical management practices, better perceptions of the workplace climate and better employee\n\nwell-being are a reflection of a better incident reporting and learning culture in a health care organization, ultimately resulting in improved patient outcomes",
    "fullText": "Abstracts of the 27th International Congress of Applied Psychology   324 Symposia  ICF framework.  The presentation emphasizes neuropsychological assessment\u2019s effectiveness for measuring an individual\u2019s abilities related to body structures, body functions, identification of limitations and abilities in activities and participation, and the individual\u2019s ability to perform in major life areas such as work and education.  The merits and limitations of utilizing qualitative and quantitative methods of neuropsychological evaluation are reviewed. A brief representation of assessment methods for providing both qualitative and quantitative information for all major aspects of cognition will be presented, with an emphasis upon their ecological validity in respect to prediction of functional skills.  The functional implications of considering assessment from a holistic perspective, including consideration of the individual\u2019s culture or other, will be emphasized. Qualities that may impact test performance also are identified.  Finally, the role of the rehabilitation neuropsychologist in the context of the treatment team is outlined, including methods for increasing the effectiveness of the rehabilitation team\u2019s efforts. Neuropsychological assessment in the rehabilitation and other health-related settings provides information for various needs, including diagnosis, treatment planning and intervention, and establishment of competencies.  Many aspects of ICF classification are pertinent for neuropsychological assessment. Obtained data are meaningful only in a person\u2019s holistic context.   Keywords: neuropsychological assessment, International Classification of Functioning, rehabilitation, health care services, cognition Improving employee engagement, well-being and performance: New tools, interventions, and solutions Chair(s): MACHIN, T. (University of Southern Queensland)  Discussant(s): FOGARTY, G. (University of Southern Queensland)  This symposium will focus on developing better tools, interventions, and solutions to improve important individual and organizational outcomes such as engagement, well-being, and performance. The presenters will represent developments from three continents (Australia, North America, and Europe). The key learning outcomes are: (1) To increase the awareness of researchers of the critical factors that influence employees\u2019 engagement with organizational initiatives; (2) To provide managers with solutions that impact positively on organizational and employee well-being; (3) To assist employees to better understand the impact of organizational variables on their well-being and performance; (4) To encourage the deployment of interventions that impact positively on employees\u2019 well-being and performance. Establishing links between organizational climate, employee well-being and historical patient outcomes MACHIN, T. (University of Southern Queensland), GOH, H. E. (University of Southern Queensland), PATRICK, J. (University of Southern Queensland), JURY, C. (Queensland Health) This research undertaken in collaboration with Queensland Health analysed the links between dimensions of workplace climate/employee well-being contained in a number of Queensland Health databases, including the Patient Satisfaction Survey, the Clinical Incident database, the compliments and complaints database, the Variable Life Adjusted Display Abstracts of the 27th International Congress of Applied Psychology   325 Symposia  (VLAD) Database and the Better Workplaces Staff Opinion Survey database. Queensland Health sought to identify in what ways workplace climate is related to patient outcomes using existing datasets collected within the Queensland Health Centre for Healthcare Improvement. The process of establishing links involved matching aggregated data for specific facilities (where possible), or failing that, larger facilities (e.g. Hospital), or the Health Service District. Once the datasets had been matched on location or facility, correlations were calculated between the aggregated scores. The results demonstrated links between the data sets. These links showed that a better workplace climate is associated with greater reported numbers of clinical incidents, especially \u201cno harm\u201d clinical incidents. There was also a link between workplace climate and patient compliments/complaints which show that unsolicited compliments received from patients and their families are clearly related to a number of positive aspects of workplace climate (workplace morale, role clarity, and appraisal and recognition) and individual morale. The results linking workplace climate and patient satisfaction showed that there is a strong positive relationship between overall patient satisfaction and role clarity, and a negative relationship between overall patient satisfaction and both workplace distress and excessive work demands. While these results relate to historical data and therefore should not be construed to reflect the current state of operation within Queensland Health, they are still indicative of some very important relationships. This is the first study to demonstrate that more positive clinical management practices, better perceptions of the workplace climate and better employee well-being are a reflection of a better incident reporting and learning culture in a health care organization, ultimately resulting in improved patient outcomes. Keywords: climate, well-being, Queensland Health, job demands, patient outcomes Alleviating burnout through enhancing civility among colleagues  LEITER, M. (Acadia University), LASCHINGER, H. S. (University of Western Ontario), DAY, A. (St Mary's University), GILIN-OORE, D. (St Mary's University) Hospital units implemented CREW (Civility, Respect, and Engagement with Work) to enhance collegiality. The research evaluated the impact of the procedure on collegial and supervisory relationships, attitudes towards work, job burnout, and work engagement. We implemented surveys before and after the CREW process on the eight intervention units and 32 control units. The survey assessed collegiality (civility and incivility among colleagues), job burnout, attitudes (commitment, satisfaction), perceptions of worklife (workload, control, reward, community, fairness, and values), and absences. The CREW process was implemented through weekly sessions facilitated by hospital personnel with mentoring from research staff. CREW units improved across the full range of measures. For example, a MANOVA for the Intervention group on the social context variables-civility, coworker incivility, supervisor incivility, instigated incivility, and respect-produced a significant improvement from Time 1 to Time 2 (F(5,413) = 6.271, p < .001, \u03b7 = .070). A 2 X 2 MANOVA for the social context variables produced a significant interaction of Time with Intervention (F(5,1958) = 3.25, p = .006, \u03b7 = .008), confirming a greater improvement in civility among intervention participants. Further, a MANOVA for the Intervention group on the burnout measures\u2014exhaustion and cynicism\u2014and turnover intention\u2014 produced a significant improvement from Time 1 to Time 2 (F(3,418) = 11.58, p<.001, \u03b7 = .077), and a 2 X 2 MANOVA for the burnout and turnover intention variables produced a significant interaction of Time with Intervention (F(3,1977) = 3.066 p=.027, \u03b7 = .005). The results confirm that the CREW process has a significant impact on the quality of interaction among colleagues. The results further support the proposition that improvements in psychological connections with work, job attitudes, and job behaviors accompany these changes in collegiality. The discussion considers the implications for the CREW process and for collegiality for workplace health. The discussion also considers the pivotal role of collegial relationships in the ",
    "source": "Australian Psychological Society"
  },
  {
    "id": 138010211,
    "doi": "10.1080/01431160500239198",
    "title": "Changing patterns of global-scale vegetation photosynthesis, 1982-1999",
    "abstract": "The primary objective of this research was to assess changes in global vegetation photosynthesis between 1982 and 1999. Global-scale Advanced Very High Resolution Radiometer (AVHRR) Normalized Difference Vegetation Index (NDVI) data from the Pathfinder AVHRR Land (PAL) and Global Inventory Modeling and Mapping Studies (GIMMS) datasets were analysed for 96% of the non-Antarctic land area of the Earth. The results showed that between 1982 and 1999 over 30% of the Earth's land surface increased and less than 5% decreased in annual average photosynthesis greater than 4%. Although both the PAL and GIMMS datasets produced broadly similar patterns of change, there were distinct differences between the two datasets. Changes in vegetation photosynthesis were occurring in spatial clusters across the globe and were being driven by climate change, El Nino-Southern Oscillation (ENSO) events and human activity",
    "fullText": "a.                b.  0.050.10.150.20.250.30.350.40.450.50.5582 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99Coastal Peru0.050.10.150.20.250.30.350.40.450.50.5582 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99West Australia East Australia   c.                d. 0.050.10.150.20.250.30.350.40.450.50.5582 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99Central Australia - Decline0.050.10.150.20.250.30.350.40.450.50.5582 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99West Australia Increase   e.                 f.  0.050.10.150.20.250.30.350.40.450.50.5582 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99Guangzhou0.050.10.150.20.250.30.350.40.450.50.5582 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99Bolivia  ",
    "source": "TAYLOR & FRANCIS LTD"
  },
  {
    "id": 133478006,
    "doi": "10.5334/dsj-2022-010",
    "title": "Quality Management Framework for Climate Datasets",
    "abstract": "Data from a variety of research programmes are increasingly used by policy makers, researchers, and private sectors to make data-driven decisions related to climate change and variability. Climate services are emerging as the link to narrow the gap between climate science and downstream users. The Global Framework for Climate Services (GFCS) of the World Meteorological Organization (WMO) offers an umbrella for the development of climate services and has identified the quality assessment, along with its use in user guidance, as a key aspect of the service provision. This offers an extra stimulus for discussing what type of quality information to focus on and how to present it to downstream users. Quality has become an important keyword for those working on data in both the private and public sectors and significant resources are now devoted to quality management of processes and products. Quality management guarantees reliability and usability of the product served, it is a key element to build trust between consumers and suppliers. Untrustworthy data could lead to a negative economic impact at best and a safety hazard at worst. In a progressive commitment to establish this relation of trust, as well as providing sufficient guidance for users, the Copernicus Climate Change Service (C3S) has made significant investments in the development of an Evaluation and Quality Control (EQC) function. This function offers a homogeneous user-driven service for the quality of the C3S Climate Data Store (CDS). Here we focus on the EQC component targeting the assessment of the CDS datasets, which include satellite and in-situ observations, reanalysis, climate projections, and seasonal forecasts. The EQC function is characterised by a two-tier review system designed to guarantee the quality of the dataset information. While the need of assessing the quality of climate data is well recognised, the methodologies, the metrics, the evaluation framework, and how to present all this information to the users have never been developed before in an operational service, encompassing all the main climate dataset categories. Building the underlying technical solutions poses unprecedented challenges and makes the C3S EQC approach unique. This paper describes the development and the implementation of the operational EQC function providing an overarching quality management service for the whole CDS data.This study is based on work carried out in the C3S_512 contract funded by Copernicus Programme and operated by ECMWF on behalf of the European Commission (Service Contract number: ECMWF/COPERNICUS720187C3S_512_BSC). We would like to acknowledge the work of colleagues from several European institutions, the data providers and C3S, who contributed to the development of the EQC framework as well as to the QAR production. We would also like to acknowledge the focus group users, who took time to review and provide valuable feedback on the QARs, QATs, minimum requirements and the CDS quality assessment tab. The authors are grateful to the anonymous reviewers for their constructive comments that have helped for the improvement of this paper.Peer Reviewed\"Article signat per 23 autors/es:  Carlo Lacagnina , Francisco Doblas-Reyes, Gilles Larnicol, Carlo Buontempo, Andr\u00e9 Obreg\u00f3n, Montserrat Costa-Sur\u00f3s, Daniel San-Mart\u00edn, Pierre-Antoine Bretonni\u00e8re, Suraj D. Polade, Vanya Romanova, Davide Putero, Federico Serva, Alba Llabr\u00e9s-Brustenga, Antonio P\u00e9rez, Davide Cavaliere, Olivier Membrive, Christian Steger, N\u00faria P\u00e9rez-Zan\u00f3n, Paolo Cristofanelli, Fabio Madonna, Marco Rosoldi, Aku Riihel\u00e4, Markel Garc\u00eda D\u00edez\"Postprint (published version",
    "fullText": "RESEARCH PAPERCORRESPONDING AUTHOR:Carlo LacagninaBarcelona Supercomputing Center, Barcelona, Spaincarlo.lacagnina@bsc.esKEYWORDS:data quality; trust; FAIR; climate services; scalabilityTO CITE THIS ARTICLE:Lacagnina, C, Doblas-Reyes, F, Larnicol, G, Buontempo, C, Obreg\u00f3n, A, Costa-Sur\u00f3s, M, San-Mart\u00edn, D, Bretonni\u00e8re, P-A, Polade, SD, Romanova, V, Putero, D, Serva, F, Llabr\u00e9s-Brustenga, A, P\u00e9rez, A, Cavaliere, D, Membrive, O, Steger, C, P\u00e9rez-Zan\u00f3n, N, Cristofanelli, P, Madonna, F, Rosoldi, M, Riihel\u00e4, A, D\u00edez, MG. 2022. Quality Management Framework for Climate Datasets. Data Science Journal, 21: 10, pp. 1\u201325. DOI: https://doi.org/10.5334/dsj-2022-010Quality Management Framework for Climate DatasetsCARLO LACAGNINA FRANCISCO DOBLAS-REYES GILLES LARNICOL CARLO BUONTEMPO ANDR\u00c9 OBREG\u00d3N MONTSERRAT COSTA-SUR\u00d3S DANIEL SAN-MART\u00cdN PIERRE-ANTOINE BRETONNI\u00c8RE SURAJ D. POLADE VANYA ROMANOVA DAVIDE PUTERO FEDERICO SERVA ALBA LLABR\u00c9S-BRUSTENGA ANTONIO P\u00c9REZ DAVIDE CAVALIERE OLIVIER MEMBRIVE CHRISTIAN STEGER N\u00daRIA P\u00c9REZ-ZAN\u00d3N PAOLO CRISTOFANELLI FABIO MADONNA MARCO ROSOLDI AKU RIIHEL\u00c4 MARKEL GARC\u00cdA D\u00cdEZ *Author affiliations can be found in the back matter of this articleABSTRACTData from a variety of research programmes are increasingly used by policy makers, researchers, and private sectors to make data-driven decisions related to climate change and variability. Climate services are emerging as the link to narrow the gap between climate science and downstream users. The Global Framework for Climate Services (GFCS) of the World Meteorological Organization (WMO) offers an umbrella for the development of climate services and has identified the quality assessment, along with its use in user guidance, as a key aspect of the service provision. This offers an extra stimulus for discussing what type of quality information to focus on and how to present it to downstream users. Quality has become an important keyword for those working on data in both the private and public sectors and significant resources are now devoted to quality management of processes and products. Quality management guarantees reliability and usability of the product served, it is a key element to build trust between consumers and suppliers. Untrustworthy data could lead to a negative economic impact at best and a safety hazard at worst. In a progressive commitment to establish this relation of trust, as well as providing sufficient guidance for users, the Copernicus Climate Change Service (C3S) has made significant investments in the development of an Evaluation and Quality Control (EQC) function. This function offers a homogeneous user-driven service for the quality of the C3S Climate Data Store (CDS). Here we focus on the EQC component targeting the assessment of the 2Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-0101. INTRODUCTIONClimate change and variability pose an unprecedented challenge to the overall society, which requires mitigation and adaptation responses to reduce the threats and maximise the opportunities presented to organisations of all kinds. The impacts of climate variability and change can take various forms such as physical, social, financial, or political, and as such climate change adaptation has a very broad scope. Both business and public administrations are vulnerable to potentially disruptive risks and are key actors in the creation of a climate-resilient future (ISO 14090:2019; ISO 14091:2021).Both monitoring and modelling of the Earth system can provide the information and guidance necessary for the policy and decision makers to deal with climate-related challenges. This has led to the establishment of various initiatives designed to better understand the Earth system through an improvement in both observational capabilities and modeling tools. As a result, an increasing amount of environmental data about past, present, and future climate is becoming available. Unfortunately, these data often come with inconsistent or missing metadata, inhomogeneous documentation, and sometimes sparse evidence concerning their uncertainty and validation. A variety of data streams is generated independently and from multiple sources, adhering to different definitions and assumptions, often not standardised across communities, and, at times, with overlapping but disconnected objectives. As a consequence, the users can feel disoriented when it comes to identifying the most appropriate dataset for an intended application (Nightingale et al. 2019).Given the ever more prominent role that climate products are assuming in decision making, it is unavoidable that the quality of these data will come under increasing scrutiny in the future. Climate services are emerging as the link to narrow down the gap between upstream climate science and downstream users. Climate services form the backbone of the process that translates climate knowledge and data into bespoke products for decision making in diverse sectors of society, ranging from public administrations to private business (Hewitt et al. 2020; Medri et al. 2012). The Global Framework for Climate Services (GFCS) of the World Meteorological Organization (WMO) stresses the increasing need for robust climate information based on observations and simulations covering future periods, ranging from several months up to centuries for economic, industrial, and political planning. Moreover, climate services play a crucial role in disseminating relevant standards (GFCS WMO) fostering adoption of common data models and formats and with sufficient metadata uniformly stored. The GFCS offers an umbrella for the development of climate services and has identified the quality assessment, along with its use in user guidance, as a key aspect of the service. The services, and the quality assessments in particular, need to be provided to users in a seamless manner and need to respond to user requirements.1 The ultimate goal is building trust between data providers and users, as well as maximising usage uptake (Callahan et al. 2017; Rfll 2020). Thus, the questions of what type of quality information to provide and how to present it to users are receiving sustained attention.1 https://public.wmo.int/en/bulletin/what-do-we-mean-climate-services.CDS datasets, which include satellite and in-situ observations, reanalysis, climate projections, and seasonal forecasts. The EQC function is characterised by a two-tier review system designed to guarantee the quality of the dataset information. While the need of assessing the quality of climate data is well recognised, the methodologies, the metrics, the evaluation framework, and how to present all this information to the users have never been developed before in an operational service, encompassing all the main climate dataset categories. Building the underlying technical solutions poses unprecedented challenges and makes the C3S EQC approach unique. This paper describes the development and the implementation of the operational EQC function providing an overarching quality management service for the whole CDS data.3Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010A relatively young operational climate service is the Copernicus Climate Change Service (C3S, Th\u00e9paut et al. 2018), one of the six operational thematic services established by the European Commission within the Copernicus Earth Observation Programme (EC 2020). The C3S, implemented by the European Centre for Medium-Range Weather Forecasts (ECMWF), aims to be an authoritative source of climate information for a wide variety of downstream users, ranging from policy makers to industrial sectors. The backbone of C3S is a cloud-based Climate Data Store (CDS), designed to be a single point of access to a catalogue of climate datasets of different categories, including in-situ and satellite observations, seasonal forecasts, reanalysis, and climate projections. In a progressive commitment to establish relations of trust between data providers and downstream users, as well as providing sufficient guidance for users to address their specific needs, C3S has made significant investments in the development of an Evaluation and Quality Control (EQC) function. By being transparent and characterising data quality attributes in a traceable and reproducible way, C3S is setting the basis for the inclusion of reliable climate data into policies and actions.The establishment of an EQC function has three main advantages: i) it guides the users into the documentation to understand properly the dataset, it simplifies the comparison across datasets and builds trust in the products available, ii) it helps the data providers to understand which information they need to deliver to be compliant with standards, increases data uptake from the users, and clarifies how to provide standardised dataset quality information, and iii) it triggers actions for the service improvement leading to the provision of the most relevant dataset information and ensuring that published datasets are mature enough to support the authoritative character of C3S. The relation of trust between data providers and downstream users generated by these advantages is a necessary condition to foster a flourishing market for climate services (Zeng et al. 2019).While the need for a quality assessment of the data available on the CDS is well recognised, the methodologies, the metrics, the assessment framework, and the way to present all this information have never been developed before in an operational service that disseminates all the main climate dataset categories. The proof-of-concept framework described in Nightingale et al. (2019) focused on observations only. Instead, here we implement the operational delivery of homogeneous EQC information across all the CDS climate dataset categories mentioned before. This objective, along with the task of building the underlying technical solutions, posed an unprecedented design challenge and makes the C3S EQC unique. The framework for the operational EQC function of the CDS builds on past and present international research initiatives. A variety of concepts were developed in previous projects (mainly the EU FP7 funded QA4ECV2 and the C3S_51 Lots 2-43), most of these concepts were integrated while moving the EQC function towards its operational phase.This paper describes the challenges, the development and the implementation of the operational EQC function providing an overarching quality assurance service for the whole CDS. The results of the dataset assessments are publicly available in the CDS Catalogue.4 The manuscript focuses on the framework implemented for the CDS datasets only, leaving out other aspects of the EQC function, such as the assessment of the software made available by the CDS to explore the datasets (i.e. Toolbox), the assessment of the CDS infrastructure and the assessment of the user\u2019s satisfaction and requirements.2. THE EQC FRAMEWORK IN A NUTSHELLCommitment to quality is fundamental to build trust among stakeholders and the EQC function is the key element of C3S devoted to reach this goal. The main purpose of the EQC framework is to provide the C3S with a consistent, structured, and pragmatic approach to enhance the dataset reliability and usability. The design of this framework adopts an iterative approach integrating continual learning and improvement.2 http://www.qa4ecv.eu/.3 https://climate.copernicus.eu/c3s51-evaluation-and-quality-control-function-climate-data-store.4 https://cds.climate.copernicus.eu.4Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010The EQC function regularly informs and recommends C3S about drawbacks, shortcomings and limitations related to the CDS datasets. These analyses are completed by a continuous user-engagement process to identify the user expectations that need to be addressed. The EQC team provides technical and scientific quality information of the CDS datasets via a set of homogeneous Quality Assurance Reports (QARs), helping to set the minimum requirements and baseline criteria for including new datasets in the CDS Catalogue. The QARs are filled templates called Quality Assurance Templates (QATs). Consistency across the QATs is obtained through the adoption of a vocabulary of homogeneous concepts and common practices.The general strategy for assessing the CDS datasets consists of five steps: \u2022\t designing QATs for all the dataset categories with a consistent terminology and a structure as similar as possible; \u2022\t interacting with the data providers, who are the ones with the best knowledge of their datasets, and encouraging them to fill in the QATs or, in case the data provider is not available, fill in the templates using the documentation publicly available; \u2022\t evaluating the content collected in the QATs, paying attention to ensure that the content is understandable for the users, the level of detail is similar across datasets, and the type of information is complete, correct and consistent with the template requests;\u2022\t performing an independent quality assessment of the dataset, looking at aspects like (meta)data completion and integrity, scientific soundness and other characteristics that illustrate the multi-faceted nature of data quality; and\u2022\t publishing the information in the CDS dataset catalogue, once the QAR is approved by the corresponding authority, which in this case is the C3S governance board.The production of the QARs calls for setting the procedures to initiate, develop and update the QARs (e.g., workflow), developing the software tools to support the assessments (e.g., data checker), and engaging with a wide range of stakeholders to choose the most adequate options for the QARs. These steps lead to the creation of QARs, which provide users with comprehensive information about the technical and scientific quality of the datasets. The different sections of the QARs are made accessible to the users in the CDS web portal through a synthesis table. The synthesis table is devised as a tool to organise and homogenise the EQC information, which is made of atomic elements corresponding to the different entries of this table. These entries contain links leading the user to the respective subsection of the QAR, where the user can find the EQC information of interest.The overall EQC framework is guided by homogeneity and scalability approaches. The former leads to consistency of the EQC information across the CDS dataset categories, the latter leads to the integration of automatic tools to produce timely and sustainable data assessments in an operational environment. In particular, the EQC framework is driven by:\u2022\t a modular and flexible system able to consider new data/information sources and new actors involved;\u2022\t as much as possible automation of information acquisition (e.g., variable description, metadata checks) and its update in order to reduce human errors, speed up the QAR production, and make the system sustainable in the long-term;\u2022\t an iterative and reproducible approach permeable to the evolving requirements of both users and C3S to ensure continuous improvement;\u2022\t a user-friendly presentation of the quality information provided, clustered to facilitate its consultation and uptake to facilitate users in making their own decisions about climate data;\u2022\t consistent provision of the CDS dataset quality information, recognising the existence of inherent differences across the dataset categories;\u2022\t transparency and traceability of the quality assessments;\u2022\t FAIR (Wilkinson et al. 2016) and TRUST (Lin et al. 2020) principles and ISO 19157:2013;\u2022\t service management practices to make the EQC activities resilient in an operational environment.5Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010Finally, the guidelines provided in Peng et al. (2021) have been followed when developing the EQC framework, as shown in Table 1.FAIR-DQI GUIDELINES (PENG ET AL. 2021)EQC FRAMEWORK DESCRIBED IN THIS PAPERGuideline 1: dataset The dataset is described with a comprehensive online page providing various information that includes DOI, rich metadata, and licence.Guideline 2: assessment modelThe assessment method is available online together with the quality information. This paper itself details further the assessment model used. The assessment model is versioned and publicly retrievable.Guideline 3: quality metadataThe assessments are captured into a structured schema/template (QAT). The quality information is standardised in a machine-readable (in our case using the CMS) and reusable form.Guideline 4: assessment reportThe quality information is structured in a template and is accessible online, versioned, and human-readable.Guideline 5: reporting The assessments are disseminated in an organized way via a web interface including the quality aspects assessed, the evaluation method, and how to understand and use the quality information.3. DATASET QUALITY INFORMATION AND ITS DISSEMINATIONThe QAT is the tool used to gather information on the most relevant aspects of the CDS datasets, informing the user in a quicker way rather than accessing and reading several documents (e.g., user guides, peer-reviewed papers, dataset descriptions). The QAT includes all the relevant quality information, in a concise and standardised form, with references and links leading to further details.The general strategy is to provide seamless QATs, which are as homogenous as possible across all dataset categories. The QATs for each dataset category (i.e., satellite and in-situ observations, reanalysis, seasonal forecasts, climate projections) are available as supplementary material. The QATs are regularly reviewed to gradually converge towards harmonisation. Much improvement has been achieved by adopting a common terminology (see section 6) and common minimum requirement fields. The homogenisation of the QATs of different dataset categories ideally tends towards adopting one single QAT for all datasets. However, this goal is not feasible due to the diverse nature of the CDS dataset categories (concepts like \u2018processing level\u2019 or \u2018quality flag\u2019 are relevant for observational datasets, but not for other categories; along the same lines, the concept of \u2018ensemble size of the hindcast\u2019 is mostly relevant for seasonal forecasts). This homogenisation effort was pragmatically addressed by mapping the different QATs, one for each category, onto a general table agnostic of the dataset type.In practice, all the QAT fields were grouped under main sections and subsections with common names for all dataset categories. An excerpt of the resulting QAT is reported in Figure 1. Having common names for sections and subsections to all the QATs allows to organise and homogenise the EQC information in a general table named synthesis table (Figure 2).Figure 1 QAT excerpt: the cells of the synthesis table (Figure 2) correspond to the subsections of the QAT (yellow text), while the column titles of the table correspond to the QAT sections (white text). The fields with an asterisk indicate the minimum requirements (see section 3.1). The QAT questions are in the grey area (left column). The middle column defines the data type, the rightmost column reports guidance about the type of content expected. Text in cyan appears as a tooltip when hovering the mouse over the web-form of the synthesis table.Table 1 Mapping of the Peng et al. (2021) guidelines to the EQC framework characteristics described here.6Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010The synthesis table entries contain links leading the user to the respective subsection of the QAR (i.e., filled QAT), where the user can find the EQC information of interest. Therefore, the structure of the synthesis table is agnostic of the dataset category, while the QAT fields, within each subsection (the information that is displayed when clicking on a cell in the synthesis table), depend on the dataset category.The synthesis table offers an effective approach to guide the users into the documentation and homogenise the access to it. It addresses a typical user requirement: \u2018most of the time, the problems with the documentation are not due to the lack of it, but to the difficulty in finding it\u2019 (extracted from the C3S User Requirement Analysis Document 09/2019) or \u2018all documentation should be easy-to-access\u2019 (Nightingale et al. 2019). For instance, a non-expert user might not know the meaning of ATBD (Algorithm Theoretical Baseline Document) and the synthesis table overcomes this complication by guiding the user through questions (i.e., QAT fields), answered with high-level information further detailed in the complete document referred (ATBD in this case). Moreover, the synthesis table offers an extra level of assurance through independent assessments and guarantees the user that all the information made available through this table is traceable and quality controlled, because the information given by the provider is double checked by the EQC team and is versioned in the CMS. An extra advantage of the synthesis table is the possibility to track which EQC material the user is interested in by recording the user\u2019s actions in the table. These actions can be analysed in a later stage to steer the future decisions of the EQC function and C3S in general. The information accessible through the synthesis table may be grouped into two categories:\u2022\t Descriptive data information. Documentation has been selected to tackle data provenance, showing the origin, history, and methodology used to create the data. This information is available prominently in the column \u2018user documentation\u2019 of the synthesis table (Figure 2), in particular in the scientific methodology part. The documentation is completed by references to more detailed material, such as uncertainty characterisation, license, citation, and the like, for further user queries. Information here is the result of the documentation and accessibility assessments. In general, content is filled in by the data provider and reviewed by the EQC team. \u2022\t Independent assessments. An analysis of the dataset quality is performed independently of the provider, with the advantage of using the same metrics and tools nonpartisan of the source where the dataset was generated. This guarantees a uniform and impartial basic evaluation across datasets. Information here is the result of the technical and scientific assessments. See Appendix III for a definition of \u2018technical\u2019 and \u2018scientific\u2019 in this context.The table is characterized by fields grouped into columns (Figure 2). The column with the header \u2018introduction\u2019 gives a quick overview of the data characteristics (e.g., name, provider, time resolution), as inspired by the WIGOS guide on metadata standards (WMO/WIGOS 2017). The column \u2018user documentation\u2019 provides the essential documentation for the effective use and understanding of the dataset (e.g., user guide). The column \u2018access\u2019 describes whether the dataset variable can be served by the CDS Toolbox and which are the archiving practices followed for this dataset. Finally, the column \u2018independent assessment\u2019, being more articulated, is explained in detail in Appendix I.Figure 2 Synthesis table, conceived as a tool to organise and homogenise the EQC information, as well as to guide users through the documentation. The table fields, each identifying an aspect of the dataset, are grouped into columns. Note the correspondence between the field \u2018dataset overview\u2019 of the column \u2018Introduction\u2019 and Figure 1.7Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-0103.1 MINIMUM REQUIREMENTS FOR PUBLICATION OF CDS DATASETSSome of the QAT entries are considered mandatory and some optional in the EQC framework. The content of the mandatory entries is considered so fundamental that, when missing, the dataset is not usable/understandable and thus unservable. These mandatory fields define the minimum requirements (MRs) for a dataset variable to be published (or withdrawn) by the CDS. The identification of these fields probably represents the first systematic effort towards the inclusion and development of an operational check of MRs, encompassing a wide range of dataset categories. Indeed, the identification of a suitable set of MRs was indicated among the \u2018Science Gaps\u2019 in assessing climate data quality by Nightingale et al. (2019). The list of MRs is specifically thought to facilitate a timely publication of a dataset in the CDS, ensuring, at the same time, a sufficient (but not necessarily optimal) quality of the dataset. The MRs cover several aspects ranging from the dataset documentation to the compliance of metadata with community standards. The fields were identified as a result of the interaction between the EQC team, data providers, C3S, and users. The analysis of the MRs leads to recommendations to the C3S governance board on whether a dataset shall be made public on (or withdrawn from) the CDS. To guarantee the maintainability of the MRs, they are an integral part of the QATs and are updated with the same frequency. See the supplementary material for a complete list of the MRs, indicated by an asterisk next to the QAT entry. Typical examples of MRs are \u2018data format\u2019, \u2018physical quantity name\u2019, \u2018user guide documentation\u2019 or \u2018validation activity description\u2019. Beyond the mandatory text necessary to fill in the QAT entries, a number of documents are also requested to be linked in the QATs as minimum requirements. These are:\u2022\t dataset user guide (e.g., seasonal forecasts SEAS5 user guide5). In the satellite observations community, this is usually referred to as PUG (Product User Guide);\u2022\t documentation describing processing of the dataset or a model/system technical documentation, including the description of the different components. In the QAT this document answers the question labelled \u2018model/system technical documentation\u2019 (e.g., reanalysis UERRA6). In the satellite observations community, this is usually referred to as ATBD (Algorithm Theoretical Baseline Document);\u2022\t product traceability chain. Only mandatory for reference datasets (e.g., in-situ observations GRUAN humidity7). Definition of reference in this context is given by GCOS8; and\u2022\t uncertainty characterisation and validations reports. In the QAT these documents answer questions about \u2018validation or inter-comparison or uncertainty characterisation activities performed\u2019 (e.g. climate projections CORDEX-CCLM9). In the satellite observations community, this is usually referred to as PQAD (Product Quality Assurance Document) and PQAR (Product Quality Assessment Report).Before publication in the CDS, it is essential that the documents listed above are made available alongside the datasets they refer to.The current version of the MRs fits the existing technology infrastructure as well as available human resources. Ideally, the list shall be extended to include basic technical checks about the data and metadata, such as time and space consistency and completeness, physical plausibility. However, it would require a technical infrastructure that was not available on the CDS at the time. In particular, it requires setting up automatic tools (a data checker software available for all the dataset categories) and tackling technical challenges (downloading and queuing time, memory disk space, enforcement of common metadata standards). Solving these technical limitations will help to extend the MRs list homogeneously across all the dataset categories.5 https://www.ecmwf.int/sites/default/files/medialibrary/2017-10/System5_guide.pdf.6 https://confluence.ecmwf.int/display/UER.7 http://datastore.copernicus-climate.eu/documents/in-situ/GRUAN_product_traceability_chain_GAIA_CLIM_Humidity.pdf.8 https://gcos.wmo.int/en/.9 https://gmd.copernicus.org/articles/7/1297/2014/.8Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-0104. DEVELOPMENT OF THE TECHNICAL SOLUTIONBuilding the framework of the EQC requires designing protocols, software tools, QATs, and workflows for the QAR production as well as following common vocabularies and practices (e.g., TRUST Principles for Data Repositories, Lin et al. 2020). Among these, we focus now on the technical solutions underlying the EQC framework. Substantial technical developments have been undertaken during the onset of the operational phase of the EQC and more will be needed while it gets more mature over time:\u2022\t a Content Management System (CMS) and its maintenance, more details below;\u2022\t a Drupal-based module, inspired by the shiny-app R package,10 to show dynamic plots resulting from the scientific assessments;\u2022\t the integration of the EQC tab into the CDS infrastructure and its synchronisation with the other catalogue elements (e.g., download tab);\u2022\t software packages adapted to perform the scientific assessment tailored to the CDS characteristics (e.g., the ESMValTool11 was adapted for climate projections analyses);\u2022\t a data-checker software to scrutinise CDS data and metadata;\u2022\t compatibility tests to check whether the data variables can be served through the CDS Toolbox; and\u2022\t setting up the software infrastructure, such as a network of virtual machines with the right environment and a Git manager for software repository, data flow architecture, and so on.4.1 CONTENT MANAGEMENT SYSTEM (CMS)At the heart of the EQC assessments is the Content Management System (CMS), an application used to manage content stored in a database and displayed in a presentation layer based on a set of templates, i.e. the QATs. Its objective is to ease the collaborative definition of the QAT structure and to facilitate and manage the creation of the QAT content. Creation of the QAT content is partially automated, as detailed in section 5.2.The CMS facilitates the QAR production following a workflow that involves several roles, described in Table 2, that access the CMS sequentially.ROLE RESPONSIBILITYEQC main contactOne EQC team member who acts as the main contact of a specific dataset category. As QAR production is a multi-actor process, it is important that there is a central person, the EQC main contact, to coordinate the QAR production. This member contacts the data providers to agree on when they are available to fill in the QAT. Once the link with the providers is established, the EQC main contact defines the QAR name, fills in the QAT entries that identify the QAR uniquely and selects the team involved in the QAR production. Eventually, the EQC team member lets the actors involved know where there is a potential issue before it impacts the production.Data providerTypically, a member of the team that provided the CDS with the dataset under evaluation. The providers fill in the information requested in the QAT, because they are considered the best source to fully describe their datasets and so are the preferential choice for this task.Evaluator An EQC member who vets the QAR content and fills in the independent assessment fields. This role interacts with the provider for guidance about the amount and type of content expected and for any clarification needed.Reviewer An EQC member who scrutinises the whole QAR content for completeness and understandability. The reviewer is fundamental, because of her/his work in checking and verifying the correctness and consistency of all information introduced, while interacting with the evaluator to address any issue encountered.Approver Role covered by one C3S governance board member, who makes decisions about the publication of the dataset, (also) based on the QAR, and conducts a final check of the QAR before making it public together with the dataset. If the QAR requires further review, it will be sent back to the EQC team, commenting about what is still needed. Otherwise, it will be published in the CDS.10 https://shiny.rstudio.com/.11 https://www.esmvaltool.org/.Table 2 Roles involved in the QAR production workflow.9Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010Figure 3 sketches the interaction between the roles involved in the generation of the QARs. The implementation of this workflow into the QAR production needs more consideration; it needs to distinguish between fast and in-depth assessment cycles as well as between common and non-common QAT fields. Details are provided in the next section.To complete the list of roles involved in the CMS, two additional roles are considered but they are not directly part of the QAR production workflow:\u2022\t QAR support role: can edit any part of published QARs to fix simple issues like typos or broken links or update technical data checks performed on new parts of a dataset regularly extended over time. Depending on the task, this role is covered by either an EQC member or a CMS automatic functionality.\u2022\t Observer: can read and comment in the CMS but is not granted the right to insert any content. This role, typically covered by a C3S technical officer, can intervene in case of blocking issues with the provider or whenever some aspects in the QARs need improvement.5. WORKFLOW AND PROCEDURES FOR THE QAR PRODUCTIONThe trade-off between timely and detailed assessment is tackled by splitting the QAR production workflow into two phases:\u2022\t A first phase (the fast assessment), mostly focused on verification of the minimum requirement fields. The dataset stewardship is scrutinised in terms of documentation, accessibility, and compliance with metadata standards.\u2022\t If the C3S governance board decides to make the fast assessment part of the QAR public together with the associated dataset, a second phase (the in-depth assessment) starts. During this phase, the complete independent assessment is performed, and the other fields are updated in case of need. The in-depth assessment focuses on the technical, scientific, and maturity data evaluation.An additional element that makes the QAR production sustainable is the identification of QAT fields associated with common content across several QARs. More details are reported in Appendix II. In the following section, it is shown how these two elements, fast/in-depth assessment and common/non-common fields, come into play during the QAR production.5.1 WORKFLOW IN A NUTSHELLIn a nutshell, the process for the QAR production for both already published and submitted for publication datasets may be summarised as follows:\u2022\t Start: o triggered by a QAR release calendar or new datasets available in the CDS\u2022\t Input: o data provider and EQC team fill in the QAT according to the workflow in the CMSFigure 3 Sketch showing the basic roles, their interactions, and their responsibilities during the QAR production within the CMS. Note the iteration loop between roles to allow refinement of the content. The sketch gives a grasp of the more complex workflow shown in the next figures.10Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010\u2022\t Output:o QARs released to support the C3S governance board while deciding whether to publish or reject a CDS dataset\u2022\t End:o C3S governance board approves/rejects the QAR in the CMS\u2022\t Manage updates:o caused by issues identified by users, indications by providers, new data available from operational datasets extending over time, regular review of new dataset documentation, additional independent assessment analysesThe trigger for a new assessment is associated with two main causes:\u2022\t A completely new dataset appears or a new version of an already existing dataset is made available (e.g. ERA5 reanalysis replaced by ERA6). As a consequence, new QARs are produced. It corresponds to \u2018start\u2019 in the steps described above.\u2022\t Correction of QAR content (e.g., broken links), new dataset documentation is made available (e.g., validation reports), a dataset is extended backward or forward in time (e.g. a new month of seasonal forecasts), additional independent analyses are performed (e.g., inter-comparison with other datasets). In this case, the dataset version remains the same, but some information in the EQC scope is updated and the QAR needs maintenance. It corresponds to \u2018manage updates\u2019 in the steps described above.Throughout the QAR production process, the user engagement team of the EQC iterates with the users to harvest their feedback about the different steps or improvements taken in a co-production process, making sure to advance in the direction of fulfilling the user\u2019s needs. These user\u2019s requests will result in reports to be discussed at regular EQC meetings, where they shall be further investigated and eventually trigger framework and QAR updates. User requirements also help to refine the QATs and to prioritise the performance metrics to be employed during the independent assessment. User engagement outcomes are thus the basis to conduct a gap analysis of the information made available to users and to steer the EQC design evolution in terms of framework and dissemination activities. This virtuous feedback loop is crucial for a user-oriented service as C3S.5.2 DATASETS ALREADY PUBLISHEDThe EQC function has been implemented after many datasets were already published in the CDS. As a consequence, a workflow needed to be envisaged to produce the necessary QARs. In this case, the trigger of the QAR initiation is a QAR release calendar, defined by the EQC team together with the C3S governance board. Once the QARs are triggered, the workflow is managed in the CMS, as shown in Figure 4.Figure 4 Sketch showing the interaction across roles during the QAR production within the CMS. Compared to Figure 3, here the distinctions between fast/in-depth assessment cycles and common/non-common fields are explicit and it is clarified at which stage the QAR is published.11Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010Given the same roles identified in section 4, the QAT is filled in the private domain during the fast assessment cycle and then published. Once public, the QAR is completed with the independent assessment during the in-depth cycle and finally updated in the public domain. Each assessment cycle distinguishes between common and non-common fields:\u2022\t The part associated with the common fields requires the data provider expertise and does not include the independent assessment, as such this part involves many members, but it does not need to go through the in-depth cycle.\u2022\t The part associated with the non-common fields is unique to each QAR, because it is tailored to each variable. During the fast assessment, almost12 any non-common field (e.g., variable description) can be extracted from precompiled tables validated with the data provider and extracted automatically to fill in the unique QAR. This part of the QAR production is nearly automatic, so it can involve fewer members, the EQC main contact to initiate the QAR and a reviewer to guarantee that the content is meeting expectations. Once the fast assessment cycle is complete, the non-common fields follow the in-depth cycle, where the evaluator includes the independent assessment material.Once published, one QAR might need to be updated. More details about the procedure in this case are given in Appendix II.5.3 DATASETS READY TO BE PUBLISHEDIn the future, the evolution of EQC will need to consider a workflow for datasets ready to be published. So far, this workflow has not been implemented. Several options are considered based on the lessons learned during the ramp-up phase of EQC. Here we give some recommendations.A new dataset is a dataset the provider considers ready to be served through the CDS. At this stage, the provider and C3S officers iterate to ingest data information, like documentation or the location where data are stored. Much of this information could be collected in the CMS (or a tool connected with the EQC CMS), which facilitates the completion of part of the QARs shortly after. Instead of EQC asking for similar information again, we can leverage the content already stored in the CMS to streamline the flow of information exchanged among the various authors involved, by introducing a workflow starting with the fast assessment explained in the previous section. Once the fast assessment cycle is complete, the dataset could be either rejected because, for instance, the minimum documentation required is not complete, or accepted for publication. When the dataset and the initial QARs are public, the in-depth assessment cycle starts. The logical flow, illustrated in Figure 5, may be summarised as follows:12 The results of the metadata checks, i.e., data format, Toolbox compliance and standard convention compliance cannot be structured in precompiled tables, but the checks are produced by automatic software validated in advance.Figure 5 Scheme of the preliminary workflow about the way EQC may be engaged in the fast assessment of datasets not yet published in the CDS. Once a dataset is published (identified with \u2018end\u2019 in the figure), the in-depth assessment cycle starts as usual. The part associated with the EQC is in light blue, while the part associated with the EQC user engagement team is in green.12Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010\u2022\t When the dataset is ready to be evaluated, the C3S governance board opens a ticket addressed to the EQC team.\u2022\t The EQC team meets regularly to assign the work for the QAR production based on, among other sources, the tickets received.\u2022\t Once the related QARs are completed of the fast assessment and approved in the CMS, the CMS closes automatically the ticket considered.\u2022\t The C3S governance board decides whether to publish, postpone the publication or discard the dataset using, among other input, the QARs made available.\u2022\t Once the dataset is published along with the preliminary QARs, the in-depth assessment to complete the QARs can start, as described in the previous section.The independent assessment is part of the in-depth cycle that always starts after the dataset is published with its QAR. However, for new datasets it would be convenient that the data provider performs the technical assessment, that is, data checks, and reports the evidence logs to the EQC team. The EQC team then controls that evidence is available for the entire dataset and performs random checks autonomously on a subset of the entire dataset. The reason for this logical flow is that there are technical limitations that make the data checks timely only when done by the provider. Indeed, the downloading and queuing time and the disk storage requirements are technical limitations that would demand more resources for EQC, while these resources are likely already allocated on the provider\u2019s side. The strategy described would also reduce duplication of efforts and optimise resources, while guaranteeing independent checks.6. PROTOCOLS AND PRACTICES COMPLEMENTING THE IMPLEMENTATION OF THE EQC FUNCTIONBesides the QAR production, the EQC function for the CDS is completed by additional protocols that make it a solid building block of C3S. Here follows a brief list of the protocols and practices considered.6.1 PROTOCOLS FOR GAP ANALYSISCommunication channels to inform C3S with recommendations to avoid gaps, address drawbacks and shortcomings and identify limitations have been established. These issues are reported via the EQC communication channels in the form of tickets sent to the rest of C3S. The tickets are sorted by resolution timing and priority as follows:\u2022\t Short-term issues (<1 month) requiring quick attention, critical/blocking issues. These answer questions along the lines of \u2018Is it something that hampers the EQC work?\u2019, \u2018Is it something limiting the user experience significantly?\u2019, \u2018Is it an obvious bug, an error on the website?\u2019, such as a Catalogue entry lands to page-not-found.\u2022\t Mid-term issues (1 to 6 months) identified problems and recommendations about the CDS data. These answer questions along the lines of \u2018Is it a problem that requires extensive analyses or impacts several aspects of the CDS?\u2019, such as unclear data licences.\u2022\t Long-term issues (>6 months) based on user requirements, non-blocking issues. These answer questions along the lines of \u2018Is it a user need that the EQC team is constantly facing when engaging with the users?\u2019, \u2018Is it a requirement coming from the EQC acting as a user?\u2019, such as the entry point to the Catalogue could be more efficient shifting from dataset category-based to variable-based. These requirements are inserted in a user requirement database and then analysed to become tickets. Usually, these tickets steer the evolution of the service over time.The different issues are analysed by C3S and may trigger internal processes to deal with them. In this respect, the EQC team supports the evolution of C3S through gap analysis of the current capabilities of the CDS and formulates recommendations.6.2 COMMON PRACTICES TO ENSURE CONSISTENCY ACROSS DATASET CATEGORIESOne key common practice to ensure consistency across dataset categories is to define a common vocabulary. The definition of shared vocabularies and common practices provides a foundation 13Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010for interoperability, reduces interpretation ambiguities, and boosts an efficient communication exchange. The efforts to harmonise existing terminologies in a structured vocabulary aim to facilitate the C3S products usage by the downstream and upstream users, and it is also beneficial for the coordination with the rest of the C3S activities ensuring consistency when referring to specific CDS elements. It shall be noted that the lack of an overarching consistent EQC vocabulary was identified as one of the priority gaps in climate data quality (Nightingale et al. 2019).Agreeing on a common terminology is by no means a simple task, as it is time-consuming and comes with a variety of challenges, especially in the case of C3S where datasets come from different communities adopting different conventions. For instance, numerous terms are interpreted differently across data communities. What is defined as \u2018product\u2019 in the satellite observations community13 is way different to what the seasonal forecasts community14 refers to or to what the ECMWF MARS (Meteorological Archival and Retrieval System) archive considers it to mean.15 Some terms are very general (e.g., \u2018observation\u2019) and lead to long discussions to reach an agreement. For these cases, a practical solution has been to include mostly CDS-related terms, leaving out general terms as much as possible. The definitions are continuously monitored and improved.According to the FAIR principles (Wilkinson et al. 2016), it is critical to use controlled vocabularies to describe and structure (meta)data in order to ensure findability and interoperability. A common vocabulary also refers to a set of common standards for data and metadata (formats and conventions) to be enforced by C3S. Indeed, gathering the metadata in a single system, as the CDS, with a common format, needs standardisation, as it is necessary to encourage data providers to convert their metadata inventories into formatted inventories that can be transferred to the C3S service. Including metadata in a consolidated and centralised system requires and/or encourages providers to agree to share the information with the community at large (Aguilar et al. 2003; Brunet et al. 2020). Having a common metadata standard for the many communities gathered by C3S is not realistic, because these communities have their own standards. Thus, a first practical approach is that each dataset category follows a community-recognised standard, as identified by the EQC team. Examples are CMIP, CORDEX, ESA-CCI, and obs4MIPs metadata conventions.Finally, consistency of the EQC framework is also achieved by commitment to transparency following the TRUST Principles for Data Repositories (Lin et al. 2020). In particular, the methodology, the software and the assessments are made available to the users through the QARs public in the CDS. This helps to increase transparency and verifiability of the assessment, as well as resilience of the processes considered, which is open to further improvement.A few more practices have been identified to ensure consistency across the QATs, among these are the following:\u2022\t Engagement with the data providers to guarantee that the QAT entries entail similar understanding. For most QAT entries, an exhaustive explanation is added to clarify the type of information requested. Explanations appear as tooltips in the CDS (see Figure 1).\u2022\t Engagement with the users. Feedback from a focus group (i.e. a users sample consulted on a regular basis to provide feedback on the new EQC releases) helps to reconsider terms that are not of immediate understanding for non-experts.\u2022\t Production of a quick-start guide to support the data providers in navigating and filling the QATs.\u2022\t Production of guidelines for the EQC team to reduce subjective or wrong interpretations of the QAT requests. Beyond fostering a common platform of understanding for the whole team, it gives resilience to the EQC function in case members leave and new ones join the team. All guides are continuously updated, leveraging the team experience.13 https://www.temis.nl/airpollution/absaai/.14 https://climate.copernicus.eu/charts/c3s_seasonal/.15 https://confluence.ecmwf.int/display/ECC/WMO%3D14+element+table.14Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010\u2022\t Standardised style of the plots and references introduced in the QARs and harmonised QAR titles and filenames to deliver independent assessment results as much uniform as possible across the dataset categories assessed.7. LESSONS LEARNEDThe evolution of the EQC function would benefit from optimising the protocols, the templates and the workflow implemented so far, while tackling the gaps identified. The main issues encountered and more general considerations follow:\u2022\t EQC workflow is a multi-actor process: collaborative iteration is key. Several lessons have been learned while working on the QAR production and made clear that the EQC framework is a multi-actor process that requires collaborative iterations with different stakeholders. Tied collaboration between the C3S contracts and approver, user engagement, and data provider is extremely important for sustaining the delivery of EQC information. Clear responsibilities and timelines have been defined. Perhaps, the most important interaction is with the data providers/producers for filling the QATs. Responses from the providers are sometimes sparse or non-existent. Data providers/producers are considered the best source to fully describe their datasets and are the preferential choice to contribute to the QARs. This gap has been narrowed by:o including the officer in charge of the relationship with the data provider contractor (a technical officer in the case of C3S), who facilitates the interaction with the provider and ensures that specialistic knowledge about the data under scrutiny is fully accounted for;o ensuring that the EQC takes place earlier in the data ingestion process than now where the EQC work is done on already published data; ando making all EQC-related tasks a contractual obligation in the data provider commitments. For brokered datasets (i.e., pre-existing dataset, not subject to the Copernicus license, to which C3S only acquires a license for the purpose of making it available in the CDS), the situation is not so different, because the contact point with EQC is the broker.\u2022\t Technical constraints limit the extension of the minimum requirements. The current version of the minimum requirements (MRs) fits the existing technological infrastructure as well as available human resources. The most important constraints to be faced are downloading and queueing time, memory disk space needs, lack of metadata information (that the provider should make available) about valid ranges for some variables, need for (land/sea) mask, and lack of automatic tools for data checks of each dataset category. To favor the development of data checker tools, common standards for data and metadata (formats and conventions) shall be enforced by C3S. Overall, climate services play a crucial role in disseminating relevant standards, in this case metadata standards (GFCS WMO16). A service aiming at providing seamless products necessarily needs to disseminate data in a common format with files structured similarly and with sufficient metadata uniformly stored. This supports interoperability of data files and archives for automated data processing through improved and extended standards and metadata. As a first step, datasets served in NetCDF should comply with the CF17 convention, but this covers only units, dimensions, and a few variables\u2019 metadata attributes. It is beneficial to ingest input data following a more constrained and controlled common vocabulary, covering variable naming, file names, grid descriptions, and global attributes. Examples of domain-specific conventions are CMIP18 and ESA-CCI.19 For the latter, C3S and ESA are coordinating to homogenise their metadata standards for satellite observation datasets. In addition, the ACDD20 conventions cover the global attributes 16 https://gfcs.wmo.int/.17 http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html.18 https://pcmdi.llnl.gov/mips/cmip5/requirements.html.19 https://climate.esa.int/sites/default/files/CCIDataStandards_v2-1_CCI-PRGM-EOPS-TN-13-0009.pdf.20 https://wiki.esipfed.org/Attribute_Convention_for_Data_Discovery_1-3.15Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010for easier discovery and interoperability of the data (e.g., spatial and time coverage description, keywords).\u2022\t Capacity building both in terms of human resources and technologies. The implementation of the EQC requires cross-disciplinary knowledge (e.g., science, data management, computer engineering) to design protocols, software, and workflows following best practices. A constraint emerged during the work described in this paper is the importance of designing solutions sustainable for a large number of datasets. Otherwise, it is not possible to guarantee the necessary throughput with the available resources. As a consequence, some choices that seem to be straightforward and better than what has been implemented could not be considered because they do not scale for the large number of datasets under scrutiny (e.g., writing individual QARs for each variable). It was necessary to automise as many parts of the workflow as possible to guarantee a timely production of the QARs. Based on our experience, it is also challenging to find a sufficient number of experts willing to regularly review in-depth all data streams. Considering that on demand requests for review do not necessarily guarantee that the same level of expertise could be kept over time, it would be appropriate to identify suitable funding mechanisms and contractual arrangements to keep the experts engaged for a longer period. The EQC framework must be tailored to the service infrastructure to be successful both in terms of human resources, coordination, and technology capabilities. This should translate into an increased effort towards capacity building, the need of which was also highlighted by Hewitt et al. (2020) for climate services in general. Capacity building shall be closely taken into account during the implementation of a sustainable EQC framework.\u2022\t Optimise the production of more insightful independent assessments. The scientific assessment needs to expand towards the diagnostics and standard metrics considered most insightful for the users. It would be beneficial to engage with the data providers to identify common baseline metrics to be applied independently. This will help to converge towards stronger provider engagement and satisfaction with the assessments performed and to reduce iterations during the in-depth cycle of the QAR production process. The huge and increasing amount of CDS datasets will need to consider pragmatic approaches to streamline the production of the independent assessment. While the fast cycle of the EQC framework has been made very efficient, more work will be needed to establish sustainable mechanisms for the detailed in-depth cycle of the EQC framework.\u2022\t Consistent and shared vocabulary. A common terminology should be shared and kept improving across the various C3S components. This facilitates consistency within the C3S (across the Catalogue entries for instance), it backs the user support desk when answering frequent questions by users about terminology, it gives a reference for the users to consult when jargon or acronyms are found, and it supports the project management to avoid ambiguities across C3S contracts. As such, the common vocabulary is considered a fundamental guidance document to be integrated in the C3S portal to benefit both users and service.\u2022\t Benchmarking and cross-service coordination. Consolidation of the EQC function also passes through the investigation of the most recent approaches. Given the challenges and opportunities that arose while implementing the EQC framework of the CDS C3S, exploration of the existing literature and coordination with other Earth data services, and in particular the other Copernicus services, is a key task to build a state-of-art EQC framework (i.e., benchmarking). It is important to investigate the standards and best practices implemented in similar services operating around the world (e.g., Leadbetter et al. 2020, RfII 2020). This helps to assess the applicability of the different approaches to the EQC function.\u2022\t Scientific gaps warrant further research. There are clear scientific gaps that hinder smooth development of protocols for data quality assessments. Further research investments shall be considered by major funding bodies (e.g., Horizon Europe) to fill the current scientific gaps. For instance,o the system maturity matrix CORE-CLIMAX (EUMETSAT 2014) was identified as a tool for the maturity assessments, but it exhibits limitations of scalability in an operational environment. An example, the in-situ observations GRUAN dataset 16Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010required three months of work to complete the maturity assessment. In an environment like C3S, with increasing datasets and new versions, this effort is not practical. Scalability requires to detail the guidelines of the assessment (e.g., specify the metadata standards to check against or the source considered for citation to score usage). Moreover, the current scoring rules of the maturity matrix might require a peer-review process, possibly involving the data provider, to reduce subjectivity of the judgments. This poses once more challenges to produce scalable and timely assessments. In addition, the literature does not offer system maturity matrices for climate projections and seasonal forecasts, which opens intriguing questions about the different role maturity and verification play in modeling and observational communities.o Another case that would benefit from further research is the development of a metadata standard convention for seasonal forecasts and ERA5 data served in GRIB2 format.o Another example is the lack of well-defined ranges of physical plausibility for all the CDS variables, as well as a list of variable names and descriptions consistent across dataset categories. For instance, so far it is not clear what reference to use to name and describe the surface temperature, it ranges from \u2018(surface) temperature\u2019 in GCOS21 to \u2018near-surface air temperature\u2019 in CMIP tables.22\u2022\t User engagement as an integral component of the EQC. User engagement needs to be an integral part of the EQC evolution towards user-endorsed practices. It is inevitable that the users will drive the requirement for the provision of bespoke dataset quality information. The FP7 EUPORIAS project (Buontempo et al. 2018) defined a set of principles for a successful climate service, which are particularly relevant for the design of an EQC framework in a user-driven context. The next phase of EQC would benefit from identifying the type of QAR content considered most useful from the user\u2019s perspective. At present, our understanding of the usage of EQC assessments by users is still limited, but more attention to uncertainty characterisation, dataset stewardship, and ways the information is presented seem good candidates to start with. Thanks to the first QARs made available online, it will be insightful to test the efficacy of the communication strategies to ensure that appropriate and accurate quality assessments reach the users and are interpreted correctly. \u2022\t Central repository of information. The tool used for the QAR production (the Content Management System in our case) may need to expand to consider the several data ingestion processes happening in C3S beyond EQC and may need to upgrade enhancing functionalities in relation to data import and synchronisation with the rest of the CDS information. The tool may be better integrated into a single system for data ingestion and repository of information within the service to avoid duplication of effort and duplication of content describing the datasets. Better integration would also make the flow of information smoother across the actors involved in the service. Given the granularity of the datasets and their quality assessments, it may be convenient to make the quality information accessible through a structured API. \u2022\t New challenges for the next phase. The next phase of EQC will need to evolve to tackle some new challenges. For instance, it remains to define the details of a workflow dealing with new datasets ready but not yet public in the CDS. Another example is defining how to trigger QAR updates and their maintenance due to dataset new versions, due to datasets extending over time and due to new documentation available. Some practical directions have been suggested, but before adopting them operationally they would need to be assessed and tested. Evolution on how the quality information is disseminated (e.g., synthesis table) and development of an advisory service about dataset robustness (e.g., scoring scheme) will also deserve further exploration. A scoring system can be introduced so that users can quickly see which atomic elements of the EQC information have a good amount of detail. The scoring scheme would not aim at determining whether one dataset is better than another comparable dataset in an absolute sense, but only 21 https://gcos.wmo.int/en/essential-climate-variables/surface-temperature.22 https://github.com/PCMDI/cmip6-cmor-tables/blob/master/Tables/CMIP6_3hr.json.17Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010indicate the amount of quality information available. The scheme has to be simple and can be based on levels of increasing amount of detail/justification provided, as inspired by common practices in the literature (e.g., Nightingale et al. 2018, GEO Label23). While working on the EQC framework, some ideas have been already put forward, the levels of appraisal may depend on the fulfillment of the minimum requirements described in this paper, making the scoring objective and prone to automation.8. SUMMARY AND CONCLUSIONSThe current framework developed for the ramp-up operational phase of the Evaluation and Quality control (EQC) for the C3S CDS was presented. The framework considers the tasks, protocols and tools required to ensure that data are reliable and usable. It is inspired by the WMO GFCS guidelines, the ISOs 14090/1 and previous EU FP7/C3S projects. The framework is driven by a holistic approach aiming at homogenising the type of information made available across different climate datasets in a way that is both human and machine readable. It is characterised by a two-tier review system to assure the quality of dataset information released to the public. On the one hand, this approach enables fair and consistent comparison across datasets and facilitates guidance on the best use of data for the intended user\u2019s application; on the other hand, this approach makes the assessments sustainable and maintainable in an operational environment. In doing so, the framework explored optimal mechanisms (e.g., fast vs in-depth assessment and common vs non-common dataset information) for setting up sustaining delivery of EQC information, meeting the guidelines of the European Roadmap for Climate Services (EC 2015) and Peng et al. (2021).The establishment of a quality management framework demonstrated benefits to the many actors involved:\u2022\t the users: easy access and guidance to quality assurance information;\u2022\t the data providers: feedback on data quality and incentive for improvement to increase data uptake and usability. This is in line with the good practice to connect the quality information with the dataset before release to reduce data misuse since this may also result in damage to the reputation of the data provider (Peng et al. 2021);\u2022\t the service itself: delivery of trusted and authoritative climate information, commitment to a user-driven evolution of the service. The service benefits from an established vehicle that triggers actions to improve the service itself; and\u2022\t the funding agencies: to get a measure of how compliant the funded datasets or services are with specific requirements.As mentioned in the introduction, it is the first time that the methodologies, the metrics, the evaluation framework and the way to present all this information are being developed in an operational service that disseminates the majority of the climate dataset categories (including in-situ and satellite observations, seasonal forecasts, reanalysis, and climate projections). Building the underlying technical solutions makes the C3S EQC unique and requires pragmatic decisions for its implementation. The first part of the EQC framework design focused on ensuring the robustness of the baselines and processes to collect the information required for the QATs, keeping their coherence and their comparability across all the datasets available in the CDS. Having a set of QATs covering consistently all the dataset categories is a unique endeavour. These activities needed continuous improvement to fine-tune the EQC framework, benefitting from the operational assessments (QARs) and user engagement process. During the second part of the EQC framework design, activities focused on defining the level of content required in the QARs and its homogeneity across. Optimisation of the QAR update by means of automation tools and workflow streamlining has also played an increasing role.The EQC framework was developed and implemented for all datasets published in the CDS at the beginning of 2020. QARs have been generated at the granularity of the variable for each dataset and made available to the users via the CDS web platform. The implementation of the EQC function addressed most of the recommendations that arose during the pre-operational phase (see Nightingale et al. 2019): (i) designing of dataset category specific 23 https://geolabel.net.18Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010QATs, (ii) enhancement of the CMS functionalities, (iii) identification of minimum requirements to publish a CDS dataset, (iv) establishment of an overarching consistent EQC vocabulary, (v) creation of guidance documents for evaluators and reviewers to guarantee consistency in the QAR production, (vi) regular benchmarking activities brought into the operational process, (vii) ability to track changes in the QAR content.Several constructive pieces of feedback from data providers, downstream users and C3S officers made the dissemination of the EQC information more robust over time. Orchestrating the different elements involved requires considerable coordination efforts and a continuous improvement approach to integrate the inputs regularly emerging from stakeholders and technical constraints. A number of lessons learned and science knowledge gaps were identified during the development of the EQC function and are detailed in the paper. These warrant further investment to comprehensively address the quality dimension of climate datasets in an operational environment.ADDITIONAL FILEThe additional file for this article can be found as follows:\u2022\t QATs. Consistent set of QAT designs for a variety of dataset categories supported by the CDS: in-situ observations, satellite observations, seasonal forecasts, global and regional climate projections, global and regional reanalyses. DOI: https://doi.org/https://doi.org/10.5334/dsj-2022-010.s1APPENDIX I: CHARACTERISTICS OF THE INDEPENDENT ASSESSMENTThe independent assessment is a fundamental piece of the quality assessments. Indeed, evidence that the dataset has been independently validated is key criteria for most data users (Nightingale et al. 2019). Applying the same approach and tools, nonpartisan of the supplier source, guarantees a uniform and impartial auditing. The independent assessment is part of the QAT and is designed to accommodate the information on the following topics: \u2022\t Data and metadata checks, performed to verify whether a reported data value is representative of what was intended to be measured or simulated and has not been contaminated by unrelated factors. Lawrence et al. (2011) postulated a generic checklist for technical quality assessments within a data review procedure. Building on the Lawrence et al.\u2019s (2011) checklist, the EQC developed a data checker software to detect whether the CDS data respond to the data models defined for the specific dataset category (i.e. community metadata standards, e.g., ESA-CCI Data Standards V2.124), have the expected format and metadata with no unforeseen gaps, and no suspicious outliers (physical plausibility).\u2022\t Basic metrics (e.g., bias, correlation, linear trends), appropriate for each dataset category, to check the scientific soundness and performance of the CDS datasets. Results are available through the synthesis table cell named \u2018expert evaluation\u2019. These standard diagnostics represent a first step for more insightful scientific assessments to be developed over time. For instance, climate projections related metrics could include performance analyses of future climate projections simulations or more reference datasets could be considered. Given the many different analyses options, priority shall be given according to the user needs. Based on our experience, it is anyway recommended to engage with the data providers to identify, together with EQC, common baseline metrics to be applied independently. This will help to converge towards stronger provider engagement and satisfaction with the assessments performed.\u2022\t System maturity matrix (SMM) assessment, which is performed for these six clusters: metadata, user documentation, uncertainty characterisation, access/feedback/update, archive, and usage. The SMM model used is based on the CORE-CLIMAX approach and is intended to extend these assessments to all dataset categories in the CDS. However, the 24 https://climate.esa.int/sites/default/files/CCIDataStandards_v2-1_CCI-PRGM-EOPS-TN-13-0009.pdf.19Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010literature does not offer a SMM for climate projections and seasonal forecasts datasets, which makes this extension exercise fitting better a research project rather than an operational EQC service. It was therefore not possible to expand the SMM to all CDS categories.\u2022\t Key strengths and limitations, reporting the concluding remarks about the independent assessment and offering guidance with not-to-miss information that helps the user to understand whether the dataset fits the specific user\u2019s application. In the future, we see this document improving its guidance facilitating the translation of complex, often technical, information into a format that can be easily understood by users focusing on the most relevant aspects (e.g., known issues, dataset highlights, uncertainty).APPENDIX II: DETAILS ABOUT THE WORKFLOWTHE CONCEPT OF COMMON AND NON-COMMON QAT FIELDSAn additional element that makes the QAR production sustainable is the identification of QAT fields associated with content common across several QARs. It shall be taken into account that the QAR granularity is at a variable level. Thus, given for instance the same model, version, model run and Catalogue entry, all the variables share some information that is exactly the same. The concept is clarified by the following example: for a given Catalogue entry = \u2018CMIP5 monthly data on single levels\u2019, model = \u2018inmcm4\u2019, experiment = \u2018amip\u2019, some answers to the QAT for climate projections have the same content, for instance \u2018description of the model\u2019 or \u2018horizontal resolution\u2019. In this respect, instead of repeating the same answers, these are written once in the common part of the QAR, while the specific answers, unique to the specific variable, enter the non-common part of the QAR. Continuing with the example above, let us suppose that we want to produce one QAR about\u2022\t variable \u20182m temperature\u2019;\u2022\t model \u2018inmcm4\u2019;\u2022\t experiment \u2018amip\u2019; and\u2022\t available in the Catalogue entry distributing monthly fields, that is, \u2018CMIP5 monthly data on single levels\u2019.When creating the QAR, the EQC main contact fills these four fields that make the QAR unique and selects the common fields associated with model \u2018inmcm4\u2019, like data format or model components description. These fields are filled once in the CMS and then propagated automatically to all variables and experiments for the same model. One extra consideration that reduces the manual intervention is recognising that the non-common fields are typically the variable name, units, and description, and these are the same independent of the specific model given the same Catalogue entry. Thus, the associated information can be extracted from precompiled tables to fit each QAR. For instance, the definition of \u20182m temperature\u2019 is the same for model \u2018inmcm4\u2019 or for model \u2018access1-3\u2019, but the common fields (e.g., model components description) are very different. As a result, the CMS makes it possible to fill in each QAR by extracting the non-common information from precompiled tables. Benefitting from this functionality, any change to these tables can be quickly propagated to hundreds of QARs. These tables need to be validated by the data provider. Note that all this also makes the approval process of thousands of QARs sustainable in the fast assessment cycle. Indeed, while the common part of a QAR needs the usual CMS workflow, the remaining non-common part fast assessment would require only automatic extraction of content from tables agreed with the data provider. The merging of agreed common fields and agreed tables for non-common fields avoids the need for the approval stage of each single QAR in the fast assessment. The EQC team spent time identifying which QAT fields are common and which are non-common, results are shown in the supplementary material.TRIGGERS FOR THE QAR UPDATEOnce published, one QAR might need to be updated till a new version of the same dataset is published in the CDS. Moreover, an update can also be caused by:20Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010\u2022\t users detecting and reporting via the helpdesk deficiencies or shortcomings with the dataset or with the QAR published. For simple actions, like typo correction, the QAR support role in the CMS can apply changes to the published QARs, whereas for more complex editing, the EQC team shall withdraw the QAR for updating. In this case, the EQC main contact restarts the QAR;\u2022\t regular updates that require manual intervention to take into account possible novelties in the documentation, for instance. The regular update depends on the QAT field and on the dataset category:o the general rule is that it is usually done once a year manually and only for the QAT common fields;o variable definitions might improve over time, for instance. In this case, C3S warns the EQC team about the improvement and the non-common fields are updated by changing manually the tables containing these fields. The manual update of the tables triggers an automatic update of the QARs affected. In the future, both the non-common fields tables and the Catalogue would benefit from automatic synchronisation preventing manual intervention; ando seasonal forecasts QARs need a more frequent manual update because new versions of the systems are upgraded frequently, often yearly. The update is done by restarting the common fields once every three months, focusing on a few QAT fields preselected (e.g., operational status of the system);\u2022\t regular updates of the assessments that do not require human intervention, that is, data checker and Toolbox compatibility software. This is particularly valuable for datasets that are regularly extended in time, such as seasonal forecasts and reanalysis (a.k.a. near real-time datasets). The appropriate parts of the QARs are automatically updated once a month, considering the last month of data available; and\u2022\t additional independent assessment analyses. This can happen during the in-depth cycle for non-common fields only. If new analyses are available (e.g., inter-comparison assessments), the in-depth cycle restarts to update the appropriate QAR.It shall be noted that in case a new version of a dataset is available, a completely new QAR has to be produced. In this respect, a new dataset version is not considered a trigger for QAR updates, but the workflow follows the usual QAR production.APPENDIX III: TECHNICAL, SCIENTIFIC AND DOCUMENTATION ASSESSMENTSThroughout the paper, the authors use the concepts of \u2018scientific\u2019 and \u2018technical\u2019 assessments. It might be worth avoiding any misunderstanding clarifying the notion in this context. The scientific assessment consists of data content and cross-data content checks, as opposed to the technical assessment that regards the data and metadata files checks (Stockhause et al. 2012). As an example, when the evaluator plots the dataset variable and checks for reproducibility of El Ni\u00f1o events against skill metrics, here the scientific soundness of the data content is considered (e.g., Haiden et al. 2019). When the evaluator looks for the metadata standard compliance (e.g., ACDD convention), here the evaluator is checking that the attributes describing the files are according to a set of community-recognised metadata characteristics (e.g., specific date-time format). There is no check of the data content, no scientific evaluation in this case, but a metadata conformity check; it is a purely technical assessment (Evans et al. 2017; Stockhause et al. 2012). The physical consistency checks are borderline in this distinction. On the one hand, these are technical checks when analyses regard the match between information in the valid ranges reported in the metadata and the max/min values in the file content. On the other hand, the check becomes more complex if a plot associated with analyses is necessary to identify the suspicious outliers. This operation becomes necessary when the metadata does not contain the valid ranges, which is a good practice to include but at times it is very challenging for exotic variables. In this paper, the physical plausibility checks are considered part of 21Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010the technical assessment, albeit automatic statistical analyses about the data content is necessary.As far as the scientific assessment is concerned, this refers to scientific analyses of the physical content described by the dataset to check for its scientific soundness. Given the nature of this assessment, it is typically carried out by domain experts. Analyses may include uncertainty estimation, validation against reference datasets, and reproducibility of temporal/spatial patterns. Once clear what is considered as technical and what is considered as scientific, part of the assessments described in this paper are about documentation (availability and completeness) and file accessibility, archiving and compatibility within the service (i.e., with the Toolbox). All these analyses are neither purely technical nor scientific because they are not about the files per se, but about the associated material needed to access and understand the dataset. These analyses enter the group of stewardship assessment. It is true that stewardship regards all the aspects about distribution of the dataset and so potentially also technical and scientific assessments enter in this category. However, with stewardship assessments here we consider any assessment that guarantees accessibility and understandability of the dataset distributed, and so anything of relevance in dataset quality that is not associated with the data and metadata file content, such as documents accompanying the dataset describing how to use it. Typical examples regard the description of the algorithms or models used to produce and process the data, provision of the DOI and licence of use, grid description, verified network address to access the data, and information about the archiving procedures. The goal is to ensure that the dataset is well documented, the processing chain is visible, and the data is readily obtainable and usable.At times, the assessments described above are accompanied by maturity assessment models. These are formal approaches to support compliance verification, usually defined in discrete stages to evaluate practices applied in organisations, services, or products. Maturity is meant as a desired or anticipated evolution from a more ad hoc approach to a more managed process (Peng 2018). Datasets associated with high maturity are produced following best practices of the community and in a more managed fashion, increasing user trust in the data record provided. It should be noted that a low maturity rating does not necessarily imply a low scientific value for a dataset. It can happen especially for datasets managed by a single investigator that may be flagged to have low maturity due to poor quality in metadata, documentation, and accessibility.ABBREVIATIONSACDD \u2013 Attribute Convention for Data DiscoveryATBD \u2013 Algorithm Theoretical Baseline DocumentC3S \u2013 Copernicus Climate Change ServiceCAMS \u2013 Copernicus Atmosphere Monitoring ServiceCDS \u2013 Climate Data StoreCF \u2013 Climate and ForecastCMEMS \u2013 Copernicus Marine Environment Monitoring ServiceCMIP \u2013 Coupled Model Intercomparison ProjectCMS \u2013 Content Management SystemCORDEX \u2013 COordinated Regional climate Downscaling EXperimentCORE-CLIMAX \u2013 COordinating Earth observation data validation for RE-analysis for CLIMAte ServiceSDOI \u2013 Digital Object IdentifierECMWF \u2013 European Centre for Medium-Range Weather ForecastsECV \u2013 Essential Climate VariablesEQC \u2013 Evaluation and Quality Control22Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010ERA5/6 \u2013 Fifth/Sixth generation of ECMWF atmospheric Re-AnalysesESA-CCI \u2013 European Space Agency Climate Change InitiativeESMValTool \u2013 Earth System Model Evaluation ToolFAIR \u2013 Findable, Accessible, Interoperable, ReusableFP7 \u2013 Seventh Framework ProgrammeGCOS \u2013 Global Climate Observing SystemGRUAN \u2013 GCOS Reference Upper-Air NetworkISO \u2013 International Organization for StandardizationMARS \u2013 Meteorological Archival and Retrieval SystemMR \u2013 Minimum RequirementObs4MIPs \u2013 Observations for Model Intercomparisons ProjectPQAD \u2013 Product Quality Assurance DocumentPQAR \u2013 Product Quality Assessment ReportPUG \u2013 Product User GuideQA4ECV \u2013 Quality Assurance for Essential Climate VariablesQAR \u2013 Quality Assurance ReportQAT \u2013 Quality Assurance TemplateSEAS5 \u2013 Fifth generation of the ECMWF seasonal forecasting systemSMM \u2013 System Maturity MatrixTRUST \u2013 Transparency, Responsibility, User focus, Sustainability and TechnologyUERRA \u2013 Uncertainties in Ensembles of Regional ReAnalysisWIGOS \u2013 WMO Integrated Global Observing SystemWMO \u2013 World Meteorological OrganizationACKNOWLEDGEMENTSThis study is based on work carried out in the C3S_512 contract funded by Copernicus Programme and operated by ECMWF on behalf of the European Commission (Service Contract number: ECMWF/COPERNICUS720187C3S_512_BSC). We would like to acknowledge the work of colleagues from several European institutions, the data providers and C3S, who contributed to the development of the EQC framework as well as to the QAR production. We would also like to acknowledge the focus group users, who took time to review and provide valuable feedback on the QARs, QATs, minimum requirements and the CDS quality assessment tab. The authors are grateful to the anonymous reviewers for their constructive comments that have helped for the improvement of this paper.COMPETING INTERESTSThe authors have no competing interests to declare.AUTHOR CONTRIBUTIONSCL is the main contributor to conceptualization, project planning, and writing of the original draft. FD contributed significantly to conceptualization, project planning, reviewing, and editing. GL contributed significantly to conceptualization, writing of the original draft, reviewing, and editing. CB contributed significantly to conceptualization, reviewing, and editing. AO and CS contributed significantly to conceptualization, project planning, reviewing, and editing. MC, DS, PB, SP, VR, DP, FS, AL, AP, DC, OM, PC, NP, FM, MR, AR, and MG contributed to conceptualization, reviewing, and editing.23Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010AUTHOR AFFILIATIONSCarlo Lacagnina  orcid.org/0000-0001-9434-9809 Barcelona Supercomputing Center, Barcelona, SpainFrancisco Doblas-Reyes  orcid.org/0000-0002-6622-4280 Barcelona Supercomputing Center, Barcelona, Spain; Instituci\u00f3 Catalana de Recerca i Estudis Avan\u00e7ats (ICREA), Barcelona, SpainGilles Larnicol  orcid.org/0000-0002-6694-1458 Magellium, Ramonville-Saint-Agne, FranceCarlo Buontempo  orcid.org/0000-0002-1874-5380 ECMWF, Shinfield Park, Reading, UKAndr\u00e9 Obreg\u00f3n  orcid.org/0000-0001-8759-4451 ECMWF, Shinfield Park, Reading, UKMontserrat Costa-Sur\u00f3s  orcid.org/0000-0002-3319-4513 Barcelona Supercomputing Center, Barcelona, SpainDaniel San-Mart\u00edn  orcid.org/0000-0002-2862-2704 Predictia Intelligent Data Solutions SL, Santander, SpainPierre-Antoine Bretonni\u00e8re  orcid.org/0000-0002-3066-6685 Barcelona Supercomputing Center, Barcelona, SpainSuraj D. Polade  orcid.org/0000-0002-3892-1433 Finnish Meteorological Institute, Helsinki, FinlandVanya Romanova  orcid.org/0000-0001-8059-108X Deutscher Wetterdienst, Offenbach, GermanyDavide Putero  orcid.org/0000-0002-9721-1036 National Research Council of Italy (CNR) \u2013 Institute of Atmospheric Sciences and Climate (ISAC), Turin \u2013 Bologna, ItalyFederico Serva  orcid.org/0000-0002-7118-0817 National Research Council of Italy (CNR) \u2013 Institute of Marine Sciences (ISMAR), Rome, ItalyAlba Llabr\u00e9s-Brustenga  orcid.org/0000-0003-2144-675X Barcelona Supercomputing Center, Barcelona, SpainAntonio P\u00e9rez  orcid.org/0000-0002-0250-1461 Predictia Intelligent Data Solutions SL, Santander, SpainDavide Cavaliere  orcid.org/0000-0002-7466-8893 National Research Council of Italy (CNR) \u2013 Institute of Marine Sciences (ISMAR), Rome, ItalyOlivier Membrive  orcid.org/0000-0001-9773-6150 M\u00e9t\u00e9o-France \u2013 Centre de M\u00e9t\u00e9orologie Spatiale, Lannion, FranceChristian Steger  orcid.org/0000-0002-8244-8751 Deutscher Wetterdienst, Offenbach, GermanyN\u00faria P\u00e9rez-Zan\u00f3n  orcid.org/0000-0001-8568-3071 Barcelona Supercomputing Center, Barcelona, SpainPaolo Cristofanelli  orcid.org/0000-0001-5666-9131 National Research Council of Italy (CNR) \u2013 Institute of Atmospheric Sciences and Climate (ISAC), Turin \u2013 Bologna, ItalyFabio Madonna  orcid.org/0000-0001-7628-8870 National Research Council of Italy (CNR) \u2013 Institute of Methodologies for Environmental Analysis (IMAA), Potenza, ItalyMarco Rosoldi  orcid.org/0000-0001-8688-481X National Research Council of Italy (CNR) \u2013 Institute of Methodologies for Environmental Analysis (IMAA), Potenza, ItalyAku Riihel\u00e4  orcid.org/0000-0001-6581-8792 Finnish Meteorological Institute, Helsinki, FinlandMarkel Garc\u00eda D\u00edez  orcid.org/0000-0002-4760-0527 Predictia Intelligent Data Solutions SL, Santander, SpainREFERENCESAguilar, E, Auer, I, Brunet, M, Peterson, TC and Wieringa, J. 2003. Guidelines on climate metadata and homogenization, WCDMP-No. 53, WMO-TD No. 1186. Geneva: World Meteorological Organization. Brunet, M, Brugnara, Y, Noone, S, Stephens, A, Valente, M A, Ventura, C, Jones, P, Gilabert, A, Br\u00f6nnimann, S, Luterbacher, J, Allan, R, Brohan, P and Compo, GP. 2020. Best Practice Guidelines for Climate Data and Metadata Formatting, Quality Control and Submission. Reading, UK: Copernicus Climate Change Service.24Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010Buontempo, C, Hanlon, HM, Bruno Soares, M, Christel, I, Soubeyroux, J-M, Viel, C, Calmanti, S, Bosi, L, Falloon, P, Palin, EJ, Vanvyve, E, Torralba, V, Gonzalez-Reviriego, N, Doblas-Reyes, F, Pope, ECD, Newton, P and Liggins, F. 2018. What have we learnt from EUPORIAS climate service prototypes? Climate Services, 9: 21\u201332. DOI: https://doi.org/10.1016/j.cliser.2017.06.003Callahan, T, Barnard, J, Helmkamp, L, Maertens, J and Kahn, M. 2017. Reporting data quality assessment results: Identifying individual and organizational barriers and solutions. eGEMs, 5(1). DOI: https://doi.org/10.5334/egems.214European Commission (EC). 2020. Copernicus and earth observation in support of eu policies. Part I, Copernicus uptake in the european commission. DOI: https://doi.org/10.2760/024084European Commission (EC), Directorate-General for Research and Innovation. 2015. European Union: A European Research and Innovation Roadmap for Climate Services. DOI: https://doi.org/10.2777/702151European Organization for the Exploitation of Meteorological Satellites (EUMETSAT). 2014. CORE-CLIMAX System Maturity Matrix Instruction Manual (Doc. No. CC/EUM/MAN/13/002). Available at https://masif.eumetsat.int/website/wcm/idc/idcplg?IdcService=GET_FILE&dDocName=PDF_CORE_CLIMAX_MANUAL&RevisionSelectionMethod=LatestReleased&Rendition=Web.Evans, B, Druken, K, Wang, J, Yang, R, Richards, C and Wyborn, L. 2017. A data quality strategy to enable fair, programmatic access across large, diverse data collections for high performance data analysis. Informatics, 4(4): 45. DOI: https://doi.org/10.3390/informatics4040045Hewitt, CD, Allis, E, Mason, SJ, Muth, M, Pulwarty, R, Shumake-Guillemot, J, Bucher, A, Brunet, M, Fischer, AM, Hama, AM, Kolli, RK, Lucio, F, Ndiaye, O and Tapia, B. 2020. Making society climate resilient: International progress under the global framework for climate services. Bulletin of the American Meteorological Society, 101(2): E237\u2013E252. DOI: https://doi.org/10.1175/BAMS-D-18-0211.1ISO 14090:2019. Adaptation to climate change \u2014 Principles, requirements and guidelines. Geneva, Switzerland. https://www.iso.org/standard/68507.html.ISO 14091:2021. Adaptation to climate change \u2014 Guidelines on vulnerability, impacts and risk assessment. Geneva, Switzerland. https://www.iso.org/standard/68508.html.ISO 19157:2013. Geographic information \u2014 Data quality. Geneva, Switzerland. https://www.iso.org/standard/32575.html.Lawrence, B, Jones, C, Matthews, B, Pepler, S and Callaghan, S. 2011. Citation and peer review of data: Moving towards formal data publication. International Journal of Digital Curation, 6(2): 4\u201337. DOI: https://doi.org/10.2218/ijdc.v6i2.205Leadbetter, A, Carr, R, Flynn, S, Meaney, W, Moran, S, Bogan, Y, Brophy, L, Lyons, K, Stokes, D and Thomas, R. 2020. Implementation of a data management quality management framework at the marine institute, Ireland. Earth Science Informatics, 13(2): 509\u2013521. DOI: https://doi.org/10.1007/s12145-019-00432-wLin, D, Crabtree, J, Dillo, I, Downs, RR, Edmunds, R, Giaretta, D, De Giusti, M, L\u2019Hours, H, Hugo, W, Jenkyns, R, Khodiyar, V, Martone, ME, Mokrane, M, Navale, V, Petters, J, Sierman, B, Sokolova, DV, Stockhause, M and Westbrook, J. 2020. The TRUST Principles for digital repositories. Scientific Data, 7(1): 144. DOI: https://doi.org/10.1038/s41597-020-0486-7Medri, S, Banos de Guisasola, E and Gualdi, S. 2012. Overview of the main international climate services. Social Science Research Network. SSRN Scholarly Paper ID 2194841. DOI: https://doi.org/10.2139/ssrn.2194841Nightingale, J, Boersma, KF, Muller, J-P, Compernolle, S, Lambert, J-C, Blessing, S, Giering, R, Gobron, N, De Smedt, I, Coheur, P, George, M, Schulz, J and Wood, A. 2018. Quality assurance framework development based on six new ecv data products to enhance user confidence for climate applications. Remote Sensing, 10(8): 1254. DOI: https://doi.org/10.3390/rs10081254Nightingale, J, Mittaz, JPD, Douglas, S, Dee, D, Ryder, J, Taylor, M, Old, C, Dieval, C, Fouron, C, Duveau, G and Merchant, C. 2019. Ten priority science gaps in assessing climate data record quality. Remote Sensing, 11(8): 986. DOI: https://doi.org/10.3390/rs11080986Peng, G. 2018. The state of assessing data stewardship maturity \u2013 an overview. Data Science Journal, 17: 7. DOI: https://doi.org/10.5334/dsj-2018-007Peng, G, Lacagnina, C, Downs, RR, Ramapriyan, H, Iv\u00e1nov\u00e1, I, Ganske, A, le Roux, J, et al. 16 Apr. 2021. International Community Guidelines for Sharing and Reusing Quality Information of Individual Earth Science Datasets, OSF Preprints. DOI: https://doi.org/10.31219/osf.io/xsu4pRfll, German Council for Scientific Information Infrastructures. 2020. The Data Quality Challenge. Recommendations for Sustainable Research in the Digital Turn. G\u00f6ttingen.Stockhause, M, H\u00f6ck, H, Toussaint, F and Lautenschlager, M. 2012. Quality assessment concept of the World Data Center for Climate and its application to CMIP5 data. Geoscientific Model Development, 5(4): 1023\u20131032. DOI: https://doi.org/10.5194/gmd-5-1023-2012Th\u00e9paut, J, Dee, D, Engelen, R and Pinty, B. 2018. The Copernicus Programme and its Climate Change Service. IGARSS 2018 IEEE International Geoscience and Remote Sensing Symposium, 1591\u20131593. DOI: https://doi.org/10.1109/IGARSS.2018.851806725Lacagnina et al.   Data Science Journal  DOI: 10.5334/dsj-2022-010TO CITE THIS ARTICLE:Lacagnina, C, Doblas-Reyes, F, Larnicol, G, Buontempo, C, Obreg\u00f3n, A, Costa-Sur\u00f3s, M, San-Mart\u00edn, D, Bretonni\u00e8re, P-A, Polade, SD, Romanova, V, Putero, D, Serva, F, Llabr\u00e9s-Brustenga, A, P\u00e9rez, A, Cavaliere, D, Membrive, O, Steger, C, P\u00e9rez-Zan\u00f3n, N, Cristofanelli, P, Madonna, F, Rosoldi, M, Riihel\u00e4, A, D\u00edez, MG. 2022. Quality Management Framework for Climate Datasets. Data Science Journal, 21: 10, pp. 1\u201325. DOI: https://doi.org/10.5334/dsj-2022-010Submitted: 25 November 2021  Accepted: 17 March 2022     Published: 04 April 2022COPYRIGHT:\u00a9 2022 The Author(s). This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License (CC-BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. See http://creativecommons.org/licenses/by/4.0/.Data Science Journal is a peer-reviewed open access journal published by Ubiquity Press.Wilkinson, MD, Dumontier, M, Aalbersberg, IJ, Appleton, G, Axton, M, Baak, A, Blomberg, N, Boiten, J-W, da Silva Santos, LB, Bourne, PE, Bouwman, J, Brookes, AJ, Clark, T, Crosas, M, Dillo, I, Dumon, O, Edmunds, S, Evelo, CT, Finkers, R, Mons, B, et al. 2016. The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1): 160018. DOI: https://doi.org/10.1038/sdata.2016.18WMO/WIGOS. 2017. WIGOS Metadata Standard. Geneva: World Meteorological Organization, WMO no. 1192.Zeng, Y, Su, Z, Barmpadimos, I, Perrels, A, Poli, P, Boersma, F, Frey, A, Ma, X, Bruin, K de, Goosen, H, John, VO, Roebeling, R, Schulz, J and Timmermans, WJ. 2019. Towards a traceable climate service: Assessment of quality and usability of essential climate variables. Remote Sensing, 11(10): 1\u201328. DOI: https://doi.org/10.3390/rs11101186",
    "source": "'Ubiquity Press, Ltd.'"
  },
  {
    "id": 45808781,
    "doi": null,
    "title": "Workshop report: Training program on ENACTS climate time series, data library and maprooms, Kigali, Rwanda",
    "abstract": "Meteo Rwanda, in collaboration with the International Research Institute for Climate and\r\nSociety (IRI), has implemented the first phase of the ENACTS (Enhancing National Climate\r\nServices) initiative. The ENACTS initiative brings climate knowledge into national decisionmaking\r\nby improving availability, access and use of climate information. Meteo Rwanda\r\nstaff has received a number of trainings on the different aspects of generating the datasets and\r\ndeveloping climate information products. However, due to the recent reorganization at Meteo\r\nRwanda, as well as updates to the tools used to generate historical data and information\r\nproducts, it was necessary to revise the training and update climate data and information\r\nproducts. The current activities, which are part of CCAFS-USAID Climate Services for\r\nAgriculture project, had two major components: (1) data quality control and generating\r\nupdated climate datasets; and (2) integrating the newly generated datasets into Meteo\r\nRwanda\u2019s maprooms. Sixteen Meteo Rwanda staff members received six days of training on\r\ndata quality control and generating updated climate datasets, and/or 4.5 days of training on\r\ndeveloping and maintaining ENACTS maprooms",
    "fullText": " \n \n \n \n \nWorkshop\treport:\tTraining\tprogram\ton\tENACTS\t\nclimate\ttime\tseries,\tdata\tlibrary\tand\tmaprooms,\t\nKigali,\tRwanda\t\n \nDecember 2015 \n \nTufa Dinku and John del Corral \n \n \n \n \n \n 1 \n \n \n \n \n \n \n \nTraining program on ENACTS \nclimate time series, data \nlibrary and maprooms \nKigali, Rwanda, December 2015 \n \nWorkshop Report \n \nCGIAR Research Program on Climate Change, \nAgriculture and Food Security (CCAFS) \n \nTufa Dinku \nJohn del Corral \n \n \n  \n 2 \nCorrect citation:  \nDinku T, del Corral J. 2016. Training program on ENACTS climate time series, data\nlibrary and maprooms, Kigali, Rwanda, December 2015. CCAFS Workshop Report. \nCGIAR Research Program on Climate Change, Agriculture and Food Security \n(CCAFS). Copenhagen, Denmark. Available online at: www.ccafs.cgiar.org  \n \nCCAFS Workshop Reports aim to disseminate interim climate change, agriculture \nand food security research and practices and stimulate feedback from the scientific \ncommunity. \n \nPublished by the CGIAR Research Program on Climate Change, Agriculture and \nFood Security (CCAFS).  \n \nCCAFS is a strategic partnership of the CGIAR and the Earth System Science \nPartnership (ESSP). CGIAR is a global research partnership for a food secure future. \nThe program is supported by the Canadian International Development Agency \n(CIDA), the Danish International Development Agency (DANIDA), the European \nUnion (EU), and the CGIAR Fund, with technical support from the International Fund \nfor Agricultural Development (IFAD). \n \nContact: \nCCAFS Coordinating Unit - Faculty of Science, Department of Plant and \nEnvironmental Sciences, University of Copenhagen, Rolighedsvej 21, DK-1958 \nFrederiksberg C, Denmark. Tel: +45 35331046; Email: ccafs@cgiar.org  \n \nCreative Commons License \n \nThis Workshop Report is licensed under a Creative Commons Attribution \u2013 \nNonCommercial\u2013NoDerivs 3.0 Unported License. \n \nArticles appearing in this publication may be freely quoted and reproduced provided \nthe source is acknowledged. No use of this publication may be made for resale or \nother commercial purposes. \n \n\u00a9 2016 CGIAR Research Program on Climate Change, Agriculture and Food \nSecurity (CCAFS). \n \n \n \nPhoto: Tufa Dinku \n \n \nDISCLAIMER: \nThis Workshop Report has been prepared as an output for USAID\u2019s Rwanda Climate \nServices for Agriculture project, under the climate risk management theme of the \nCCAFS program, and has not been peer reviewed. Any opinions stated herein are \nthose of the author(s) and do not necessarily reflect the policies or opinions of \nCCAFS, donor agencies, or partners. \nAll images remain the sole property of their source and may not be used for any \npurpose without written permission of the source. \n 3 \nAbstract  \nMeteo Rwanda, in collaboration with the International Research Institute for Climate and \nSociety (IRI), has implemented the first phase of the ENACTS (Enhancing National Climate \nServices) initiative. The ENACTS initiative brings climate knowledge into national decision-\nmaking by improving availability, access and use of climate information.  Meteo Rwanda \nstaff has received a number of trainings on the different aspects of generating the datasets and \ndeveloping climate information products.  However, due to the recent reorganization at Meteo \nRwanda, as well as updates to the tools used to generate historical data and information \nproducts, it was necessary to revise the training and update climate data and information \nproducts.  The current activities, which are part of CCAFS-USAID Climate Services for \nAgriculture project, had two major components:\t(1) data quality control and generating \nupdated climate datasets; and (2) integrating the newly generated datasets into Meteo \nRwanda\u2019s maprooms.\tSixteen Meteo Rwanda staff members received six days of training on \ndata quality control and generating updated climate datasets, and/or 4.5 days of training on \ndeveloping and maintaining ENACTS maprooms\t\n \nKeywords \nQuality Control, Merging, Satellite, Reanalysis. \n \n  4 \nAbout the authors  \nTufa Dinku is a Research Scientist at the International Research Institute for Climate and \nSociety (IRI), at Columbia University, focused on evaluating and improving climate datasets. \nContact: tufa@iri.columbia.edu  \nJohn del Coral is a Senior Staff Associate at the International Research Institute for Climate \nand Society (IRI), at Columbia University, focused on database management, GIS and \nsemantic technology. Contact: jdcorral@iri.columbia.edu  \n 5 \nAcknowledgements \nThis report is an output of USAID\u2019s Rwanda Climate Services for Agriculture project, and \nwas made possible through support provided by the Rwanda Mission, U.S. Agency for \nInternational Development. The opinions expressed herein are those of the authors, and do not \nnecessarily reflect the views of the U.S. Agency for International Development. \n  6 \nContents  \nIntroduction ................................................................................................................................ 8\t\nActivity 1: Data quality control and generating updated climate data sets ................................ 9\t\nTraining .................................................................................................................................. 9\t\nWork on generating data ...................................................................................................... 10\t\nActivity 2: Integrating the newly generated datasets into Meteo Rwanda\u2019s maprooms .......... 20\t\nConclusion ................................................................................................................................ 21\t\nAppendix 1. Training Program ................................................................................................. 22\t\nAppendix 2. Participant List ..................................................................................................... 24\t\n \n 7 \nAcronyms \nCDT Climate Data Tools \nCHIRP Climate Hazard Group Infrared Precipitation \nENACTS Enhancing National Climate Services \nIRI International Research Institute for Climate and Society \nJRA55 Japan 55 Years Reanalysis \nQGIS Quantum Geographic Information System \nUSAID United States Agency for International Development \n \n \n \n  8 \n \nIntroduction \nThe Rwanda Meteorological Agency (Meteo Rwanda), in collaboration with the International \nResearch Institute for Climate and Society (IRI) has embarked on a unique multi-faceted \ninitiative called Enhancing National Climate Services (ENACTS). The initiative aims to bring \nclimate knowledge into national decision-making by improving availability, access and use of \nclimate information.  Availability of climate data is improved by combining quality-\ncontrolled data from the national observation network, which is very sparse over many parts \nof the country, with satellite estimates for rainfall and elevation maps and reanalysis products \nfor temperature. These new data sets have been used to develop some information products \nthat are made available through Meteo\u2019s web page. The Rwanda Climate Services for \nAgriculture (CSA) project, funded by USAID and led by CCAFS, builds on and incorporates \nENACTS as part of its Outcome 3: Climate Information Provision.  \nThrough an earlier project, Meteo Rwanda staff received several trainings by IRI experts on \nthe different aspects of generating the data sets and developing the products.  However, due to \nthe recent, and ongoing, reorganization at Meteo Rwanda, many people have left and some \nnew staffs have been hired. Thus, Meteo Rwanda has asked for refresher training for old staff \nas well as training new staff.  There have also been some new developments in methodology \nand tools. Thus, this training introduced all participants to these new methods and tools. One \nof the new tools is quality control at daily time scale, which will enable Meteo Rwanda to \nmake correction to the data in their database. This also prepared the grounds for generating \ndata at daily time scale, which is required for new products planned for Rwanda\u2019s agriculture \nsector.  \nThis training prepared Meteo Rwanda for the development of new products and services \nplanned within the CSA project.   The training activities had two major components: \n1. Data quality control and generating updated climate datasets, and. \n2. Integrating the newly generated datasets into Meteo Rwanda\u2019s maprooms. \n \n 9 \nThe first training was conducted at two different time: 30 November to 3 December 2015 and \n27 to 29 January 2016.  Twelve participants received training on data quality control and \ngenerating updated climate datasets.  The second training took place 14 to 18 December 2015. \nAn overlapping set of twelve participants were trained on developing and maintaining \nENACTS maprooms. Daily quality check was performed on temperature as part of the \ntraining. Quality control of daily rainfall data was done outside the training due time \nconstraints. Some serious quality issues were observed. The quality-controlled data were used \nto update dekadal (10-day) rainfall and temperature time series.  The updated rainfall time \nseries goes from 1981 to 2015 while the temperature time series covers the period 1961 to \n2014. \nActivity 1: Data quality control and generating updated \nclimate data sets \nThis was conducted during 30 November to 3 December 2015 and 27 to 29 January 2016. It \nincluded a training component, and actual guided work on generating datasets.   \nTraining \nThe training was a revision of previous ENACTS training. However, most of the trainees \nwere totally new to subjects and lack background in meteorology.  The training included both \ntheoretical background and practical exercises on Meteo Rwanda\u2019s actual data. The topics \ncovered in the training (Appendix 1) included the following: \nQuality control of station data. Data quality control is a critical component of ENACTS. \nThus, significant amount of time is spent on this aspect of the training.  Trainees were first be \nintroduced to the need for quality control of climate data, different types and sources of error \nwith examples and different approaches to quality control of climate data. Then trainees then \nperformed actual quality control of temperature data using IRI\u2019s Climate Data Tools (CDT). \nSatellite rainfall estimation. An overview of satellite remote sensing, different satellites and \nsensor types, and the use of the different sensors for rainfall estimation was presented and \ndiscussed. \n  10 \nClimate Reanalysis Data. Climate reanalysis data are used for interpolating temperature data. \nThus, a brief introduction was provided on the concept of climate reanalysis products. \nInterpolation of climate data. The trainees were introduced to different interpolations \nmethods, and their strengths and weakness.  Then they explored different interpolation \nmethods and the different factors that may affect interpolated values using their data and the \nCDT tool. \nCombining satellite data with station measurements. The trainees were exposed to some \nmerging techniques and then were shown how using auxiliary information, such as \ntopography for temperature or satellite rainfall estimates for rainfall, could improve the \nquality of the interpolated values. \nWork on generating data \nQuality control \nMost of this was done mostly outside the training time because there was no enough time to \ndo it as part of the training. However, the trainees did do some practical exercises on data \nquality control and merging station data with proxies. \nThe quality control for rainfall has two parts: (1) detecting false zeros; and (2) identifying \nextremely high rainfall values. The first case happens when given station reports significantly \nhigher number of zero values compared to the surrounding stations. This occurs mainly when \nmissing observations are reported as blanks and the data entry staff interpret the blanks as \nzeros.  The IRI Climate Data Tools (CDT) compares the given station with surrounding \nstations to identify the problem. An example of the output from CDT is given in Table 1.  For \ninstance, the first station in the table (ID = 20101600)\treported\tall\tzero\tvalues\t(val.stn=100%)\t\nfor\tNov\t1988.\t\tHowever,\ta\tstation\tjust\t11km\taway\treported\tonly\t45%\tzeros\tfor\tsame\tmonth.\t\nThe\taverage\tfor\tstations\tin\tthe\tarea\tis\t44%,\twhile\tthe\tmaximum\tis\t53%.\tThus,\tstation\t\n20101600\tseems\tto\thave\tfalse\tzeros,\twhich\tneed\tto\tbe\tchecked. \nThe other quality heck procedure is to identify extreme values. This is done both by \ncomparing each observation of each station with historical records for a specific month \n(temporal check) as well as comparing the station value with surrounding stations for the \nsame day (spatial check). Two examples of the temporal check are presented in Figures 1 and \n 11 \n2 below. The red bars indicate are what CDT identified as suspect values. These values need \nto be checked with the original paper data and fixed. Figure 3 is an example of the spatial \ncheck where a station observation is compared with values from the surrounding stations. The \nsuspect value is shown in read. In Fig. 3, the value of the station is 248 mm while the \nmaximum from the neighbouring stations is 63mm. Note that these are daily values. Thus, the \n248 mm for that station is very unlikely and need to be checked. \nTable 1: A sample output of zero-check. \nID YY MM val.stn nrst.val nrst.stn avg.nbrs max.nbrs \n20101600 1988 11 100 45 11 44 53 \n20107500 1984 1 100 32 3 48 58 \n20306600 1992 11 100 30 6 31 40 \n20306600 1992 12 100 45 6 45 61 \n20513500 1984 6 100 97 8 55 97 \n20513500 1985 7 100 0 17 0 0 \n20513500 1986 7 100 19 17 19 19 \n20601500 1981 9 100 53 6 62 70 \n20602100 1997 3 100 45 4 48 52 \n20604200 1981 12 100 52 5 55 61 \n20703200 1990 6 100 50 14 50 50 \n \nFigure 1: A sample output of the temporal quality check procedure. The extreme values \nsuspected by the quality control procedure (CDT) are shown in red. The number at the \ntop is station ID. \n \n  12 \nFigure 2: Same as Fig. 1, but for another station. \n \nFigure 3: Result of spatial quality check. The suspect value is given in red. \n \nThe quality control for temperature is different from rainfall in that there is no zero-check. \nHowever, the extreme values could be either low or high. Both temporal and spatial checks \nhave been performed.  Examples are given below in Figures 4 through 7. Figures 4 and 5 are \nfor minimum temperature while figures 6 and 7 are for maximum temperature. Figure 4 \nshows both high and low extreme values flagged by the quality check procedure. In this case, \nparticularly for the low values case, a number of observations have been flagged as suspect. \nOn the other hand, Fig. 5 shows one very unlikely extreme value (> 120oC). This could most \nprobably be a data entry problem. \n 13 \n \nFigure 4: Minimum temperature time series for the month April. The red color shows \nextreme values flagged by CDT as suspect observations.  \nFigure 5: Minimum temperature time series for station 20411100, for the month April. \n  14 \nFigure 6. Maximum temperature time series for station 20509200 in the month of \nNovember. \nFigure 7: Maximum temperature time series for the month of July. \n \nSometimes it might not be to detect observation errors just by using objective methods. An \nexample is given in Figure 8. Here the values are not extreme; thus, the quality-check \nprocedure did not flag any values (no red bars).  However, the constant values of 15oC shown \nin the figure do not look like reasonable observations. It is hard to imagine that temperature \nvalues will be constant for so many days. These values must be wrong. These kinds of errors \ncould only be found by looking at the data station-by-station and month-by-month as shown \nhere.  \n \n 15 \nFigure 8: Suspect observations of constant value (15oC). These values have not been \nflagged as suspects, but do look right. \nGenerating updated climate data sets  \nThe approach adopted for generating rainfall time series involves the following steps: \n1. Use the data from 1981 to 1993 to calculate climatological adjustments factors for each \ndekad (dekads 1 to 36). Figure 9 shows stations used for this purpose \n2. Interpolate the adjustment factors to the required grid points; and \n3. Apply the adjustment factors to all satellite time series from 1981 to present; \n4. Remove mean dekadal bias (averaged over the whole country) for each year using \navailable stations; \n5. Merge the output from the pervious step with available station data for each of dekad of \neach year. Stations used for this step are shown in Fig. 9. \nThe last two steps results in two different rainfall time series. The time series from step 4 have \nrelatively more homogenous time series except around 1994 (Fig 10).  These are used for the \nClimate Analysis maproom. They can also be used for other climate analysis activities.  The \ndata from step 5 is relatively the most accurate as it combines station observation for each \ndekad. The problem with this data is that the number of stations used varies from year-to-\nyears, with almost no station data for part of 1994 and few stations for many years after that \n(Fig. 10). These data are good for applications that do need to compare one year with another.  \n \n  16 \nFigure 9: Distribution of stations used for bias removal (left) and combining with \nsatellite estimates dekad-by-dekad (right).  \n \nFigure 10: Number of stations with data for each year-month-dekad from 1981 to 2014. \n \nFigure 11 below compares station data with satellite and combined station-satellite products. \nThe combined product is an output from step 5 above. \n \n 17 \nFigure 11: Comparisons of station observation (left), satellite estimate (center) and \ncombined product (right) for a given dekad. \n \nFigure 12: Comparison of satellite estimate (left) and combined-station product (right) \nwith station observations. \n \nThere are no satellite temperature estimates going back 30 years. Thus, reanalysis data are \nused as a proxy. Reanalysis products are climate data generated by systematically combining \nclimate observations (analyses) with climate model forecasts using data assimilation schemes \nand climate models. For this work, we use the Japanese 55-year Reanalysis, or JRA55 \n(http://jra.kishou.go.jp/JRA-55/index_en.html#about). This product has a coarse spatial \nresolution of about 50km. Thus, the reanalysis data are downscaled to 5km spatial resolution \nusing station observations and elevation maps. The following steps are used to reconstruct the \ntemperature time series: \nSatellite Combined \n  18 \n1. Downscale reanalysis data from 50km to 5km; \n2. Use the data from 1981 to 1990 to calculate adjustments factors for each dekad; \n3. Interpolate the adjustment factors; and \n4. Apply the adjustment factors to all downscaled reanalysis data from 1981 to 2014. \n5. Merge the adjusted reanalysis from previous step with station measurements from every \ndekad of every year. \nAs in the case of rainfall, step 4 produces a more homogenous data with less accuracy, while \nstep 5 results in relatively more accurate products with less homogeneity. The first products \ncan  be used for other climate analysis that requires comparing data from different periods. \nThe second product can be used for applications that may not involve comparison of data \nfrom different periods.  This is because the data from different periods, particularly periods \nwith significantly different number of stations, will have different qualities. \nFigure 13 compares station measurements of maximum temperature with downscaled \nreanalysis products and combined station-reanalysis data for a particular dekad.  For \nmaximum temperature, even the downscaled reanalysis data is very close to station \nobservation. This could also be observed from the scatter plots in Fig. 14, which compare \nboth reanalysis and the combined products with station measurements. \nFigure 13: Comparison of station observation (left), reanalysis (center) and combined \nproduct for a given dekad. \n \nStation Reanalysis Combined \n 19 \nFigure 14: Comparison of downscaled reanalysis (left) and combined product (right) \nwith station observations of maximum temperature \n \nFor minimum temperature (Fig. 15), the reanalysis product overestimates temperature values. \nThis is also shown in the scatter plot of Fig. 16. Figure 16 also shows that the combined \nproduct is a significant improvement over the downscaled reanalysis data. \nFigure 15: Comparison of station observation (left), reanalysis (center) and combined \nproduct of minimum temperature for a given dekad.  \n \nReanalysis Combined Station \nReanalysis Combined \n  20 \nFigure 16: Comparison of downscaled reanalysis (left) and combined product (right) \nwith station observations of minimum temperature. \nActivity 2: Integrating the newly generated datasets \ninto Meteo Rwanda\u2019s maprooms \nThis was conducted from 14 to 18 December 2015. The first 4 training days were held at the \nClassic Hotel, Kigali with 10 participants from the Rwanda Meteorological Agency, 2 of \nwhich were female. The final day of training was held at the Rwanda Meteorological Agency, \nKigali with 3 male participants. All training materials and training schedule were distributed \nto the participants through online access to a GoogleDrive folder. \nDay 1: Presentation of Data Library and maproom components.  Detailed demonstration and \nexplanation of all the ENACTS maprooms in the Rwanda Data Library. Participants were \nable to browse the maprooms and try different functions in the maproom. \nDay 2: Introduction to the ENACTS Data Catalog. Description of how to add and update \ndatasets in the Rwanda Data Library. Demonstration of basic Linux commands. \nDemonstration of viewing and manipulating ENACTS datasets. \nDay 3: Introduction to QGIS (open source geographic information system).  Installation of \nQGIS on participants' laptops.  Demonstration of digitizing a seasonal forecast for Rwanda \nReanalysis Combined \n 21 \nusing QGIS. Practice by participants of digitizing using QGIS.  More detailed explanation of \nENACTS maprooms. \nDay 4: Demonstration of adding GIS datasets to ENACTS Data Catalog.  Demonstration of \nbuilding and modifying ENACTS maprooms. Answering of follow-up questions about QGIS. \nDay 5: Demonstration of creating a backup disk for the Rwanda Data Library and maprooms. \nReview of backup and recovery procedures for the Rwanda Data Library. Updated ENACTS \ndata was transferred to the Rwanda Data Library server. \nConclusion  \nThe two training activities built Meteo Rwanda\u2019s capacity in different areas. The training on \ndata further improved Meteo Rwanda\u2019s capacity in quality control of station data and \ngenerating merged climate time series. The generated and updated datasets also enable Meteo \nRwanda to provide improved services. \nThe maproom  training prepared for official launch of the ENACTS Maproom on the website \nof the Rwanda Meteorological Agency in 2016.  It is recommended to monitor connectivity \nand functionality of Maproom server and monitor updates of the ENACTS Monitoring \nmaproom. Meteo Rwanda has solid computer, network, and electricity infrastructure. It is \npoised to be able to provide consistent online climate services for many sectors in Rwanda. \nFurther development is needed to engage stakeholders. This can be done with sector-focused \nmaprooms and sector-focused training workshops. In-depth training on the maintenance and \nuse of the ENACTS products and maprooms needs to be done. This will make the staff at \nMeteo Rwanda more confident and self-sufficient. \n \n \n \n  22 \nAppendix 1. Training Program \nActivity 1. Generating historical climate time series \nMonday, 30 November \nIntroduction to the training \nInstallation and testing of software \nIntroduction to the Climate Data Tools (CDT) \nIntroduction satellite rainfall estimation \nTuesday 1 December \nIntroduction to Climate Reanalysis products \nIntroduction to quality control of station data \nPractical: QC of RR and TT data \nWednesday 3 December \nIntroduction to interpolation of climate variables \nIntroduction to merging data from different sources \nPractical: merging data from different sources \nThursday 4 December \nUpdating data for the monitoring maproom \nExtracting data for points, boxes, and administrative boundaries \nSummary \nActivity 2. Maproom development and exploitation \nMonday 14 December \nOverview of Maproom structure \nMaproom and Data Library linkage to datasets and functions \nPractical: Analysis of Dominant Climate trends in each region \nTuesday 15 December \nAdd updated Monitoring data to Data Library \nQuality check Monitoring maproom with new data \nPractical: Process most recent dekad for Monitoring maproom \nWednesday 16 December \nIntroduction to git repository of Maproom on BitBucket \nUpdating the local copy and repository copy of the Maproom \nPractical: Set up BitBucket accounts for primary users \n 23 \nThursday 17 December \nProcedure for making backup copy of Data Library \nPractical: Make backup of Data Library \nFriday 18 December \nMonitoring methods for Data Library and Maproom \nHow to debug problems and fix them \nPractical: Open platform for questions and follow-up \n \n  \n  24 \nAppendix 2. Participant List \nName Title Institute Gender \nTraining \n1 2 \nJean Marie Niyitegeka Forecasting Officer Meteo-Rwanda M Y Y \nValens Rwakaozyo Data Processing Meteo-Rwanda M Y Y \nDidace Musoni Assistant Director General Meteo-Rwanda M Y Y \nSerge Senyana Meteo Co-Application Officer Meteo-Rwanda M Y Y \nBlandine Mukamanea Observations Processing Officer Meteo-Rwanda F Y Y \nVuguziga Floribert Forecasting Officer Meteo-Rwanda M Y Y \nAnnah Muteteri  Meteo-Rwanda F Y N \nFelicien Nsabakunze  Meteo-Rwanda M Y N \nJanet Umuhoza  Meteo-Rwanda F Y N \nMathieu Mugunga Mbati  Meteo-Rwanda M Y N \nFrancois Nsengiyumva  Meteo-Rwanda M Y N \nClarisse Mukazarukommo Observations Officer Meteo-Rwanda F N Y \nJonah Kazora Forecasting Officer Meteo-Rwanda M N Y \nJoseph Hazabintwari Observations Officer Meteo-Rwanda M N Y \nJean Paul Kalisa Observations Officer Meteo-Rwanda M N Y \nBrave Rukagumya Observations Officer Meteo-Rwanda M Y Y \nJohn del Coral Trainer IRI M N Y \nTufa Dinku Trainer IRI M Y N \n\t\n \n",
    "source": "CGIAR Research Program on Climate Change, Agriculture and Food Security"
  }
]