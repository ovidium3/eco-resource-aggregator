#!/usr/bin/env python
"""Build keyword_map.py from a plain text list of terms.

Usage:
    python categorize_keywords.py keywords.txt
After running, import KEYWORDS from keyword_map.py.
"""
import argparse, pprint, textwrap
from pathlib import Path
from transformers import pipeline
from categories import CATEGORIES


# ------------------- GLOBALS ---------------------------------------
DEF_MODEL = "facebook/bart-large-mnli"   # NLI model


def main():
    """Build keyword_map.py from a plain text list of terms.
    Usage:
        python categorize_keywords.py keywords.txt
    After running, import KEYWORDS from keyword_map.py.
    """
    ap = argparse.ArgumentParser()
    ap.add_argument("keyword_file", help="text file, one term per line")
    ap.add_argument("--model", default=DEF_MODEL)
    ap.add_argument("--outfile", default="keyword_map.py")
    args = ap.parse_args()

    terms = [t.strip() for t in Path(args.keyword_file).read_text().splitlines()
             if t.strip()]
    print(f"Loaded {len(terms)} terms from {args.keyword_file}")

    clf = pipeline("zero-shot-classification",
                   model=args.model,
                   device_map="auto")

    buckets = {c: [] for c in CATEGORIES}
    for term in terms:
        result = clf(
            term,
            candidate_labels=CATEGORIES,
            hypothesis_template="This term is related to {}."
        )
        cat = result["labels"][0]
        buckets[cat].append(term)

    out = Path(args.outfile)
    with out.open("w") as f:
        f.write("# Auto‑generated by categorize_keywords.py\n")
        f.write("KEYWORDS = ")
        pprint.pprint(buckets, stream=f, width=120)

    print(textwrap.dedent(f"""
        ✔ Keyword map written to {out.resolve()}
        Import via:
            from keyword_map import KEYWORDS
    """).strip())


if __name__ == "__main__":
    main()
